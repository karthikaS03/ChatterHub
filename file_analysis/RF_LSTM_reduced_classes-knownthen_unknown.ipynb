{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import json \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import keras \n",
    "\n",
    "# import numpy\n",
    "# import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import glob\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadFromMerged=True\n",
    "loadFromIndexes= False\n",
    "Mapper='S'\n",
    "IgnoreEmpty= True\n",
    "FoldID =\"1\"\n",
    "Epoch_count=100\n",
    "Batch_size=5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data the old way\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_data= [] \n",
    "# y_data= [] \n",
    "\n",
    "\n",
    "# with open( '../files/txt/seq_mapping_large.txt' ) as f:\n",
    "#     x_data =   f.readlines()\n",
    "\n",
    "# with open( '../files/txt/command_mapping_large.txt' ) as f:\n",
    "#     y_data = f.readlines()\n",
    "    \n",
    "    \n",
    "# x_data =[ np.array([ int(y) for y in x.strip().split( ' ') ])   for x in  x_data ] \n",
    "# y_data =[ x.strip().split(' ') for x in  y_data ] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Load The Data The New Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  mapps the input records to a integer array for the input\n",
    "def mapping_x( inp, includeDirection = False , TrimAt= 15 ):\n",
    "    if includeDirection:\n",
    "        return np.array([ int(x[\"packet_length\"]) * (1 if x['packet_source']=='hub' else -1)  for x in inp ][:15])\n",
    "    else:\n",
    "        return np.array([ int(x[\"packet_length\"])  for x in inp ][:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping_y_service(inp):\n",
    "    return np.array(  list(set([x[\"event\"] for x in inp])) if (len(inp )>0) else [\"none\"] )\n",
    "\n",
    "def mapping_y_service_event(inp):\n",
    "    return np.array(  list(set([ \"%s-%s\"%( x[\"event\"] ,x[\"val\"] ) for x in inp])) if (len(inp )>0) else [\"none\"] )\n",
    "\n",
    "def mapping_y_device_service(inp):\n",
    "    return np.array(  list(set([ \"%s & %s\"%( x[\"device\"] ,x[\"event\"] ) for x in inp])) if (len(inp )>0) else [\"none\"] )\n",
    "\n",
    "def mapping_y_full(inp):\n",
    "    return np.array(  list(set([ \"%s & %s & %s\"%( x[\"device\"] ,x[\"event\"], x['val'] ) for x in inp])) if (len(inp )>0) else [\"none\"] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cleans the data removing emply nodes and turning the nodes into sarrays by calling the mapping function \n",
    "def clean_data( x_data, y_data , removeempty=True, Mapping='S'):\n",
    "    cleans = [] \n",
    "    cleans = (sorted([ x for x in y_data if (removeempty and len(y_data[x]) > 0) or not removeempty  ] ))\n",
    "    \n",
    "    ret_x  = [x_data[x] for x in cleans]\n",
    "    ret_y  = [y_data[x] for x in cleans] \n",
    "    \n",
    "    print( len(y_data), len(cleans) )\n",
    "    \n",
    "    ret_x  = [ mapping_x(x) for x in ret_x ] \n",
    "    ret_y_s = [ mapping_y_service(y) for y in ret_y ]\n",
    "    if Mapping=='S':\n",
    "        ret_y  = [ mapping_y_service(y) for y in ret_y ]\n",
    "    elif Mapping=='SE':\n",
    "        ret_y  = [ mapping_y_service_event(y) for y in ret_y ]\n",
    "    elif Mapping=='DS':\n",
    "        ret_y  = [ mapping_y_device_service(y) for y in ret_y ]\n",
    "    elif Mapping=='F':\n",
    "        ret_y  = [ mapping_y_full(y) for y in ret_y ]\n",
    "    return ret_x, ret_y, ret_y_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in load from merged\n",
      "58958 57867\n",
      "loading from test files\n",
      "found files :  4\n",
      "32069 32069\n",
      "19968 19968\n",
      "9109 9109\n",
      "6404 6404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(57867, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= []\n",
    "y= []\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "y_test_service= []\n",
    "\n",
    "x_train = {}\n",
    "y_train = {}\n",
    "\n",
    "test_names = []\n",
    "\n",
    "add_to_trainig = [0,2]\n",
    "\n",
    "if loadFromMerged:\n",
    "    print(\"in load from merged\")\n",
    "    with open(  '../files/train/test/test_homes/final_upload/Merged_final_with_home/merged_hub_segments_final.json'  ) as f:\n",
    "        y_data = json.load(f)\n",
    "\n",
    "    with open(  '../files/train/test/test_homes/final_upload/Merged_final_with_home/merged_pcap_segments_final.json'  ) as f:\n",
    "        x_data = json.load(f)\n",
    "        \n",
    "#     with open(  '../files/train/merged/hub_segments_2.json'  ) as f:\n",
    "#         y_data = json.load(f)\n",
    "\n",
    "#     with open(  '../files/train/merged/pcap_segments_2.json'  ) as f:\n",
    "#         x_data = json.load(f)\n",
    "  \n",
    "    if len( y_data ) != len(x_data) :\n",
    "        print( pick )\n",
    "        \n",
    "    \n",
    "    x_train,y_train, y_train_service= clean_data( x_data, y_data, IgnoreEmpty , Mapping=Mapper )\n",
    "    \n",
    "    #     continue\n",
    "#     if loadFromIndexes:\n",
    "#         print(\"load from indexes\")\n",
    "#         with open(\"../files/train/merged/items_2_test-train_indexes.json\")  as f:\n",
    "#             index_info = json.load(f)\n",
    "\n",
    "\n",
    "#         for i in index_info[FoldID][\"test\"]:\n",
    "#             x_test[str(i)]=(x_data[str(i)] )\n",
    "#             y_test[str(i)]=(y_data[str(i)] )\n",
    "\n",
    "#         for i in index_info[FoldID][\"train\"]:\n",
    "#             x_train[str(i)]=(  x_data[str(i)] )\n",
    "#             y_train[str(i)]=(  y_data[str(i)] )\n",
    "        \n",
    "#         x_test_t,y_test_t= clean_data( x_test, y_test, IgnoreEmpty , Mapping=Mapper)\n",
    "#         x_test.append(x_test_t)\n",
    "#         y_test.append(y_test_t)\n",
    "    #     else :\n",
    "    print(\"loading from test files\")\n",
    "    test_files = sorted(glob.glob( '../files/train/test/test_homes/final_upload/usecases/pcap_segments_final_final/home*.json' ))\n",
    "    print( \"found files : \" , len(test_files) )\n",
    "    for pick  in test_files:\n",
    "        fname  = os.path.basename(pick)\n",
    "        test_names.append( fname )\n",
    "        with open( os.path.join( '../files/train/test/test_homes/final_upload/usecases/hub_segments_final_final/', fname) ) as f:\n",
    "            y_data_test = json.load(f)\n",
    "\n",
    "        with open( os.path.join('../files/train/test/test_homes/final_upload/usecases/pcap_segments_final_final/', fname) ) as f:\n",
    "            x_data_test = json.load(f)\n",
    "\n",
    "\n",
    "        t_x,t_y, t_z= clean_data( x_data_test, y_data_test, False , Mapping=Mapper )\n",
    "\n",
    "#         if test_files.index(pick) in add_to_trainig:\n",
    "#             x_test_t,y_test_t, y_test_service_t= clean_data( x_data_test, y_data_test, IgnoreEmpty , Mapping=Mapper)\n",
    "#             x_train.extend(x_test_t)\n",
    "#             y_train.extend(y_test_t)\n",
    "#             y_train_service.extend(y_test_service_t)\n",
    "\n",
    "\n",
    "        x_test.append(t_x)\n",
    "        y_test.append(t_y)\n",
    "        y_test_service.append(t_z)\n",
    "            \n",
    "#     x_test = x_data[ index_info[\"1\"][\"test\"]  ]\n",
    "#     y_test = y_data[ index_info[\"1\"][\"test\"]  ]\n",
    "    \n",
    "#     x_train = x_data[ index_info[\"1\"][\"train\"]  ]\n",
    "#     y_train = y_data[ index_info[\"1\"][\"train\"]  ]\n",
    "#     x.extend(t_x)\n",
    "#     y.extend(t_y)\n",
    "else:\n",
    "    for pick in sorted(glob.glob( '../files/train/hub_segments/*.json' )):\n",
    "        fname  = os.path.basename(pick)\n",
    "        test_names.append( fname )\n",
    "        with open( os.path.join( '../files/train/hub_segments/', fname) ) as f:\n",
    "            y_data = json.load(f)\n",
    "\n",
    "        with open( os.path.join('../files/train/pcap_segments/', fname) ) as f:\n",
    "            x_data = json.load(f)\n",
    "\n",
    "        if len( y_data ) != len(x_data) :\n",
    "            print( pick )\n",
    "            continue\n",
    "\n",
    "        t_x,t_y= clean_data( x_data, y_data, True )\n",
    "\n",
    "        x.extend( t_x)\n",
    "        y.extend(t_y)\n",
    "\n",
    "x= np.array(x)\n",
    "y= np.array(y)\n",
    "\n",
    "# x_train = np.append( x_train, x_test[0] , axis=0)\n",
    "# x_train = np.append( x_train, x_test[2] , axis=0)\n",
    "\n",
    "# y_train = np.append( y_train, y_test[0] , axis=0)\n",
    "# y_train = np.append( y_train, y_test[2] , axis=0)\n",
    "\n",
    "\n",
    "len(x_train), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Mittigation Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packet Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# days =[7,4,2,2]\n",
    "# t_sum =0\n",
    "# for ii in range(len(x_test)):\n",
    "#     p = x_test[ii]\n",
    "#     sizes = np.unique(np.concatenate(p), return_counts=True)\n",
    "#     sums= 0 \n",
    "#     for i in range(len(sizes[0])):\n",
    "# #         print( \"%d--> %d\" % ( sizes[0][i], sizes[1][i] ) )\n",
    "#         if  sizes[0][i] < 1000:\n",
    "#             sums+= (1000-sizes[0][i] )* sizes[1][i]\n",
    "#     t_sum +=(sums / days[ii] )/1000000 \n",
    "#     print ( (sums / days[ii] )/1000000)\n",
    "# print('--------')\n",
    "# print(t_sum/4)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# days =[7,4,2,2]\n",
    "# t_sum =0\n",
    "# d_sum = 0\n",
    "# import math\n",
    "# for ii in range(len(x_test)):\n",
    "\n",
    "#     p = x_test[ii]\n",
    "#     for i in p : \n",
    "#         t_sum += math.ceil(np.sum(i) / 2000)\n",
    "#         d_sum+= np.sum( i )\n",
    "# total_fixed  =  t_sum* 2000 / 15\n",
    "\n",
    "# print ( total_fixed , d_sum, d_sum-t_sum*2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packet Insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# days =[7,4,2,2]\n",
    "# t_sum =0\n",
    "# for ii in range(len(x_test)):\n",
    "#     p = x_test[ii]\n",
    "#     sizes = np.unique(np.concatenate(p), return_counts=True)\n",
    "#     sums= 0 \n",
    "#     for i in range(len(sizes[0])):\n",
    "# #         print( \"%d--> %d\" % ( sizes[0][i], sizes[1][i] ) )\n",
    "#         if  sizes[0][i] < 1000:\n",
    "#             sums+= (1000-sizes[0][i] )* sizes[1][i]\n",
    "#     t_sum +=(sums / days[ii] )/1000000 \n",
    "#     print ( (sums / days[ii] )/1000000)\n",
    "# print('--------')\n",
    "# print(t_sum/4)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sets the classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'acceleration'),\n",
       " (1, 'activity'),\n",
       " (2, 'battery'),\n",
       " (3, 'button'),\n",
       " (4, 'colorTemperature'),\n",
       " (5, 'contact'),\n",
       " (6, 'level'),\n",
       " (7, 'lock'),\n",
       " (8, 'motion'),\n",
       " (9, 'ping'),\n",
       " (10, 'status'),\n",
       " (11, 'switch'),\n",
       " (12, 'temperature'),\n",
       " (13, 'threeAxis'),\n",
       " (14, 'unknown'),\n",
       " (15, 'water')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = sorted(list(np.unique(  np.concatenate( y_train  ))))\n",
    "# print([ (i , classes[i]) for i in range( len(classes) ) ])\n",
    "\n",
    "service_classes = sorted(list(np.unique(  np.concatenate( y_train_service  ))))\n",
    "[ (i , service_classes[i]) for i in range( len(service_classes) ) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter the records by service/event types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_clean_event( inp, return_clean= True  ):\n",
    "    return is_clean(inp, return_clean=return_clean, to_keep=[ 'no_logs', 'lock-unlocked', 'on/off-XXX', 'raw-XXX', 'read_attr_-_raw-XXX' ] )\n",
    "#     if return_clean:\n",
    "#         return  'no_logs' not in inp and 'lock-unlocked' not in inp and 'on/off-XXX' not in inp and 'raw-XXX' not in inp and 'read_attr_-_raw-XXX' not in inp\n",
    "#     else:\n",
    "#         return  'lock-locked' in inp or 'lock-unlocked'  in inp or 'on/off-XXX' in inp or  'raw-XXX' in inp  or 'read_attr_-_raw-XXX' in inp \n",
    "     \n",
    "def is_clean_service( inp, return_clean= True  ):\n",
    "    return is_clean(inp, to_keep=['no_logs','unknown', 'read_attr_-_raw'], return_clean=return_clean )\n",
    "    \n",
    "#     if return_clean:\n",
    "#         return  'no_logs' not in inp and 'unknown' not in inp and 'read_attr_-_raw' not in inp #and 'ping' not in inp \n",
    "#     else:\n",
    "#         return  'no_logs' in inp or  'unknown' in inp  or 'read_attr_-_raw' in inp #or 'ping' in inp \n",
    "\n",
    "def is_clean(inp, to_keep=[], return_clean=True):\n",
    "    ret = False \n",
    "    \n",
    "    for x  in to_keep:\n",
    "        if x in inp:\n",
    "            ret = True\n",
    "            \n",
    "    if not return_clean:\n",
    "        ret = not ret\n",
    "        \n",
    "    return ret\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "choose services to keep : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "services_to_keep = ['button', 'colorTemperature', 'contact', 'lock', 'motion',\"ping\", 'switch','unknown'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find records which need change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indexes_to_change =  [ i for i in range(len(y_train)) if\n",
    "              is_clean( y_train[i],to_keep=services_to_keep, return_clean=False) ] \n",
    "\n",
    "y_train = [ ['unknown'] if i in train_indexes_to_change else y_train[i] \n",
    "           for i in range(len(y_train)) ] \n",
    "\n",
    "for t_index in range(len(y_test)):\n",
    "    test_indexes_to_change =  [ i for i in range(len(y_test[t_index])) if\n",
    "                  is_clean( y_test[t_index][i],to_keep=services_to_keep, return_clean=False) ] \n",
    "\n",
    "    y_test[t_index] = [ ['unknown'] if i in test_indexes_to_change else y_test[t_index][i] \n",
    "               for i in range(len(y_test[t_index])) ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_unknown_y_train = [ [1,0] if (len(x) == 1 and \"unknown\" in x) else [0,1]   for x in y_train ]\n",
    "\n",
    "known_unknown_y_test= [] \n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    \n",
    "    known_unknown_y_test.append( [ [1,0] if (len(x) == 1 and \"unknown\" in x) else [0,1]   for x in y_test[i] ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove classes that are ignored from the services list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = services_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toKeep = [ i for i in range(len(y_train)) if is_clean_event( y_train[i]) ] if Mapper=='SE' else [ i for i in range(len(y_train)) if is_clean_service( y_train[i]) ]\n",
    "# x_train= [ x_train[i] for i in toKeep ]\n",
    "# y_train= [ y_train[i] for i in toKeep ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for j in range(len(x_test)):\n",
    "#     toChange= [ i for i in range(len(y_test[j])) if is_clean_service( y_test[j][i], False) ]\n",
    "#     y_test[j] = [ (y_test[j][i] if i not in toChange else np.array( ['none'])) for i in range(len(y_test[j])) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes.remove('read_attr_-_raw-XXX')\n",
    "# classes.remove('on/off-XXX')\n",
    "# classes.remove('raw-XXX')\n",
    "# classes.remove('lock-unlocked')\n",
    "# classes.remove('lock-locked')\n",
    "\n",
    "\n",
    "# classes.remove('read_attr_-_raw')\n",
    "# classes.remove('on/off')\n",
    "# classes.remove('raw')\n",
    "# classes.remove('unknown')\n",
    "\n",
    "# classes.remove('lock')\n",
    "# # classes.remove('lock')\n",
    "\n",
    "\n",
    "# classes.remove('switch-on')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ===== end of unknown packet control====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_raw( x_data,y_data, dim_size = 128, zero_pad = False, normalize = False ,\n",
    "                    classes=None, twoD= False, as_string=False ):\n",
    "#  y data \n",
    "# \"\"\"\n",
    "# this functino is in charge of preprocessing the records , the sourc e json contains a lot of extra stuff, this function tailors\n",
    "# the data and it fixes their lenghth\n",
    "# \"\"\"\n",
    "    if classes is None:\n",
    "        classes  = sorted(list(np.unique(  np.concatenate( y_data  ))))\n",
    "    else :\n",
    "        classes = sorted(classes)\n",
    "    y_data_categorical = []  \n",
    "\n",
    "    for x in y_data:\n",
    "        temp = np.zeros( len(classes) )\n",
    "        for y in x : \n",
    "            if y in classes:\n",
    "                temp[ classes.index( y ) ] = 1\n",
    "        y_data_categorical.append( temp )\n",
    "    y_data_categorical = np.vstack(y_data_categorical)\n",
    "\n",
    "#     x_data = np.array( x_data) / 1500.0\n",
    "    \n",
    "    x_data_temp = [] \n",
    "    \n",
    "    if not zero_pad:\n",
    "        if twoD:\n",
    "            for x in x_data:\n",
    "                temp = [] #list(x)\n",
    "                lst = list(x)\n",
    "                while dim_size**2 - len(temp )   > len(lst):\n",
    "                    temp.extend(lst)\n",
    "\n",
    "                while len(temp) < dim_size**2:\n",
    "                    temp.append( 0 )\n",
    "\n",
    "                x_data_temp.append(np.array( temp).reshape(dim_size,dim_size))\n",
    "\n",
    "\n",
    "            x_data_temp = np.array( x_data_temp )\n",
    "            x_data_temp=x_data_temp.reshape(x_data_temp.shape+(1,))\n",
    "        else: \n",
    "            temp = [] \n",
    "            lst = list(x)\n",
    "            for x in x_data:\n",
    "                temp = [] #list(x)\n",
    "                lst = list(x)\n",
    "                while dim_size - len(temp )   > len(lst):\n",
    "                    temp.extend(lst)\n",
    "\n",
    "                while len(temp) < dim_size:\n",
    "                    temp.append( 0 )\n",
    "                \n",
    "                x_data_temp.append(np.array( temp))\n",
    "            \n",
    "    else :\n",
    "        x_data_temp = sequence.pad_sequences(x_data, maxlen=dim_size)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if normalize:\n",
    "        x_data_temp = np.array( x_data_temp) / (np.amax( x_data_temp) + 0.000000000001)\n",
    "    else :\n",
    "        x_data_temp = np.array(x_data_temp)\n",
    "    \n",
    "    \n",
    "    if as_string:\n",
    "        x_data_temp = [ ' '.join(x) for x in x_data_temp ]\n",
    "    \n",
    "    return x_data_temp ,y_data_categorical , classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,_=pre_process_raw( x_train, y_train , 15, zero_pad=True, normalize=True, classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9999999999999997, 0.0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.amax(x), np.amin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_recall_shit( inp ):\n",
    "    tp = inp[1][1]\n",
    "    tn = inp[0][0]\n",
    "    fp = inp[0][1] \n",
    "    fn = inp[1][0]\n",
    "    \n",
    "    acc = (tp+tn)*1.0 / ( tp+tn+fp+fn)*1.0\n",
    "    recall = tp*1.0/ ( tp+fn ) *1.0\n",
    "    prec = tp*1.0 / ( tp+fp )*1.0\n",
    "    \n",
    "#     F= 2.0*( prec* recall )/ (prec+recall)\n",
    "    F= 2.0*( tp)/ (2*tp + fp + fn)\n",
    "    \n",
    "    return acc, recall, prec, F\n",
    "\n",
    "def acc_match( true, pred ):\n",
    "    \"\"\"\n",
    "    returns exact mathc accuracy\n",
    "    \"\"\"\n",
    " \n",
    "    return (len( [ x  for x  in  [np.sum(np.abs( true[i]- pred[i] )) for i in range(len(true))] if x  == 0]))*1.0 / len(true)\n",
    "\n",
    "\n",
    "# def acc_none_zero ( true, pred ):\n",
    "    \n",
    "\n",
    "def acc_match_wierd( true, pred ):\n",
    "    \"\"\"\n",
    "    returns exact mathc accuracy\n",
    "    \"\"\"\n",
    "    level = 6 \n",
    "    switch = 11\n",
    "    threeAxis=13\n",
    "    accel = 0 \n",
    "    status=10\n",
    "    contact=5\n",
    "    \n",
    "    counter  = 0 \n",
    "    for i in range( len (true) ):\n",
    "        if np.sum(np.abs( true[i]- pred[i] ))==0 :\n",
    "            counter+=1\n",
    "        else : \n",
    "            t_rec = np.array(list( pred[i]))\n",
    "            \n",
    "            if true[i][level]==1 and true[i][switch]==1 and t_rec[level]==1 :\n",
    "                t_rec[switch]=1\n",
    "            \n",
    "            if true[i][threeAxis]==1 and true[i][accel]==1 and t_rec[threeAxis]==1:\n",
    "                t_rec[accel] =1\n",
    "            \n",
    "            if true[i][status]==1 and true[i][contact]==1 and t_rec[status]==1:\n",
    "                t_rec[contact]=1\n",
    "#             print(t_rec , true[i])    \n",
    "            if np.sum(np.abs( true[i]- t_rec ))==0 :\n",
    "                counter+=1   \n",
    "            \n",
    "             \n",
    "            \n",
    "    \n",
    "    return counter*1.0 / len(true)\n",
    "\n",
    "\n",
    "def print_info(y_test, pred , classes , confidance=0.5 ):\n",
    "    \n",
    "    counts = np.sum( y_test.astype(int) , axis=0)\n",
    "    \n",
    "    pred[pred>=confidance] = 1\n",
    "    pred[pred<confidance] = 0\n",
    "    \n",
    "#     acc_wierd  =acc_match_wierd(y_test, pred)\n",
    "    \n",
    "    conf= multilabel_confusion_matrix( y_test , pred.astype(int), labels= range(len(classes)))\n",
    "    accs = [make_recall_shit(x) for x in conf]\n",
    "    print( \"%30s  %8s   %8s  %8s  %8s %8s %22s\"  %( \"Class\",\"Accuracy\", \"Recall\",\"Precision\",\"F Score\" , \"Count\", \"TP/TN/FP/FN\"))\n",
    "    print( \"------------------------------------------------------------------------\" )\n",
    "    \n",
    "    for index in range(len(classes)):\n",
    "        tp = conf[index][1][1]\n",
    "        tn = conf[index][0][0]\n",
    "        fp = conf[index][0][1] \n",
    "        fn = conf[index][1][0]\n",
    "        print( \"%30s  %8.3f   %8.3f  %8.3f  %8.3f  %8d %5d/%5d/%5d/%5d\"  %\n",
    "             (classes[index],\n",
    "              accs[index][0],\n",
    "              accs[index][1],\n",
    "              accs[index][2],\n",
    "              accs[index][3],\n",
    "              counts[index],\n",
    "                  tp ,\n",
    "                tn ,\n",
    "                fp ,\n",
    "                fn ))\n",
    "    n_zeros_true = len([ x  for x  in  [np.sum(np.abs( y_test[i] )) for i in range(len(y_test))] if x  == 0]  )\n",
    "    n_zeros_pred = len([ x  for x  in  [np.sum(np.abs( pred[i] )) for i in range(len(pred))] if x  == 0]  )\n",
    "    \n",
    "    accs = np.nan_to_num(accs)\n",
    "    \n",
    "    print (\"------------------------------------------------------------------------\")\n",
    "    print( \"%30s  %8.3f   %8.3f  %8.3f  %8.3f  %8d %5d/%5d/%5d/%5d\"  %\n",
    "             (\"AVERAGES\",\n",
    "              np.average( accs, axis=0)[0],\n",
    "              np.average( accs, axis=0)[1],\n",
    "              np.average( accs, axis=0)[2],\n",
    "              np.average( accs, axis=0)[3],\n",
    "              len(y_test),\n",
    "                  0 ,\n",
    "                0,\n",
    "                0 ,\n",
    "                0 ))\n",
    "    \n",
    "    print ( \"Exact Match ACC : %.5f \" % acc_match( y_test, pred )  )\n",
    "#     print ( \"Wierd Exact Match ACC : %.5f\" % acc_wierd)\n",
    "    print ( \"Total Records : %d \" % len(y_test)  )\n",
    "    print ( \"Total ZXeros in True : %d (%.3f)%%\" % (n_zeros_true ,  n_zeros_true * 1.0/ len(y_test)  ))\n",
    "    print ( \"Total ZXeros in Test : %d (%.3f)%%\" % (n_zeros_pred ,  n_zeros_pred * 1.0/ len(y_test)  ) )\n",
    "    print ('=============================================================================')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def f1_perRow(y_true, y_pred):\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "\n",
    "def f1_perClass(y_true, y_pred):\n",
    "\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss_perClass(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "def f1_loss_perRow(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "def make_readable_results ( inp , classes , conffidance=True):\n",
    "    ret = [] \n",
    "    inp =inp.astype(int)\n",
    "    for xx in range(len(inp)) :\n",
    "        u = inp[xx]\n",
    "        temp = []\n",
    "        for j in range(len(u)) : \n",
    "            if u[j] >0:\n",
    "                temp.append(classes[j])\n",
    "        ret.append(temp)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def makeReadable( model , data, gt, path , classes, x, confidance=0.5):\n",
    "    #collect across multi models\n",
    "    from keras.models import load_model\n",
    "    x=[]\n",
    "    for i in range(0,16):\n",
    "        model=load_model('number'+str(i)+'.h5',custom_objects={'f1_perRow':f1_perRow,'f1_perClass':f1_perClass,'f1_loss_perRow':f1_loss_perRow,'f1_loss_perClass':f1_loss_perClass})\n",
    "        x.append(model.predict(data))\n",
    "    x=np.array(x)\n",
    "    x=np.transpose(x)\n",
    "    x=np.squeeze(x)\n",
    "    print(x.shape)\n",
    "        \n",
    "        \n",
    "    pred_temp  = x\n",
    "    #pred_temp = model.predict(data)\n",
    "    print_info(gt, x, classes , confidance=confidance)\n",
    "#     print( len(classes ), len( pred_temp[0] ) )\n",
    "#     xcc= make_readable_results(pred_temp , classes)\n",
    "#     y_gt = make_readable_results( gt, classes )\n",
    "#     temp_dic = {} \n",
    "#     for pick in range(len(xcc)): \n",
    "#         temp_dic[ pick +1 ] =  { 'seq': str(data[pick]),\n",
    "#                                'pred': xcc[pick],\n",
    "#                                 'true':y_gt[pick]\n",
    "#                                }   \n",
    "\n",
    "#     with open(path , 'w') as f:\n",
    "#         json.dump(temp_dic , f, indent=4)\n",
    "\n",
    "\n",
    "# def makeReadable( model , data, gt, path , classes, confidance=0.7, x):\n",
    "#     pred_temp = model.predict( data)\n",
    "#     print_info(gt, pred_temp, classes , confidance=confidance)\n",
    "#     xcc= make_readable_results( pred_temp , classes )\n",
    "#     temp_dic = {} \n",
    "#     for pick in range(len(xcc)): \n",
    "#         temp_dic[ pick +1 ] = xcc[pick]  \n",
    "\n",
    "#     with open(path , 'w') as f:\n",
    "#         json.dump(temp_dic , f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcualte per class accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest baseline calc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preproicess the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_size= 50\n",
    "x_random_forest_train,y_random_forest_train, _ = pre_process_raw( x_train, y_train , dim_size, zero_pad=True, normalize=False, classes=classes)\n",
    "rf_tests  = [ pre_process_raw( x_test[i], y_test[i] , dim_size, zero_pad=True, normalize=False, classes=classes) for i in range(len(x_test)) ] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first stage RF will learn if it is a known or unknown instance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "knownity_rf =  RandomForestClassifier(n_estimators=960, max_depth=9050,\n",
    "                             random_state=0 )\n",
    "knownity_rf_results = knownity_rf.fit(x_random_forest_train, known_unknown_y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_known_unknown_pred=knownity_rf.predict(x_random_forest_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================HOME Case : home_muhammed_final.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                       unknown     0.968      0.995     0.963     0.979     23896 23768/ 7271/  902/  128\n",
      "                         known     0.968      0.890     0.983     0.934      8173  7271/23768/  128/  902\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.968      0.942     0.973     0.956     32069     0/    0/    0/    0\n",
      "Exact Match ACC : 0.96788 \n",
      "Total Records : 32069 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 0 (0.000)%\n",
      "=============================================================================\n",
      "==================HOME Case : home_os_final.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                       unknown     0.973      0.988     0.976     0.982     14537 14360/ 5074/  357/  177\n",
      "                         known     0.973      0.934     0.966     0.950      5431  5074/14360/  177/  357\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.973      0.961     0.971     0.966     19968     0/    0/    0/    0\n",
      "Exact Match ACC : 0.97326 \n",
      "Total Records : 19968 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 0 (0.000)%\n",
      "=============================================================================\n",
      "==================HOME Case : home_os_final_reduced.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                       unknown     0.968      0.988     0.969     0.978      6563  6483/ 2336/  210/   80\n",
      "                         known     0.968      0.918     0.967     0.942      2546  2336/ 6483/   80/  210\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.968      0.953     0.968     0.960      9109     0/    0/    0/    0\n",
      "Exact Match ACC : 0.96816 \n",
      "Total Records : 9109 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 0 (0.000)%\n",
      "=============================================================================\n",
      "==================HOME Case : home_sk_final.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                       unknown     0.962      0.993     0.945     0.968      3722  3697/ 2466/  216/   25\n",
      "                         known     0.962      0.919     0.990     0.953      2682  2466/ 3697/   25/  216\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.962      0.956     0.967     0.961      6404     0/    0/    0/    0\n",
      "Exact Match ACC : 0.96237 \n",
      "Total Records : 6404 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 0 (0.000)%\n",
      "=============================================================================\n"
     ]
    }
   ],
   "source": [
    "test_known_unknown_predicted = []\n",
    "for i in range(len(rf_tests)) :\n",
    "    print( \"==================HOME Case : %s =============\" % test_names[ i] )\n",
    "    rf_pred=knownity_rf.predict( rf_tests[i][0])\n",
    "    test_known_unknown_predicted.append(rf_pred)\n",
    "    print_info( np.array( known_unknown_y_test[i]), rf_pred, [\"unknown\",\"known\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_indexes_train  = [ i for i in range(len(train_known_unknown_pred)) if train_known_unknown_pred[i][1] ==1 ]\n",
    "\n",
    "x_train_known = x_random_forest_train[known_indexes_train]\n",
    "y_train_known = y_random_forest_train[known_indexes_train]\n",
    "\n",
    "rf_test_known = [] \n",
    "\n",
    "for test_index in range(len(rf_tests)):\n",
    "    known_indexes= [ i for i in range(len(test_known_unknown_predicted[test_index])) if test_known_unknown_predicted[test_index][i][1] ==1 ]\n",
    "    \n",
    "    rf_test_known.append(  (rf_tests[test_index][0][known_indexes], \n",
    "                            rf_tests[test_index][1][known_indexes],\n",
    "                            rf_tests[test_index][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=960, max_depth=9050,\n",
    "                             random_state=0 )\n",
    "t_hist = clf.fit(x_train_known, y_train_known)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================HOME Case : home_muhammed_final.json =============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omid/.conda/envs/iot/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n",
      "/home/omid/.conda/envs/iot/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if __name__ == '__main__':\n",
      "/home/omid/.conda/envs/iot/lib/python3.6/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                        button     1.000      0.857     0.857     0.857         7     6/ 7391/    1/    1\n",
      "              colorTemperature     1.000        nan       nan       nan         0     0/ 7399/    0/    0\n",
      "                       contact     0.999      0.966     0.862     0.911        58    56/ 7332/    9/    2\n",
      "                          lock     0.995      1.000     0.229     0.373        11    11/ 7351/   37/    0\n",
      "                        motion     0.990      0.819     0.852     0.835       238   195/ 7127/   34/   43\n",
      "                          ping     0.992      1.000     0.992     0.996      6952  6951/  390/   57/    1\n",
      "                        switch     1.000      0.957     0.957     0.957        23    22/ 7375/    1/    1\n",
      "                       unknown     0.983      0.897     0.961     0.928       905   812/ 6461/   33/   93\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.995      0.812     0.714     0.732      7399     0/    0/    0/    0\n",
      "Exact Match ACC : 0.97702 \n",
      "Total Records : 7399 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 21 (0.003)%\n",
      "=============================================================================\n",
      "==================HOME Case : home_os_final.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                        button     0.999      0.400     0.667     0.500         5     2/ 5245/    1/    3\n",
      "              colorTemperature     1.000      1.000     1.000     1.000         4     4/ 5247/    0/    0\n",
      "                       contact     0.996      0.915     0.922     0.919       130   119/ 5111/   10/   11\n",
      "                          lock     0.991      0.964     0.380     0.545        28    27/ 5179/   44/    1\n",
      "                        motion     0.978      0.667     0.452     0.539        99    66/ 5072/   80/   33\n",
      "                          ping     0.992      1.000     0.992     0.996      4814  4814/  397/   40/    0\n",
      "                        switch     0.998      0.929     0.542     0.684        14    13/ 5226/   11/    1\n",
      "                       unknown     0.970      0.764     0.973     0.856       624   477/ 4614/   13/  147\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.991      0.830     0.741     0.755      5251     0/    0/    0/    0\n",
      "Exact Match ACC : 0.95734 \n",
      "Total Records : 5251 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 17 (0.003)%\n",
      "=============================================================================\n",
      "==================HOME Case : home_os_final_reduced.json =============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omid/.conda/envs/iot/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n",
      "/home/omid/.conda/envs/iot/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if __name__ == '__main__':\n",
      "/home/omid/.conda/envs/iot/lib/python3.6/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                        button     0.999      0.000     0.000     0.000         1     0/ 2414/    1/    1\n",
      "              colorTemperature     1.000        nan       nan       nan         0     0/ 2416/    0/    0\n",
      "                       contact     0.997      0.942     0.925     0.933        52    49/ 2360/    4/    3\n",
      "                          lock     0.992      0.923     0.400     0.558        13    12/ 2385/   18/    1\n",
      "                        motion     0.978      0.654     0.500     0.567        52    34/ 2330/   34/   18\n",
      "                          ping     0.990      1.000     0.989     0.994      2222  2222/  169/   25/    0\n",
      "                        switch     0.998      1.000     0.375     0.545         3     3/ 2408/    5/    0\n",
      "                       unknown     0.971      0.781     0.966     0.864       288   225/ 2120/    8/   63\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.991      0.663     0.519     0.558      2416     0/    0/    0/    0\n",
      "Exact Match ACC : 0.95820 \n",
      "Total Records : 2416 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 9 (0.004)%\n",
      "=============================================================================\n",
      "==================HOME Case : home_sk_final.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                        button     0.995      0.429     1.000     0.600        21     9/ 2470/    0/   12\n",
      "              colorTemperature     1.000        nan       nan       nan         0     0/ 2491/    0/    0\n",
      "                       contact     0.996      0.902     0.989     0.944       102    92/ 2388/    1/   10\n",
      "                          lock     0.998        nan     0.000     0.000         0     0/ 2487/    4/    0\n",
      "                        motion     0.989      0.706     0.735     0.720        51    36/ 2427/   13/   15\n",
      "                          ping     0.997      1.000     0.997     0.998      2300  2300/  183/    8/    0\n",
      "                        switch     0.997      0.867     0.684     0.765        15    13/ 2470/    6/    2\n",
      "                       unknown     0.991      0.909     0.966     0.937       187   170/ 2298/    6/   17\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.995      0.602     0.671     0.620      2491     0/    0/    0/    0\n",
      "Exact Match ACC : 0.97591 \n",
      "Total Records : 2491 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 13 (0.005)%\n",
      "=============================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omid/.conda/envs/iot/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n",
      "/home/omid/.conda/envs/iot/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if __name__ == '__main__':\n",
      "/home/omid/.conda/envs/iot/lib/python3.6/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(rf_test_known)) :\n",
    "    print( \"==================HOME Case : %s =============\" % test_names[ i] )\n",
    "    rf_pred= clf.predict( rf_test_known[i][0])\n",
    "    print_info( rf_test_known[i][1], rf_pred, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "dim_size =15\n",
    "x_lstm_prossed_train,y_lstm_prossed_train, _ = pre_process_raw( x_train, y_train , dim_size, zero_pad=False, normalize=False,classes=classes)\n",
    "_, y_s_lstm_processed_train ,_ =  pre_process_raw( x_train, y_train_service , dim_size, zero_pad=False, normalize=False,classes=service_classes)\n",
    "# x_lstm_prossed_test,y_lstm_prossed_test, _ = pre_process_raw( x_test, y_test_2 , dim_size, zero_pad=True, normalize=False,classes=classes)\n",
    "lstm_tests  = [ pre_process_raw( x_test[i], y_test[i] , dim_size, zero_pad=False, normalize=False, classes=classes) for i in range(len(x_test)) ] \n",
    "lstm_tests_services  = [ pre_process_raw( x_test[i], y_test_service[i] , dim_size, zero_pad=False, normalize=True, classes=service_classes) for i in range(len(x_test)) ] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_indexes_train  = [ i for i in range(len(train_known_unknown_pred)) if train_known_unknown_pred[i][1] ==1 ]\n",
    "\n",
    "x_lstm_prossed_train = x_lstm_prossed_train[known_indexes_train]\n",
    "y_lstm_prossed_train = y_lstm_prossed_train[known_indexes_train]\n",
    "\n",
    "lstm_tests_known = [] \n",
    "\n",
    "for test_index in range(len(rf_tests)):\n",
    "    known_indexes= [ i for i in range(len(test_known_unknown_predicted[test_index])) if test_known_unknown_predicted[test_index][i][1] ==1 ]\n",
    "    \n",
    "    lstm_tests_known.append(  (lstm_tests[test_index][0][known_indexes], \n",
    "                            lstm_tests[test_index][1][known_indexes],\n",
    "                            lstm_tests[test_index][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_lstm_prossed_test2 = np.expand_dims(x_lstm_prossed_test,axis=1)\n",
    "# x_lstm_prossed_train2 =np.expand_dims(x_lstm_prossed_train,axis=1)\n",
    "\n",
    "for tt  in range( len(lstm_tests_known) ):\n",
    "    lstm_tests_known[tt]= (lstm_tests_known[tt][0].reshape(len(lstm_tests_known[tt][0]),dim_size,1) ,\n",
    "                           lstm_tests_known[tt][1],\n",
    "                           lstm_tests_services[tt][1] )\n",
    "# x_lstm_prossed_test2 = x_lstm_prossed_test.reshape(len(x_lstm_prossed_test),dim_size,1)\n",
    "x_lstm_prossed_train2 =x_lstm_prossed_train.reshape(len(x_lstm_prossed_train),dim_size,1)\n",
    "\n",
    "# y_lstm_prossed_test2 = y_lstm_prossed_test.reshape(len(y_lstm_prossed_test),len(classes),1)\n",
    "# y_lstm_prossed_train2 =y_lstm_prossed_train.reshape(len(y_lstm_prossed_train),len(classes),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 15, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 15, 128)      512         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 15, 128)      512         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 15, 128)      0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 15, 128)      512         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 15, 128)      0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 15, 128)      512         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 15, 128)      49280       dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 15, 128)      0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 15, 128)      49280       conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 15, 128)      0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 15, 128)      512         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 15, 128)      49280       dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 15, 128)      0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, 15, 100)      91600       conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 15, 128)      0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   (None, 15, 100)      40800       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 15, 128)      12928       lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 15, 128)      49280       dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 15, 128)      12928       lstm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 15, 128)      16512       dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 15, 128)      49280       conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 15, 128)      16512       dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 15, 128)      16512       dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 15, 128)      512         conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 15, 128)      16512       dense_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 15, 128)      0           dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 15, 128)      0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 15, 128)      0           dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 1920)         0           dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 15, 128)      0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 1920)         0           dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 128)          245888      flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 15, 128)      49280       dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 128)          245888      flatten_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 128)          0           dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 1920)         0           conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 128)          0           dense_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mergerguy (Concatenate)         (None, 2176)         0           dropout_19[0][0]                 \n",
      "                                                                 flatten_9[0][0]                  \n",
      "                                                                 dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 128)          278656      mergerguy[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 128)          16512       dense_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 128)          16512       dense_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "to_service1 (Dense)             (None, 130)          16770       dense_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "to_service2 (Dense)             (None, 130)          17030       to_service1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "service_output (Dense)          (None, 8)            1048        to_service2[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,361,360\n",
      "Trainable params: 1,360,336\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "24427/24427 [==============================] - 5s 222us/step - loss: 41.0789 - f1_perRow: 0.1408 - f1_perClass: 0.2438 - acc: 0.4151\n",
      "Epoch 2/100\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 23.1859 - f1_perRow: 0.2311 - f1_perClass: 0.5514 - acc: 0.8173\n",
      "Epoch 3/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 12.3100 - f1_perRow: 0.3912 - f1_perClass: 0.8365 - acc: 0.8173\n",
      "Epoch 4/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 10.8088 - f1_perRow: 0.4712 - f1_perClass: 0.8315 - acc: 0.8173\n",
      "Epoch 5/100\n",
      "24427/24427 [==============================] - 1s 35us/step - loss: 9.2991 - f1_perRow: 0.5437 - f1_perClass: 0.8641 - acc: 0.8173\n",
      "Epoch 6/100\n",
      "24427/24427 [==============================] - 1s 35us/step - loss: 8.2308 - f1_perRow: 0.5894 - f1_perClass: 0.8674 - acc: 0.8173\n",
      "Epoch 7/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 7.7272 - f1_perRow: 0.6167 - f1_perClass: 0.8756 - acc: 0.8173\n",
      "Epoch 8/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 7.2661 - f1_perRow: 0.6604 - f1_perClass: 0.8880 - acc: 0.8173\n",
      "Epoch 9/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 6.6659 - f1_perRow: 0.6704 - f1_perClass: 0.8940 - acc: 0.8173\n",
      "Epoch 10/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 6.3963 - f1_perRow: 0.6905 - f1_perClass: 0.8988 - acc: 0.8173\n",
      "Epoch 11/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 6.1473 - f1_perRow: 0.7121 - f1_perClass: 0.9040 - acc: 0.8173\n",
      "Epoch 12/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 5.9350 - f1_perRow: 0.7252 - f1_perClass: 0.9078 - acc: 0.8173\n",
      "Epoch 13/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 5.8245 - f1_perRow: 0.7424 - f1_perClass: 0.9098 - acc: 0.8173\n",
      "Epoch 14/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 5.7463 - f1_perRow: 0.7533 - f1_perClass: 0.9112 - acc: 0.8173\n",
      "Epoch 15/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 5.6585 - f1_perRow: 0.7561 - f1_perClass: 0.9129 - acc: 0.8173\n",
      "Epoch 16/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 5.5560 - f1_perRow: 0.7581 - f1_perClass: 0.9139 - acc: 0.8173\n",
      "Epoch 17/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 5.4923 - f1_perRow: 0.7650 - f1_perClass: 0.9149 - acc: 0.8173\n",
      "Epoch 18/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 5.4304 - f1_perRow: 0.7684 - f1_perClass: 0.9154 - acc: 0.8173\n",
      "Epoch 19/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 5.3711 - f1_perRow: 0.7787 - f1_perClass: 0.9166 - acc: 0.8173\n",
      "Epoch 20/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 5.3201 - f1_perRow: 0.7835 - f1_perClass: 0.9173 - acc: 0.8173\n",
      "Epoch 21/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 5.2729 - f1_perRow: 0.7824 - f1_perClass: 0.9179 - acc: 0.8173\n",
      "Epoch 22/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 5.2442 - f1_perRow: 0.7923 - f1_perClass: 0.9192 - acc: 0.8174\n",
      "Epoch 23/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 5.1842 - f1_perRow: 0.7865 - f1_perClass: 0.9193 - acc: 0.8173\n",
      "Epoch 24/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 5.1633 - f1_perRow: 0.7998 - f1_perClass: 0.9203 - acc: 0.8173\n",
      "Epoch 25/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 5.1048 - f1_perRow: 0.7998 - f1_perClass: 0.9206 - acc: 0.8174\n",
      "Epoch 26/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 5.0856 - f1_perRow: 0.8125 - f1_perClass: 0.9220 - acc: 0.8173\n",
      "Epoch 27/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 5.0728 - f1_perRow: 0.8052 - f1_perClass: 0.9216 - acc: 0.8173\n",
      "Epoch 28/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 5.0408 - f1_perRow: 0.8148 - f1_perClass: 0.9225 - acc: 0.8173\n",
      "Epoch 29/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 5.0084 - f1_perRow: 0.8172 - f1_perClass: 0.9228 - acc: 0.8174\n",
      "Epoch 30/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.9887 - f1_perRow: 0.8188 - f1_perClass: 0.9230 - acc: 0.8174\n",
      "Epoch 31/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.9827 - f1_perRow: 0.8269 - f1_perClass: 0.9238 - acc: 0.8173\n",
      "Epoch 32/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.9711 - f1_perRow: 0.8220 - f1_perClass: 0.9231 - acc: 0.8174\n",
      "Epoch 33/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.9483 - f1_perRow: 0.8258 - f1_perClass: 0.9236 - acc: 0.8173\n",
      "Epoch 34/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.9351 - f1_perRow: 0.8318 - f1_perClass: 0.9243 - acc: 0.8174\n",
      "Epoch 35/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.9270 - f1_perRow: 0.8282 - f1_perClass: 0.9238 - acc: 0.8174\n",
      "Epoch 36/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.9258 - f1_perRow: 0.8318 - f1_perClass: 0.9240 - acc: 0.8174\n",
      "Epoch 37/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.8973 - f1_perRow: 0.8324 - f1_perClass: 0.9243 - acc: 0.8173\n",
      "Epoch 38/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.8990 - f1_perRow: 0.8299 - f1_perClass: 0.9240 - acc: 0.8174\n",
      "Epoch 39/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.8771 - f1_perRow: 0.8371 - f1_perClass: 0.9249 - acc: 0.8174\n",
      "Epoch 40/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.8729 - f1_perRow: 0.8381 - f1_perClass: 0.9247 - acc: 0.8174\n",
      "Epoch 41/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.8859 - f1_perRow: 0.8347 - f1_perClass: 0.9244 - acc: 0.8174\n",
      "Epoch 42/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.8648 - f1_perRow: 0.8347 - f1_perClass: 0.9247 - acc: 0.8173\n",
      "Epoch 43/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.8688 - f1_perRow: 0.8384 - f1_perClass: 0.9251 - acc: 0.8174\n",
      "Epoch 44/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.8481 - f1_perRow: 0.8395 - f1_perClass: 0.9250 - acc: 0.8174\n",
      "Epoch 45/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.8380 - f1_perRow: 0.8417 - f1_perClass: 0.9252 - acc: 0.8173\n",
      "Epoch 46/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.8420 - f1_perRow: 0.8390 - f1_perClass: 0.9252 - acc: 0.8174\n",
      "Epoch 47/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.8408 - f1_perRow: 0.8424 - f1_perClass: 0.9254 - acc: 0.8174\n",
      "Epoch 48/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.8081 - f1_perRow: 0.8435 - f1_perClass: 0.9256 - acc: 0.8174\n",
      "Epoch 49/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.8250 - f1_perRow: 0.8412 - f1_perClass: 0.9254 - acc: 0.8174\n",
      "Epoch 50/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.8167 - f1_perRow: 0.8443 - f1_perClass: 0.9256 - acc: 0.8174\n",
      "Epoch 51/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.8297 - f1_perRow: 0.8406 - f1_perClass: 0.9256 - acc: 0.8175\n",
      "Epoch 52/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.8131 - f1_perRow: 0.8444 - f1_perClass: 0.9257 - acc: 0.8176\n",
      "Epoch 53/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.8126 - f1_perRow: 0.8471 - f1_perClass: 0.9260 - acc: 0.8175\n",
      "Epoch 54/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.8192 - f1_perRow: 0.8384 - f1_perClass: 0.9256 - acc: 0.8174\n",
      "Epoch 55/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.8076 - f1_perRow: 0.8437 - f1_perClass: 0.9257 - acc: 0.8174\n",
      "Epoch 56/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.8045 - f1_perRow: 0.8468 - f1_perClass: 0.9260 - acc: 0.8174\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.8123 - f1_perRow: 0.8425 - f1_perClass: 0.9255 - acc: 0.8174\n",
      "Epoch 58/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.8077 - f1_perRow: 0.8444 - f1_perClass: 0.9259 - acc: 0.8174\n",
      "Epoch 59/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.8009 - f1_perRow: 0.8436 - f1_perClass: 0.9259 - acc: 0.8174\n",
      "Epoch 60/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7821 - f1_perRow: 0.8470 - f1_perClass: 0.9262 - acc: 0.8174\n",
      "Epoch 61/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7791 - f1_perRow: 0.8463 - f1_perClass: 0.9261 - acc: 0.8174\n",
      "Epoch 62/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7766 - f1_perRow: 0.8453 - f1_perClass: 0.9259 - acc: 0.8175\n",
      "Epoch 63/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7761 - f1_perRow: 0.8480 - f1_perClass: 0.9262 - acc: 0.8175\n",
      "Epoch 64/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7868 - f1_perRow: 0.8463 - f1_perClass: 0.9261 - acc: 0.8173\n",
      "Epoch 65/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7771 - f1_perRow: 0.8498 - f1_perClass: 0.9265 - acc: 0.8174\n",
      "Epoch 66/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7626 - f1_perRow: 0.8469 - f1_perClass: 0.9262 - acc: 0.8173\n",
      "Epoch 67/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7626 - f1_perRow: 0.8463 - f1_perClass: 0.9262 - acc: 0.8175\n",
      "Epoch 68/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7699 - f1_perRow: 0.8495 - f1_perClass: 0.9265 - acc: 0.8174\n",
      "Epoch 69/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7562 - f1_perRow: 0.8485 - f1_perClass: 0.9263 - acc: 0.8176\n",
      "Epoch 70/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7617 - f1_perRow: 0.8487 - f1_perClass: 0.9263 - acc: 0.8175\n",
      "Epoch 71/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7600 - f1_perRow: 0.8513 - f1_perClass: 0.9268 - acc: 0.8183\n",
      "Epoch 72/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7508 - f1_perRow: 0.8472 - f1_perClass: 0.9264 - acc: 0.8187\n",
      "Epoch 73/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7360 - f1_perRow: 0.8492 - f1_perClass: 0.9265 - acc: 0.8177\n",
      "Epoch 74/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7564 - f1_perRow: 0.8522 - f1_perClass: 0.9267 - acc: 0.8175\n",
      "Epoch 75/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7382 - f1_perRow: 0.8508 - f1_perClass: 0.9265 - acc: 0.8175\n",
      "Epoch 76/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7392 - f1_perRow: 0.8509 - f1_perClass: 0.9265 - acc: 0.8176\n",
      "Epoch 77/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7410 - f1_perRow: 0.8504 - f1_perClass: 0.9266 - acc: 0.8184\n",
      "Epoch 78/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7396 - f1_perRow: 0.8489 - f1_perClass: 0.9265 - acc: 0.8197\n",
      "Epoch 79/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7469 - f1_perRow: 0.8519 - f1_perClass: 0.9266 - acc: 0.8182\n",
      "Epoch 80/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7478 - f1_perRow: 0.8510 - f1_perClass: 0.9266 - acc: 0.8181\n",
      "Epoch 81/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7312 - f1_perRow: 0.8519 - f1_perClass: 0.9267 - acc: 0.8180\n",
      "Epoch 82/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7460 - f1_perRow: 0.8531 - f1_perClass: 0.9269 - acc: 0.8186\n",
      "Epoch 83/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7352 - f1_perRow: 0.8483 - f1_perClass: 0.9265 - acc: 0.8188\n",
      "Epoch 84/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7386 - f1_perRow: 0.8523 - f1_perClass: 0.9269 - acc: 0.8190\n",
      "Epoch 85/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7243 - f1_perRow: 0.8546 - f1_perClass: 0.9270 - acc: 0.8179\n",
      "Epoch 86/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7249 - f1_perRow: 0.8507 - f1_perClass: 0.9266 - acc: 0.8182\n",
      "Epoch 87/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7359 - f1_perRow: 0.8546 - f1_perClass: 0.9272 - acc: 0.8191\n",
      "Epoch 88/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7086 - f1_perRow: 0.8533 - f1_perClass: 0.9269 - acc: 0.8191\n",
      "Epoch 89/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7256 - f1_perRow: 0.8514 - f1_perClass: 0.9268 - acc: 0.8183\n",
      "Epoch 90/100\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 4.7315 - f1_perRow: 0.8551 - f1_perClass: 0.9270 - acc: 0.8182\n",
      "Epoch 91/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7217 - f1_perRow: 0.8513 - f1_perClass: 0.9268 - acc: 0.8186\n",
      "Epoch 92/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7268 - f1_perRow: 0.8530 - f1_perClass: 0.9270 - acc: 0.8190\n",
      "Epoch 93/100\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 4.7104 - f1_perRow: 0.8553 - f1_perClass: 0.9272 - acc: 0.8186\n",
      "Epoch 94/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7222 - f1_perRow: 0.8516 - f1_perClass: 0.9269 - acc: 0.8190\n",
      "Epoch 95/100\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 4.7167 - f1_perRow: 0.8519 - f1_perClass: 0.9268 - acc: 0.8191\n",
      "Epoch 96/100\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 4.7024 - f1_perRow: 0.8552 - f1_perClass: 0.9273 - acc: 0.8191\n",
      "Epoch 97/100\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 4.7093 - f1_perRow: 0.8517 - f1_perClass: 0.9268 - acc: 0.8185\n",
      "Epoch 98/100\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7103 - f1_perRow: 0.8554 - f1_perClass: 0.9272 - acc: 0.8184\n",
      "Epoch 99/100\n",
      "24427/24427 [==============================] - 1s 38us/step - loss: 4.7077 - f1_perRow: 0.8530 - f1_perClass: 0.9270 - acc: 0.8197\n",
      "Epoch 100/100\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 4.7060 - f1_perRow: 0.8535 - f1_perClass: 0.9270 - acc: 0.8198\n"
     ]
    }
   ],
   "source": [
    "## Bidirectional LSTM :D \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Flatten\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import Input, merge, concatenate, Conv2D, MaxPooling2D, Activation, UpSampling2D, Dropout, Conv2DTranspose, UpSampling2D, Lambda\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization as bn\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.merge import add\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers.merge import add\n",
    "from keras.layers import Dense,Conv1D,Dropout,Activation,BatchNormalization,MaxPooling1D,Flatten,Masking,TimeDistributed\n",
    "from keras.layers.recurrent import LSTM,GRU,SimpleRNN\n",
    "from keras.models import Input,Sequential,Model\n",
    "from keras.layers.merge import add\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import MSE,MSLE\n",
    "model2 = Sequential()\n",
    "\n",
    "\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def f1_perRow(y_true, y_pred):\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "\n",
    "def f1_perClass(y_true, y_pred):\n",
    "\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss_perClass(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "def f1_loss_perRow(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inputs  = Input( ( dim_size,1 ) )\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(inputs)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# bi_d_1 = Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat') (inputs)\n",
    "lstm_1 =  LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True)(out)\n",
    "# lstm_2 = LSTM(30 ,  recurrent_dropout=0.14, return_sequences=True)(lstm_1)\n",
    "\n",
    "bi_d_1 =Dense(128, activation='relu')  (lstm_1)\n",
    "lstm_1 =  Dense(128, activation='relu')(bi_d_1)\n",
    "lstm_2 = Dense(128, activation='relu')(lstm_1)\n",
    "\n",
    "\n",
    "\n",
    "# td_1    = TimeDistributed(Dense(256, activation='relu'))(lstm_2)\n",
    "# dout_1  = Dropout(0.1)(td_1)\n",
    "dout_1  = Dropout(0.1)(lstm_2)\n",
    "flt_1   = Flatten()(dout_1)\n",
    "dense_1 = Dense(128, activation='relu')(flt_1)\n",
    "dout_2  = Dropout(0.2)(dense_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# bi_d_1 = Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat') (inputs)\n",
    "lstm_1 =  LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True)(inputs)\n",
    "# lstm_1 = LSTM(40 ,  recurrent_dropout=0.14, return_sequences=True)(lstm_1)\n",
    "\n",
    "bi_d_raw_1 =Dense(128, activation='relu')  (lstm_1)\n",
    "lstm_raw_1 =  Dense(128, activation='relu')(bi_d_raw_1)\n",
    "lstm_raw_2 = Dense(128, activation='relu')(lstm_raw_1)\n",
    "\n",
    "dout_1  = Dropout(0.1)(lstm_raw_2)\n",
    "flt_1   = Flatten()(dout_1)\n",
    "dense_1 = Dense(128, activation='relu')(flt_1)\n",
    "dout_3  = Dropout(0.2)(dense_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(inputs)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# out = Flatten()(out)\n",
    "# out = MaxPooling1D(2,padding='same', name ='pooling')(out)\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# out = Flatten()(out)\n",
    "# out = MaxPooling1D(2,padding='same', name ='pooling2')(out)\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# out = Flatten()(out)\n",
    "# out = MaxPooling1D(2,padding='same', name ='pooling')(out)\n",
    "\n",
    "\n",
    "\n",
    "# fl_out_1 = Flatten()(dout_2)\n",
    "\n",
    "fl_out_cnn = Flatten()(out)\n",
    "\n",
    "# out_new = concatenate( [fl_out_1, fl_out_cnn] , name='mergerguy')\n",
    "out_new = concatenate( [dout_2, fl_out_cnn,dout_3] , name='mergerguy')\n",
    "\n",
    "dens_out_1 = Dense( 128, activation='relu' )(out_new)\n",
    "dens_out_2 = Dense( 128, activation='relu' )(dens_out_1)\n",
    "dens_out_3 = Dense( 128, activation='relu' )(dens_out_2)\n",
    "\n",
    "# fl2  = Flatten()(out_new)\n",
    "\n",
    "out_put_final = Dense(len(classes), activation='sigmoid', name='Event_output')(dens_out_3)\n",
    "\n",
    "toService_1 = Dense( 130, name=\"to_service1\" )(dens_out_3)\n",
    "toService_1 = Dense( 130, name=\"to_service2\" )(toService_1)\n",
    "\n",
    "service_output = Dense(len(classes  ), activation=\"sigmoid\", name = 'service_output')(toService_1)\n",
    "\n",
    "\n",
    "\n",
    "model2 = Model(inputs=[inputs], outputs=[service_output])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model2.add(Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat'))\n",
    "# model2.add( LSTM(60 ,  recurrent_dropout=0.04, return_sequences=True))\n",
    "# model2.add( LSTM(30 ,  recurrent_dropout=0.14, return_sequences=True))\n",
    "# # model2.add(Bidirectional( LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True),merge_mode='concat'))\n",
    "# model2.add(TimeDistributed(Dense(256, activation='relu')))\n",
    "# model2.add(Dropout(0.1))\n",
    "# model2.add(Flatten())\n",
    "# model2.add(Dense(128, activation='relu'))\n",
    "# model2.add(Dropout(0.2))\n",
    "# model2.add(Dense(len(classes), activation='sigmoid'))\n",
    "\n",
    " \n",
    "weights = [\n",
    "1.0/(57.0 / len(y_train)),\n",
    "1.0/(19.0 / len(y_train)),\n",
    "1.0/(7.0 / len(y_train)),\n",
    "1.0/(14.0 / len(y_train)),\n",
    "1.0/(6.0 / len(y_train)),\n",
    "1.0/(176.0 / len(y_train)),\n",
    "1.0/(27.0 / len(y_train)),\n",
    "1.0/(35.0 / len(y_train)),\n",
    "1.0/(371.0 / len(y_train)),\n",
    "1.0/(11111.0 / len(y_train)),\n",
    "1.0/(4842.0 / len(y_train)),\n",
    "1.0/(119.0 / len(y_train)),\n",
    "1.0/(21.0 / len(y_train)),\n",
    "1.0/(1168.0 / len(y_train)),\n",
    "1.0/(63.0 / len(y_train)),\n",
    "1.0/(13305.0 / len(y_train)),\n",
    "1.0/(11111.0 / len(y_train)),\n",
    "]\n",
    "    \n",
    "\n",
    "\n",
    "# model2.compile(loss=weighted_categorical_crossentropy(weights=weights), optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "losses = {\n",
    "#     \"service_output\": f1_loss_perClass ,\n",
    "    \"service_output\": f1_loss_perRow ,\n",
    "    \"service_output\": \"categorical_crossentropy\",\n",
    "}\n",
    "lossWeights = {#\"service_output\": 20,\n",
    "               \"service_output\": 30.0 ,\n",
    "    \"service_output\": 20}\n",
    " \n",
    "\n",
    "\n",
    "model2.compile(loss=losses,loss_weights=lossWeights, optimizer='adam', metrics=[f1_perRow,f1_perClass,'acc'])\n",
    "# model2.compile(loss=losses, loss_weights=lossWeights, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('IoTDownNet', monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "print(model2.summary())\n",
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=100, batch_size=12500, shuffle=True, callbacks=callbacks_list)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 4.7279 - f1_perRow: 0.8576 - f1_perClass: 0.9273 - acc: 0.8188\n",
      "Epoch 2/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7058 - f1_perRow: 0.8533 - f1_perClass: 0.9268 - acc: 0.8184\n",
      "Epoch 3/200\n",
      "24427/24427 [==============================] - 1s 35us/step - loss: 4.7014 - f1_perRow: 0.8500 - f1_perClass: 0.9268 - acc: 0.8191\n",
      "Epoch 4/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7062 - f1_perRow: 0.8528 - f1_perClass: 0.9272 - acc: 0.8193\n",
      "Epoch 5/200\n",
      "24427/24427 [==============================] - 1s 35us/step - loss: 4.7033 - f1_perRow: 0.8542 - f1_perClass: 0.9271 - acc: 0.8195\n",
      "Epoch 6/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6969 - f1_perRow: 0.8547 - f1_perClass: 0.9272 - acc: 0.8192\n",
      "Epoch 7/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7076 - f1_perRow: 0.8545 - f1_perClass: 0.9270 - acc: 0.8194\n",
      "Epoch 8/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6946 - f1_perRow: 0.8565 - f1_perClass: 0.9275 - acc: 0.8198\n",
      "Epoch 9/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7062 - f1_perRow: 0.8532 - f1_perClass: 0.9269 - acc: 0.8204\n",
      "Epoch 10/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6939 - f1_perRow: 0.8583 - f1_perClass: 0.9277 - acc: 0.8201\n",
      "Epoch 11/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7064 - f1_perRow: 0.8550 - f1_perClass: 0.9274 - acc: 0.8204\n",
      "Epoch 12/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7038 - f1_perRow: 0.8523 - f1_perClass: 0.9267 - acc: 0.8202\n",
      "Epoch 13/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6937 - f1_perRow: 0.8581 - f1_perClass: 0.9277 - acc: 0.8195\n",
      "Epoch 14/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6969 - f1_perRow: 0.8569 - f1_perClass: 0.9275 - acc: 0.8195\n",
      "Epoch 15/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6975 - f1_perRow: 0.8564 - f1_perClass: 0.9273 - acc: 0.8195\n",
      "Epoch 16/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6851 - f1_perRow: 0.8569 - f1_perClass: 0.9275 - acc: 0.8194\n",
      "Epoch 17/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6948 - f1_perRow: 0.8536 - f1_perClass: 0.9270 - acc: 0.8190\n",
      "Epoch 18/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6850 - f1_perRow: 0.8539 - f1_perClass: 0.9273 - acc: 0.8187\n",
      "Epoch 19/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6770 - f1_perRow: 0.8581 - f1_perClass: 0.9277 - acc: 0.8190\n",
      "Epoch 20/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6749 - f1_perRow: 0.8574 - f1_perClass: 0.9273 - acc: 0.8186\n",
      "Epoch 21/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6693 - f1_perRow: 0.8575 - f1_perClass: 0.9275 - acc: 0.8185\n",
      "Epoch 22/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6639 - f1_perRow: 0.8583 - f1_perClass: 0.9276 - acc: 0.8188\n",
      "Epoch 23/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6709 - f1_perRow: 0.8577 - f1_perClass: 0.9276 - acc: 0.8193\n",
      "Epoch 24/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6672 - f1_perRow: 0.8568 - f1_perClass: 0.9276 - acc: 0.8198\n",
      "Epoch 25/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6726 - f1_perRow: 0.8582 - f1_perClass: 0.9275 - acc: 0.8199\n",
      "Epoch 26/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6569 - f1_perRow: 0.8596 - f1_perClass: 0.9278 - acc: 0.8204\n",
      "Epoch 27/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6828 - f1_perRow: 0.8587 - f1_perClass: 0.9275 - acc: 0.8199\n",
      "Epoch 28/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6768 - f1_perRow: 0.8569 - f1_perClass: 0.9274 - acc: 0.8206\n",
      "Epoch 29/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6765 - f1_perRow: 0.8579 - f1_perClass: 0.9277 - acc: 0.8211\n",
      "Epoch 30/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6771 - f1_perRow: 0.8574 - f1_perClass: 0.9275 - acc: 0.8209\n",
      "Epoch 31/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6543 - f1_perRow: 0.8588 - f1_perClass: 0.9280 - acc: 0.8210\n",
      "Epoch 32/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6684 - f1_perRow: 0.8574 - f1_perClass: 0.9276 - acc: 0.8207\n",
      "Epoch 33/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6577 - f1_perRow: 0.8582 - f1_perClass: 0.9276 - acc: 0.8209\n",
      "Epoch 34/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6634 - f1_perRow: 0.8599 - f1_perClass: 0.9279 - acc: 0.8209\n",
      "Epoch 35/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6569 - f1_perRow: 0.8596 - f1_perClass: 0.9278 - acc: 0.8207\n",
      "Epoch 36/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6650 - f1_perRow: 0.8572 - f1_perClass: 0.9273 - acc: 0.8210\n",
      "Epoch 37/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6434 - f1_perRow: 0.8612 - f1_perClass: 0.9281 - acc: 0.8211\n",
      "Epoch 38/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6638 - f1_perRow: 0.8582 - f1_perClass: 0.9277 - acc: 0.8211\n",
      "Epoch 39/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6549 - f1_perRow: 0.8557 - f1_perClass: 0.9275 - acc: 0.8211\n",
      "Epoch 40/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6604 - f1_perRow: 0.8587 - f1_perClass: 0.9279 - acc: 0.8213\n",
      "Epoch 41/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6616 - f1_perRow: 0.8588 - f1_perClass: 0.9276 - acc: 0.8212\n",
      "Epoch 42/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6648 - f1_perRow: 0.8612 - f1_perClass: 0.9281 - acc: 0.8210\n",
      "Epoch 43/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6537 - f1_perRow: 0.8593 - f1_perClass: 0.9277 - acc: 0.8212\n",
      "Epoch 44/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6518 - f1_perRow: 0.8593 - f1_perClass: 0.9279 - acc: 0.8220\n",
      "Epoch 45/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6692 - f1_perRow: 0.8568 - f1_perClass: 0.9278 - acc: 0.8220\n",
      "Epoch 46/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6483 - f1_perRow: 0.8576 - f1_perClass: 0.9277 - acc: 0.8215\n",
      "Epoch 47/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6588 - f1_perRow: 0.8603 - f1_perClass: 0.9280 - acc: 0.8222\n",
      "Epoch 48/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6557 - f1_perRow: 0.8604 - f1_perClass: 0.9277 - acc: 0.8216\n",
      "Epoch 49/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6663 - f1_perRow: 0.8577 - f1_perClass: 0.9277 - acc: 0.8218\n",
      "Epoch 50/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6653 - f1_perRow: 0.8591 - f1_perClass: 0.9277 - acc: 0.8222\n",
      "Epoch 51/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6446 - f1_perRow: 0.8620 - f1_perClass: 0.9283 - acc: 0.8223\n",
      "Epoch 52/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6429 - f1_perRow: 0.8591 - f1_perClass: 0.9278 - acc: 0.8220\n",
      "Epoch 53/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6550 - f1_perRow: 0.8588 - f1_perClass: 0.9276 - acc: 0.8222\n",
      "Epoch 54/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6507 - f1_perRow: 0.8646 - f1_perClass: 0.9282 - acc: 0.8219\n",
      "Epoch 55/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6545 - f1_perRow: 0.8606 - f1_perClass: 0.9278 - acc: 0.8221\n",
      "Epoch 56/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6447 - f1_perRow: 0.8597 - f1_perClass: 0.9277 - acc: 0.8226\n",
      "Epoch 57/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6479 - f1_perRow: 0.8625 - f1_perClass: 0.9280 - acc: 0.8215\n",
      "Epoch 58/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6481 - f1_perRow: 0.8620 - f1_perClass: 0.9280 - acc: 0.8213\n",
      "Epoch 59/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6517 - f1_perRow: 0.8585 - f1_perClass: 0.9277 - acc: 0.8220\n",
      "Epoch 60/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.6820 - f1_perRow: 0.8565 - f1_perClass: 0.9275 - acc: 0.8218\n",
      "Epoch 61/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.7629 - f1_perRow: 0.8482 - f1_perClass: 0.9263 - acc: 0.8367\n",
      "Epoch 62/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 5.6288 - f1_perRow: 0.7851 - f1_perClass: 0.9196 - acc: 0.8549\n",
      "Epoch 63/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 7.8786 - f1_perRow: 0.6857 - f1_perClass: 0.8990 - acc: 0.8405\n",
      "Epoch 64/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 6.8921 - f1_perRow: 0.7366 - f1_perClass: 0.9048 - acc: 0.8209\n",
      "Epoch 65/200\n",
      "24427/24427 [==============================] - 1s 38us/step - loss: 6.9108 - f1_perRow: 0.6684 - f1_perClass: 0.9020 - acc: 0.8528\n",
      "Epoch 66/200\n",
      "24427/24427 [==============================] - 1s 38us/step - loss: 7.1197 - f1_perRow: 0.6161 - f1_perClass: 0.8922 - acc: 0.8587\n",
      "Epoch 67/200\n",
      "24427/24427 [==============================] - 1s 39us/step - loss: 6.6928 - f1_perRow: 0.6567 - f1_perClass: 0.9017 - acc: 0.8705\n",
      "Epoch 68/200\n",
      "24427/24427 [==============================] - 1s 40us/step - loss: 6.4460 - f1_perRow: 0.6840 - f1_perClass: 0.9050 - acc: 0.8647\n",
      "Epoch 69/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 6.2751 - f1_perRow: 0.6797 - f1_perClass: 0.9027 - acc: 0.8654\n",
      "Epoch 70/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 6.0112 - f1_perRow: 0.6965 - f1_perClass: 0.9066 - acc: 0.8579\n",
      "Epoch 71/200\n",
      "24427/24427 [==============================] - 1s 38us/step - loss: 5.7146 - f1_perRow: 0.7046 - f1_perClass: 0.9114 - acc: 0.8550\n",
      "Epoch 72/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 5.5610 - f1_perRow: 0.7224 - f1_perClass: 0.9142 - acc: 0.8496\n",
      "Epoch 73/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 5.4622 - f1_perRow: 0.7302 - f1_perClass: 0.9160 - acc: 0.8584\n",
      "Epoch 74/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 5.4176 - f1_perRow: 0.7282 - f1_perClass: 0.9162 - acc: 0.8668\n",
      "Epoch 75/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 5.3529 - f1_perRow: 0.7307 - f1_perClass: 0.9170 - acc: 0.8612\n",
      "Epoch 76/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 5.1653 - f1_perRow: 0.7302 - f1_perClass: 0.9212 - acc: 0.8601\n",
      "Epoch 77/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.9978 - f1_perRow: 0.7413 - f1_perClass: 0.9240 - acc: 0.8551\n",
      "Epoch 78/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.9816 - f1_perRow: 0.7449 - f1_perClass: 0.9242 - acc: 0.8502\n",
      "Epoch 79/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.9403 - f1_perRow: 0.7495 - f1_perClass: 0.9250 - acc: 0.8575\n",
      "Epoch 80/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 4.9046 - f1_perRow: 0.7506 - f1_perClass: 0.9255 - acc: 0.8630\n",
      "Epoch 81/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 4.8413 - f1_perRow: 0.7493 - f1_perClass: 0.9258 - acc: 0.8671\n",
      "Epoch 82/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 4.5749 - f1_perRow: 0.7472 - f1_perClass: 0.9313 - acc: 0.8702\n",
      "Epoch 83/200\n",
      "24427/24427 [==============================] - 1s 39us/step - loss: 4.3132 - f1_perRow: 0.7532 - f1_perClass: 0.9378 - acc: 0.8720\n",
      "Epoch 84/200\n",
      "24427/24427 [==============================] - 1s 38us/step - loss: 4.2441 - f1_perRow: 0.7612 - f1_perClass: 0.9403 - acc: 0.8712\n",
      "Epoch 85/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 4.2095 - f1_perRow: 0.7621 - f1_perClass: 0.9409 - acc: 0.8724\n",
      "Epoch 86/200\n",
      "24427/24427 [==============================] - 1s 38us/step - loss: 4.1746 - f1_perRow: 0.7600 - f1_perClass: 0.9407 - acc: 0.8757\n",
      "Epoch 87/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 4.1290 - f1_perRow: 0.7629 - f1_perClass: 0.9413 - acc: 0.8799\n",
      "Epoch 88/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 4.0645 - f1_perRow: 0.7676 - f1_perClass: 0.9432 - acc: 0.8947\n",
      "Epoch 89/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 3.9266 - f1_perRow: 0.7808 - f1_perClass: 0.9469 - acc: 0.9056\n",
      "Epoch 90/200\n",
      "24427/24427 [==============================] - 1s 38us/step - loss: 3.6858 - f1_perRow: 0.8014 - f1_perClass: 0.9510 - acc: 0.9067\n",
      "Epoch 91/200\n",
      "24427/24427 [==============================] - 1s 39us/step - loss: 3.6398 - f1_perRow: 0.8057 - f1_perClass: 0.9525 - acc: 0.9081\n",
      "Epoch 92/200\n",
      "24427/24427 [==============================] - 1s 39us/step - loss: 3.5886 - f1_perRow: 0.8082 - f1_perClass: 0.9541 - acc: 0.9104\n",
      "Epoch 93/200\n",
      "24427/24427 [==============================] - 1s 41us/step - loss: 3.5857 - f1_perRow: 0.8083 - f1_perClass: 0.9543 - acc: 0.9148\n",
      "Epoch 94/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 3.5619 - f1_perRow: 0.8103 - f1_perClass: 0.9544 - acc: 0.9176\n",
      "Epoch 95/200\n",
      "24427/24427 [==============================] - 1s 41us/step - loss: 3.5591 - f1_perRow: 0.8115 - f1_perClass: 0.9543 - acc: 0.9180\n",
      "Epoch 96/200\n",
      "24427/24427 [==============================] - 1s 38us/step - loss: 3.5274 - f1_perRow: 0.8126 - f1_perClass: 0.9544 - acc: 0.9169\n",
      "Epoch 97/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 3.5120 - f1_perRow: 0.8117 - f1_perClass: 0.9542 - acc: 0.9163\n",
      "Epoch 98/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 3.5011 - f1_perRow: 0.8116 - f1_perClass: 0.9540 - acc: 0.9169\n",
      "Epoch 99/200\n",
      "24427/24427 [==============================] - 1s 41us/step - loss: 3.4867 - f1_perRow: 0.8205 - f1_perClass: 0.9553 - acc: 0.9185\n",
      "Epoch 100/200\n",
      "24427/24427 [==============================] - 1s 38us/step - loss: 3.3881 - f1_perRow: 0.8302 - f1_perClass: 0.9571 - acc: 0.9203\n",
      "Epoch 101/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 3.3770 - f1_perRow: 0.8319 - f1_perClass: 0.9577 - acc: 0.9202\n",
      "Epoch 102/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 3.3610 - f1_perRow: 0.8321 - f1_perClass: 0.9576 - acc: 0.9205\n",
      "Epoch 103/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 3.3349 - f1_perRow: 0.8284 - f1_perClass: 0.9570 - acc: 0.9192\n",
      "Epoch 104/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 3.3136 - f1_perRow: 0.8256 - f1_perClass: 0.9571 - acc: 0.9201\n",
      "Epoch 105/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 3.3158 - f1_perRow: 0.8200 - f1_perClass: 0.9566 - acc: 0.9196\n",
      "Epoch 106/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 3.3067 - f1_perRow: 0.8210 - f1_perClass: 0.9561 - acc: 0.9200\n",
      "Epoch 107/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 3.3010 - f1_perRow: 0.8234 - f1_perClass: 0.9563 - acc: 0.9205\n",
      "Epoch 108/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 3.2888 - f1_perRow: 0.8257 - f1_perClass: 0.9572 - acc: 0.9205\n",
      "Epoch 109/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 3.2674 - f1_perRow: 0.8264 - f1_perClass: 0.9574 - acc: 0.9223\n",
      "Epoch 110/200\n",
      "24427/24427 [==============================] - 1s 41us/step - loss: 3.2673 - f1_perRow: 0.8187 - f1_perClass: 0.9568 - acc: 0.9219\n",
      "Epoch 111/200\n",
      "24427/24427 [==============================] - 1s 39us/step - loss: 3.2712 - f1_perRow: 0.8164 - f1_perClass: 0.9564 - acc: 0.9223\n",
      "Epoch 112/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 3.2601 - f1_perRow: 0.8302 - f1_perClass: 0.9584 - acc: 0.9226\n",
      "Epoch 113/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24427/24427 [==============================] - 1s 36us/step - loss: 3.2604 - f1_perRow: 0.8309 - f1_perClass: 0.9579 - acc: 0.9231\n",
      "Epoch 114/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 3.2515 - f1_perRow: 0.8326 - f1_perClass: 0.9587 - acc: 0.9236\n",
      "Epoch 115/200\n",
      "24427/24427 [==============================] - 1s 38us/step - loss: 3.2036 - f1_perRow: 0.8373 - f1_perClass: 0.9597 - acc: 0.9257\n",
      "Epoch 116/200\n",
      "24427/24427 [==============================] - 1s 38us/step - loss: 3.1905 - f1_perRow: 0.8329 - f1_perClass: 0.9589 - acc: 0.9266\n",
      "Epoch 117/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 3.1778 - f1_perRow: 0.8311 - f1_perClass: 0.9593 - acc: 0.9264\n",
      "Epoch 118/200\n",
      "24427/24427 [==============================] - 1s 40us/step - loss: 3.1659 - f1_perRow: 0.8299 - f1_perClass: 0.9587 - acc: 0.9300\n",
      "Epoch 119/200\n",
      "24427/24427 [==============================] - 1s 38us/step - loss: 3.1622 - f1_perRow: 0.8326 - f1_perClass: 0.9600 - acc: 0.9342\n",
      "Epoch 120/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 3.1464 - f1_perRow: 0.8367 - f1_perClass: 0.9596 - acc: 0.9395\n",
      "Epoch 121/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 3.1311 - f1_perRow: 0.8398 - f1_perClass: 0.9606 - acc: 0.9499\n",
      "Epoch 122/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 3.1316 - f1_perRow: 0.8332 - f1_perClass: 0.9599 - acc: 0.9539\n",
      "Epoch 123/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 3.0931 - f1_perRow: 0.8295 - f1_perClass: 0.9587 - acc: 0.9587\n",
      "Epoch 124/200\n",
      "24427/24427 [==============================] - 1s 41us/step - loss: 3.0243 - f1_perRow: 0.8097 - f1_perClass: 0.9594 - acc: 0.9680\n",
      "Epoch 125/200\n",
      "24427/24427 [==============================] - 1s 38us/step - loss: 2.7273 - f1_perRow: 0.8170 - f1_perClass: 0.9485 - acc: 0.9714\n",
      "Epoch 126/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 2.6095 - f1_perRow: 0.8419 - f1_perClass: 0.9548 - acc: 0.9702\n",
      "Epoch 127/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 2.5667 - f1_perRow: 0.8506 - f1_perClass: 0.9747 - acc: 0.9783\n",
      "Epoch 128/200\n",
      "24427/24427 [==============================] - 1s 41us/step - loss: 2.5554 - f1_perRow: 0.8553 - f1_perClass: 0.9764 - acc: 0.9780\n",
      "Epoch 129/200\n",
      "24427/24427 [==============================] - 1s 38us/step - loss: 2.5602 - f1_perRow: 0.8520 - f1_perClass: 0.9757 - acc: 0.9783\n",
      "Epoch 130/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 2.5279 - f1_perRow: 0.8628 - f1_perClass: 0.9777 - acc: 0.9805\n",
      "Epoch 131/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 2.5398 - f1_perRow: 0.8660 - f1_perClass: 0.9778 - acc: 0.9789\n",
      "Epoch 132/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 2.5366 - f1_perRow: 0.8638 - f1_perClass: 0.9770 - acc: 0.9790\n",
      "Epoch 133/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 2.4829 - f1_perRow: 0.8684 - f1_perClass: 0.9784 - acc: 0.9805\n",
      "Epoch 134/200\n",
      "24427/24427 [==============================] - 1s 38us/step - loss: 2.4771 - f1_perRow: 0.8674 - f1_perClass: 0.9788 - acc: 0.9800\n",
      "Epoch 135/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 2.4783 - f1_perRow: 0.8646 - f1_perClass: 0.9781 - acc: 0.9799\n",
      "Epoch 136/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 2.4623 - f1_perRow: 0.8663 - f1_perClass: 0.9781 - acc: 0.9806\n",
      "Epoch 137/200\n",
      "24427/24427 [==============================] - 1s 38us/step - loss: 2.4414 - f1_perRow: 0.8680 - f1_perClass: 0.9784 - acc: 0.9808\n",
      "Epoch 138/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 2.4247 - f1_perRow: 0.8649 - f1_perClass: 0.9778 - acc: 0.9815\n",
      "Epoch 139/200\n",
      "24427/24427 [==============================] - 1s 38us/step - loss: 2.4219 - f1_perRow: 0.8607 - f1_perClass: 0.9776 - acc: 0.9812\n",
      "Epoch 140/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 2.3896 - f1_perRow: 0.8518 - f1_perClass: 0.9764 - acc: 0.9810\n",
      "Epoch 141/200\n",
      "24427/24427 [==============================] - 1s 38us/step - loss: 2.3693 - f1_perRow: 0.8377 - f1_perClass: 0.9740 - acc: 0.9818\n",
      "Epoch 142/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 2.3674 - f1_perRow: 0.8186 - f1_perClass: 0.9700 - acc: 0.9819\n",
      "Epoch 143/200\n",
      "24427/24427 [==============================] - 1s 38us/step - loss: 2.3664 - f1_perRow: 0.8076 - f1_perClass: 0.9681 - acc: 0.9826\n",
      "Epoch 144/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 2.3338 - f1_perRow: 0.8146 - f1_perClass: 0.9698 - acc: 0.9823\n",
      "Epoch 145/200\n",
      "24427/24427 [==============================] - 1s 41us/step - loss: 2.3886 - f1_perRow: 0.8083 - f1_perClass: 0.9710 - acc: 0.9809\n",
      "Epoch 146/200\n",
      "24427/24427 [==============================] - 1s 40us/step - loss: 2.3299 - f1_perRow: 0.8239 - f1_perClass: 0.9731 - acc: 0.9826\n",
      "Epoch 147/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 2.4098 - f1_perRow: 0.8218 - f1_perClass: 0.9733 - acc: 0.9812\n",
      "Epoch 148/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 2.5122 - f1_perRow: 0.8222 - f1_perClass: 0.9734 - acc: 0.9806\n",
      "Epoch 149/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 2.4158 - f1_perRow: 0.8497 - f1_perClass: 0.9764 - acc: 0.9811\n",
      "Epoch 150/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 2.3923 - f1_perRow: 0.8593 - f1_perClass: 0.9772 - acc: 0.9826\n",
      "Epoch 151/200\n",
      "24427/24427 [==============================] - 1s 39us/step - loss: 2.4000 - f1_perRow: 0.8542 - f1_perClass: 0.9768 - acc: 0.9814\n",
      "Epoch 152/200\n",
      "24427/24427 [==============================] - 1s 38us/step - loss: 2.3786 - f1_perRow: 0.8584 - f1_perClass: 0.9776 - acc: 0.9819\n",
      "Epoch 153/200\n",
      "24427/24427 [==============================] - 1s 38us/step - loss: 2.3786 - f1_perRow: 0.8574 - f1_perClass: 0.9774 - acc: 0.9809\n",
      "Epoch 154/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 2.3829 - f1_perRow: 0.8582 - f1_perClass: 0.9775 - acc: 0.9811\n",
      "Epoch 155/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 2.3717 - f1_perRow: 0.8538 - f1_perClass: 0.9760 - acc: 0.9807\n",
      "Epoch 156/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 2.3562 - f1_perRow: 0.8510 - f1_perClass: 0.9757 - acc: 0.9815\n",
      "Epoch 157/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 2.3437 - f1_perRow: 0.8463 - f1_perClass: 0.9758 - acc: 0.9828\n",
      "Epoch 158/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 2.3351 - f1_perRow: 0.8398 - f1_perClass: 0.9751 - acc: 0.9828\n",
      "Epoch 159/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 2.3218 - f1_perRow: 0.8379 - f1_perClass: 0.9752 - acc: 0.9831\n",
      "Epoch 160/200\n",
      "24427/24427 [==============================] - 1s 38us/step - loss: 2.3270 - f1_perRow: 0.8331 - f1_perClass: 0.9742 - acc: 0.9837\n",
      "Epoch 161/200\n",
      "24427/24427 [==============================] - 1s 38us/step - loss: 2.3500 - f1_perRow: 0.8383 - f1_perClass: 0.9745 - acc: 0.9818\n",
      "Epoch 162/200\n",
      "24427/24427 [==============================] - 1s 38us/step - loss: 2.3336 - f1_perRow: 0.8574 - f1_perClass: 0.9770 - acc: 0.9823\n",
      "Epoch 163/200\n",
      "24427/24427 [==============================] - 1s 38us/step - loss: 2.3097 - f1_perRow: 0.8551 - f1_perClass: 0.9755 - acc: 0.9834\n",
      "Epoch 164/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 2.3100 - f1_perRow: 0.8451 - f1_perClass: 0.9740 - acc: 0.9835\n",
      "Epoch 165/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 2.3422 - f1_perRow: 0.8352 - f1_perClass: 0.9727 - acc: 0.9828\n",
      "Epoch 166/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 2.3125 - f1_perRow: 0.8372 - f1_perClass: 0.9711 - acc: 0.9834\n",
      "Epoch 167/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 2.2743 - f1_perRow: 0.8388 - f1_perClass: 0.9677 - acc: 0.9830\n",
      "Epoch 168/200\n",
      "24427/24427 [==============================] - 1s 41us/step - loss: 2.3436 - f1_perRow: 0.8242 - f1_perClass: 0.9624 - acc: 0.9824\n",
      "Epoch 169/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24427/24427 [==============================] - 1s 40us/step - loss: 2.4633 - f1_perRow: 0.8357 - f1_perClass: 0.9651 - acc: 0.9809\n",
      "Epoch 170/200\n",
      "24427/24427 [==============================] - 1s 41us/step - loss: 2.4551 - f1_perRow: 0.8358 - f1_perClass: 0.9639 - acc: 0.9811\n",
      "Epoch 171/200\n",
      "24427/24427 [==============================] - 1s 41us/step - loss: 2.3706 - f1_perRow: 0.8619 - f1_perClass: 0.9683 - acc: 0.9818\n",
      "Epoch 172/200\n",
      "24427/24427 [==============================] - 1s 41us/step - loss: 2.3839 - f1_perRow: 0.8631 - f1_perClass: 0.9694 - acc: 0.9825\n",
      "Epoch 173/200\n",
      "24427/24427 [==============================] - 1s 38us/step - loss: 2.3659 - f1_perRow: 0.8581 - f1_perClass: 0.9702 - acc: 0.9821\n",
      "Epoch 174/200\n",
      "24427/24427 [==============================] - 1s 38us/step - loss: 2.3703 - f1_perRow: 0.8555 - f1_perClass: 0.9700 - acc: 0.9813\n",
      "Epoch 175/200\n",
      "24427/24427 [==============================] - 1s 39us/step - loss: 2.3713 - f1_perRow: 0.8559 - f1_perClass: 0.9702 - acc: 0.9810\n",
      "Epoch 176/200\n",
      "24427/24427 [==============================] - 1s 38us/step - loss: 2.3432 - f1_perRow: 0.8630 - f1_perClass: 0.9699 - acc: 0.9813\n",
      "Epoch 177/200\n",
      "24427/24427 [==============================] - 1s 38us/step - loss: 2.3388 - f1_perRow: 0.8679 - f1_perClass: 0.9666 - acc: 0.9823\n",
      "Epoch 178/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 2.3534 - f1_perRow: 0.8621 - f1_perClass: 0.9637 - acc: 0.9824\n",
      "Epoch 179/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 2.3415 - f1_perRow: 0.8647 - f1_perClass: 0.9661 - acc: 0.9822\n",
      "Epoch 180/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 2.3384 - f1_perRow: 0.8591 - f1_perClass: 0.9674 - acc: 0.9815\n",
      "Epoch 181/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 2.3279 - f1_perRow: 0.8603 - f1_perClass: 0.9647 - acc: 0.9828\n",
      "Epoch 182/200\n",
      "24427/24427 [==============================] - 1s 36us/step - loss: 2.3263 - f1_perRow: 0.8616 - f1_perClass: 0.9614 - acc: 0.9824\n",
      "Epoch 183/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 2.3191 - f1_perRow: 0.8674 - f1_perClass: 0.9635 - acc: 0.9822\n",
      "Epoch 184/200\n",
      "24427/24427 [==============================] - 1s 38us/step - loss: 2.3042 - f1_perRow: 0.8658 - f1_perClass: 0.9592 - acc: 0.9841\n",
      "Epoch 185/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 2.2957 - f1_perRow: 0.8633 - f1_perClass: 0.9563 - acc: 0.9836\n",
      "Epoch 186/200\n",
      "24427/24427 [==============================] - 1s 38us/step - loss: 2.2704 - f1_perRow: 0.8643 - f1_perClass: 0.9567 - acc: 0.9831\n",
      "Epoch 187/200\n",
      "24427/24427 [==============================] - 1s 41us/step - loss: 2.2760 - f1_perRow: 0.8576 - f1_perClass: 0.9511 - acc: 0.9843\n",
      "Epoch 188/200\n",
      "24427/24427 [==============================] - 1s 41us/step - loss: 2.2988 - f1_perRow: 0.8567 - f1_perClass: 0.9458 - acc: 0.9826\n",
      "Epoch 189/200\n",
      "24427/24427 [==============================] - 1s 40us/step - loss: 2.3871 - f1_perRow: 0.8375 - f1_perClass: 0.9377 - acc: 0.9813\n",
      "Epoch 190/200\n",
      "24427/24427 [==============================] - 1s 41us/step - loss: 2.5030 - f1_perRow: 0.8243 - f1_perClass: 0.9402 - acc: 0.9811\n",
      "Epoch 191/200\n",
      "24427/24427 [==============================] - 1s 41us/step - loss: 2.4801 - f1_perRow: 0.8208 - f1_perClass: 0.9359 - acc: 0.9815\n",
      "Epoch 192/200\n",
      "24427/24427 [==============================] - 1s 41us/step - loss: 2.4282 - f1_perRow: 0.8457 - f1_perClass: 0.9428 - acc: 0.9810\n",
      "Epoch 193/200\n",
      "24427/24427 [==============================] - 1s 40us/step - loss: 2.3934 - f1_perRow: 0.8592 - f1_perClass: 0.9509 - acc: 0.9810\n",
      "Epoch 194/200\n",
      "24427/24427 [==============================] - 1s 40us/step - loss: 2.3949 - f1_perRow: 0.8602 - f1_perClass: 0.9469 - acc: 0.9811\n",
      "Epoch 195/200\n",
      "24427/24427 [==============================] - 1s 40us/step - loss: 2.3817 - f1_perRow: 0.8597 - f1_perClass: 0.9384 - acc: 0.9807\n",
      "Epoch 196/200\n",
      "24427/24427 [==============================] - 1s 39us/step - loss: 2.3831 - f1_perRow: 0.8558 - f1_perClass: 0.9218 - acc: 0.9808\n",
      "Epoch 197/200\n",
      "24427/24427 [==============================] - 1s 41us/step - loss: 2.3729 - f1_perRow: 0.8543 - f1_perClass: 0.9063 - acc: 0.9807\n",
      "Epoch 198/200\n",
      "24427/24427 [==============================] - 1s 38us/step - loss: 2.3584 - f1_perRow: 0.8513 - f1_perClass: 0.8841 - acc: 0.9811\n",
      "Epoch 199/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 2.3639 - f1_perRow: 0.8474 - f1_perClass: 0.8561 - acc: 0.9807\n",
      "Epoch 200/200\n",
      "24427/24427 [==============================] - 1s 37us/step - loss: 2.3553 - f1_perRow: 0.8469 - f1_perClass: 0.8496 - acc: 0.9808\n"
     ]
    }
   ],
   "source": [
    "# print(model2.summary())\n",
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=200, batch_size=12500, shuffle=True, callbacks=callbacks_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================HOME Case : home_muhammed_final.json =============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omid/.conda/envs/iot/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n",
      "/home/omid/.conda/envs/iot/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if __name__ == '__main__':\n",
      "/home/omid/.conda/envs/iot/lib/python3.6/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                        button     0.981      0.714     0.035     0.068         7     5/ 7256/  136/    2\n",
      "              colorTemperature     1.000        nan       nan       nan         0     0/ 7399/    0/    0\n",
      "                       contact     0.997      0.983     0.704     0.820        58    57/ 7317/   24/    1\n",
      "                          lock     0.995      0.909     0.217     0.351        11    10/ 7352/   36/    1\n",
      "                        motion     0.987      0.815     0.792     0.803       238   194/ 7110/   51/   44\n",
      "                          ping     0.991      1.000     0.991     0.995      6952  6950/  383/   64/    2\n",
      "                        switch     0.999      0.913     0.840     0.875        23    21/ 7372/    4/    2\n",
      "                       unknown     0.987      0.935     0.955     0.945       905   846/ 6454/   40/   59\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.992      0.784     0.567     0.607      7399     0/    0/    0/    0\n",
      "Exact Match ACC : 0.96121 \n",
      "Total Records : 7399 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 5 (0.001)%\n",
      "=============================================================================\n",
      "==================HOME Case : home_os_final.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                        button     0.977      0.600     0.025     0.048         5     3/ 5129/  117/    2\n",
      "              colorTemperature     1.000      1.000     1.000     1.000         4     4/ 5247/    0/    0\n",
      "                       contact     0.994      0.946     0.837     0.888       130   123/ 5097/   24/    7\n",
      "                          lock     0.992      0.964     0.386     0.551        28    27/ 5180/   43/    1\n",
      "                        motion     0.978      0.707     0.452     0.551        99    70/ 5067/   85/   29\n",
      "                          ping     0.991      0.999     0.991     0.995      4814  4811/  391/   46/    3\n",
      "                        switch     0.999      0.929     0.684     0.788        14    13/ 5231/    6/    1\n",
      "                       unknown     0.975      0.830     0.956     0.889       624   518/ 4603/   24/  106\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.988      0.872     0.666     0.714      5251     0/    0/    0/    0\n",
      "Exact Match ACC : 0.94725 \n",
      "Total Records : 5251 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 4 (0.001)%\n",
      "=============================================================================\n",
      "==================HOME Case : home_os_final_reduced.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                        button     0.979      1.000     0.020     0.038         1     1/ 2365/   50/    0\n",
      "              colorTemperature     1.000        nan       nan       nan         0     0/ 2416/    0/    0\n",
      "                       contact     0.995      0.981     0.810     0.887        52    51/ 2352/   12/    1\n",
      "                          lock     0.993      0.923     0.414     0.571        13    12/ 2386/   17/    1\n",
      "                        motion     0.978      0.673     0.486     0.565        52    35/ 2327/   37/   17\n",
      "                          ping     0.988      0.999     0.988     0.994      2222  2220/  167/   27/    2\n",
      "                        switch     0.999      1.000     0.600     0.750         3     3/ 2411/    2/    0\n",
      "                       unknown     0.974      0.833     0.945     0.886       288   240/ 2114/   14/   48\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.988      0.801     0.533     0.586      2416     0/    0/    0/    0\n",
      "Exact Match ACC : 0.94785 \n",
      "Total Records : 2416 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 2 (0.001)%\n",
      "=============================================================================\n",
      "==================HOME Case : home_sk_final.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                        button     0.978      0.476     0.189     0.270        21    10/ 2427/   43/   11\n",
      "              colorTemperature     1.000        nan       nan       nan         0     0/ 2491/    0/    0\n",
      "                       contact     0.996      0.961     0.942     0.951       102    98/ 2383/    6/    4\n",
      "                          lock     0.998        nan     0.000     0.000         0     0/ 2485/    6/    0\n",
      "                        motion     0.986      0.725     0.627     0.673        51    37/ 2418/   22/   14\n",
      "                          ping     0.997      1.000     0.997     0.998      2300  2300/  183/    8/    0\n",
      "                        switch     0.996      0.667     0.667     0.667        15    10/ 2471/    5/    5\n",
      "                       unknown     0.986      0.888     0.917     0.902       187   166/ 2289/   15/   21\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.992      0.590     0.542     0.558      2491     0/    0/    0/    0\n",
      "Exact Match ACC : 0.96267 \n",
      "Total Records : 2491 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 5 (0.002)%\n",
      "=============================================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(lstm_tests_known)) :\n",
    "    print( \"==================HOME Case : %s =============\" % test_names[i])\n",
    "    lstm_pred= model2.predict( lstm_tests_known[i][0])\n",
    "    \n",
    "#     print_info( lstm_tests[i][1], lstm_pred, classes , confidance=0.7)\n",
    "    print_info( lstm_tests_known[i][1], lstm_pred, classes , confidance=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7efc6c5d3b38>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist2.history['f1_perClass'], c='red')\n",
    "plt.plot(hist2.history['loss'], c='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lstm_tests)) :\n",
    "    print( \"==================HOME Case : %s =============\" % test_names[i])\n",
    "    lstm_pred= model2.predict( lstm_tests[i][0])\n",
    "    \n",
    "#     print_info( lstm_tests[i][1], lstm_pred, classes , confidance=0.7)\n",
    "    print_info( lstm_tests[i][1], lstm_pred, classes , confidance=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist2.history['f1_perClass'], c='red')\n",
    "plt.plot(hist2.history['loss'], c='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bidirectional LSTM :D \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Flatten\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import Input, merge, concatenate, Conv2D, MaxPooling2D, Activation, UpSampling2D, Dropout, Conv2DTranspose, UpSampling2D, Lambda\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization as bn\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.merge import add\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers.merge import add\n",
    "from keras.layers import Dense,Conv1D,Dropout,Activation,BatchNormalization,MaxPooling1D,Flatten,Masking,TimeDistributed\n",
    "from keras.layers.recurrent import LSTM,GRU,SimpleRNN\n",
    "from keras.models import Input,Sequential,Model\n",
    "from keras.layers.merge import add\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import MSE,MSLE\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def f1_perRow(y_true, y_pred):\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "\n",
    "def f1_perClass(y_true, y_pred):\n",
    "\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss_perClass(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "def f1_loss_perRow(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "\n",
    "\n",
    "inputs  = Input( ( dim_size,1 ) )\n",
    "\n",
    "\n",
    "\n",
    "# bi_d_1 = Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat') (inputs)\n",
    "lstm_1 =  LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True)(inputs)\n",
    "# lstm_2 = LSTM(30 ,  recurrent_dropout=0.14, return_sequences=True)(lstm_1)\n",
    "\n",
    "bi_d_1 =Dense(128, activation='relu')  (lstm_1)\n",
    "lstm_1 =  Dense(128, activation='relu')(bi_d_1)\n",
    "lstm_2 = Dense(128, activation='relu')(lstm_1)\n",
    "\n",
    "\n",
    "\n",
    "# td_1    = TimeDistributed(Dense(256, activation='relu'))(lstm_2)\n",
    "# dout_1  = Dropout(0.1)(td_1)\n",
    "dout_1  = Dropout(0.1)(lstm_2)\n",
    "flt_1   = Flatten()(dout_1)\n",
    "dense_1 = Dense(128, activation='relu')(flt_1)\n",
    "dout_2  = Dropout(0.2)(dense_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dens_out_3 = Dense( 128, activation='relu' )(dout_2)\n",
    "\n",
    "\n",
    "# fl2  = Flatten()(out_new)\n",
    "\n",
    "out_put_final = Dense(len(classes), activation='sigmoid', name='Event_output')(dens_out_3)\n",
    "\n",
    "toService_1 = Dense( 130, name=\"to_service1\" )(dens_out_3)\n",
    "toService_1 = Dense( 130, name=\"to_service2\" )(toService_1)\n",
    "\n",
    "service_output = Dense(len(classes  ), activation=\"sigmoid\", name = 'service_output')(toService_1)\n",
    "\n",
    "\n",
    "\n",
    "model2 = Model(inputs=[inputs], outputs=[service_output])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model2.add(Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat'))\n",
    "# model2.add( LSTM(60 ,  recurrent_dropout=0.04, return_sequences=True))\n",
    "# model2.add( LSTM(30 ,  recurrent_dropout=0.14, return_sequences=True))\n",
    "# # model2.add(Bidirectional( LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True),merge_mode='concat'))\n",
    "# model2.add(TimeDistributed(Dense(256, activation='relu')))\n",
    "# model2.add(Dropout(0.1))\n",
    "# model2.add(Flatten())\n",
    "# model2.add(Dense(128, activation='relu'))\n",
    "# model2.add(Dropout(0.2))\n",
    "# model2.add(Dense(len(classes), activation='sigmoid'))\n",
    "\n",
    "# model2.compile(loss=weighted_categorical_crossentropy(weights=weights), optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "losses = {\n",
    "#     \"service_output\": f1_loss_perClass ,\n",
    "    \"service_output\": f1_loss_perRow ,\n",
    "    \"service_output\": \"categorical_crossentropy\",\n",
    "}\n",
    "lossWeights = {#\"service_output\": 20,\n",
    "               \"service_output\": 30.0 ,\n",
    "    \"service_output\": 20}\n",
    " \n",
    "\n",
    "\n",
    "#model2.compile(loss=losses,loss_weights=lossWeights, optimizer='adam', metrics=[f1_perRow,f1_perClass,'acc'])\n",
    "model2.compile(loss=losses, loss_weights=lossWeights, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('IoTDownNet', monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "print(model2.summary())\n",
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=1000, batch_size=12500, shuffle=True, callbacks=callbacks_list)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bidirectional LSTM :D \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Flatten\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import Input, merge, concatenate, Conv2D, MaxPooling2D, Activation, UpSampling2D, Dropout, Conv2DTranspose, UpSampling2D, Lambda\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization as bn\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.merge import add\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers.merge import add\n",
    "from keras.layers import Dense,Conv1D,Dropout,Activation,BatchNormalization,MaxPooling1D,Flatten,Masking,TimeDistributed\n",
    "from keras.layers.recurrent import LSTM,GRU,SimpleRNN\n",
    "from keras.models import Input,Sequential,Model\n",
    "from keras.layers.merge import add\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import MSE,MSLE\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def f1_perRow(y_true, y_pred):\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "\n",
    "def f1_perClass(y_true, y_pred):\n",
    "\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss_perClass(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "def f1_loss_perRow(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "\n",
    "\n",
    "inputs  = Input( ( dim_size,1 ) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(inputs)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# out = Flatten()(out)\n",
    "# out = MaxPooling1D(2,padding='same', name ='pooling')(out)\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# out = Flatten()(out)\n",
    "# out = MaxPooling1D(2,padding='same', name ='pooling2')(out)\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# out = Flatten()(out)\n",
    "# out = MaxPooling1D(2,padding='same', name ='pooling')(out)\n",
    "\n",
    "\n",
    "\n",
    "# fl_out_1 = Flatten()(dout_2)\n",
    "\n",
    "fl_out_cnn = Flatten()(out)\n",
    "\n",
    "\n",
    "dens_out_3 = Dense( 128, activation='relu' )(fl_out_cnn)\n",
    "\n",
    "# fl2  = Flatten()(out_new)\n",
    "\n",
    "out_put_final = Dense(len(classes), activation='sigmoid', name='Event_output')(dens_out_3)\n",
    "\n",
    "toService_1 = Dense( 130, name=\"to_service1\" )(dens_out_3)\n",
    "toService_1 = Dense( 130, name=\"to_service2\" )(toService_1)\n",
    "\n",
    "service_output = Dense(len(classes  ), activation=\"sigmoid\", name = 'service_output')(toService_1)\n",
    "\n",
    "\n",
    "\n",
    "model2 = Model(inputs=[inputs], outputs=[service_output])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model2.add(Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat'))\n",
    "# model2.add( LSTM(60 ,  recurrent_dropout=0.04, return_sequences=True))\n",
    "# model2.add( LSTM(30 ,  recurrent_dropout=0.14, return_sequences=True))\n",
    "# # model2.add(Bidirectional( LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True),merge_mode='concat'))\n",
    "# model2.add(TimeDistributed(Dense(256, activation='relu')))\n",
    "# model2.add(Dropout(0.1))\n",
    "# model2.add(Flatten())\n",
    "# model2.add(Dense(128, activation='relu'))\n",
    "# model2.add(Dropout(0.2))\n",
    "# model2.add(Dense(len(classes), activation='sigmoid'))\n",
    "\n",
    "# model2.compile(loss=weighted_categorical_crossentropy(weights=weights), optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "losses = {\n",
    "#     \"service_output\": f1_loss_perClass ,\n",
    "    \"service_output\": f1_loss_perRow ,\n",
    "    \"service_output\": \"categorical_crossentropy\",\n",
    "}\n",
    "lossWeights = {#\"service_output\": 20,\n",
    "               \"service_output\": 30.0 ,\n",
    "    \"service_output\": 20}\n",
    " \n",
    "\n",
    "\n",
    "model2.compile(loss=losses,loss_weights=lossWeights, optimizer='adam', metrics=[f1_perRow,f1_perClass,'acc'])\n",
    "# model2.compile(loss=losses, loss_weights=lossWeights, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('IoTDownNet', monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "print(model2.summary())\n",
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=100, batch_size=12500, shuffle=True, callbacks=callbacks_list)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bidirectional LSTM :D \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Flatten\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import Input, merge, concatenate, Conv2D, MaxPooling2D, Activation, UpSampling2D, Dropout, Conv2DTranspose, UpSampling2D, Lambda\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization as bn\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.merge import add\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers.merge import add\n",
    "from keras.layers import Dense,Conv1D,Dropout,Activation,BatchNormalization,MaxPooling1D,Flatten,Masking,TimeDistributed\n",
    "from keras.layers.recurrent import LSTM,GRU,SimpleRNN\n",
    "from keras.models import Input,Sequential,Model\n",
    "from keras.layers.merge import add\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import MSE,MSLE\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def f1_perRow(y_true, y_pred):\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "\n",
    "def f1_perClass(y_true, y_pred):\n",
    "\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss_perClass(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1)) \n",
    "\n",
    "def f1_loss_perRow(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))\n",
    "\n",
    "\n",
    "\n",
    "inputs  = Input( ( dim_size,1 ) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# bi_d_1 = Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat') (inputs)\n",
    "lstm_1 =  LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True)(inputs)\n",
    "# lstm_1 = LSTM(40 ,  recurrent_dropout=0.14, return_sequences=True)(lstm_1)\n",
    "\n",
    "bi_d_raw_1 =Dense(128, activation='relu')  (lstm_1)\n",
    "lstm_raw_1 =  Dense(128, activation='relu')(bi_d_raw_1)\n",
    "lstm_raw_2 = Dense(128, activation='relu')(lstm_raw_1)\n",
    "\n",
    "dout_1  = Dropout(0.1)(lstm_raw_2)\n",
    "flt_1   = Flatten()(dout_1)\n",
    "dense_1 = Dense(128, activation='relu')(flt_1)\n",
    "dout_3  = Dropout(0.2)(dense_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(inputs)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# out = Flatten()(out)\n",
    "# out = MaxPooling1D(2,padding='same', name ='pooling')(out)\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# out = Flatten()(out)\n",
    "# out = MaxPooling1D(2,padding='same', name ='pooling2')(out)\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# out = Flatten()(out)\n",
    "# out = MaxPooling1D(2,padding='same', name ='pooling')(out)\n",
    "\n",
    "\n",
    "\n",
    "# fl_out_1 = Flatten()(dout_2)\n",
    "\n",
    "fl_out_cnn = Flatten()(out)\n",
    "\n",
    "# out_new = concatenate( [fl_out_1, fl_out_cnn] , name='mergerguy')\n",
    "out_new = concatenate( [ fl_out_cnn,dout_3] , name='mergerguy')\n",
    "\n",
    "dens_out_1 = Dense( 128, activation='relu' )(out_new)\n",
    "dens_out_2 = Dense( 128, activation='relu' )(dens_out_1)\n",
    "dens_out_3 = Dense( 128, activation='relu' )(dens_out_2)\n",
    "\n",
    "\n",
    "\n",
    "service_output = Dense(len(classes  ), activation=\"sigmoid\", name = 'service_output')(dens_out_3)\n",
    "\n",
    "\n",
    "\n",
    "model2 = Model(inputs=[inputs], outputs=[service_output])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model2.add(Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat'))\n",
    "# model2.add( LSTM(60 ,  recurrent_dropout=0.04, return_sequences=True))\n",
    "# model2.add( LSTM(30 ,  recurrent_dropout=0.14, return_sequences=True))\n",
    "# # model2.add(Bidirectional( LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True),merge_mode='concat'))\n",
    "# model2.add(TimeDistributed(Dense(256, activation='relu')))\n",
    "# model2.add(Dropout(0.1))\n",
    "# model2.add(Flatten())\n",
    "# model2.add(Dense(128, activation='relu'))\n",
    "# model2.add(Dropout(0.2))\n",
    "# model2.add(Dense(len(classes), activation='sigmoid'))\n",
    "\n",
    "# model2.compile(loss=weighted_categorical_crossentropy(weights=weights), optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "losses = {\n",
    "#     \"service_output\": f1_loss_perClass ,\n",
    "    \"service_output\": f1_loss_perRow ,\n",
    "    \"service_output\": \"categorical_crossentropy\",\n",
    "}\n",
    "lossWeights = {#\"service_output\": 20,\n",
    "               \"service_output\": 30.0 ,\n",
    "    \"service_output\": 20}\n",
    " \n",
    "\n",
    "\n",
    "model2.compile(loss=losses,loss_weights=lossWeights, optimizer='adam', metrics=[f1_perRow,f1_perClass,'acc'])\n",
    "# model2.compile(loss=losses, loss_weights=lossWeights, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('IoTDownNet', monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "print(model2.summary())\n",
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=100, batch_size=12500, shuffle=True, callbacks=callbacks_list)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {\n",
    "#     \"service_output\": f1_loss_perClass ,\n",
    "    \"service_output\": f1_loss_perClass ,\n",
    "#     \"service_output\": \"categorical_crossentropy\",\n",
    "}\n",
    "lossWeights = {#\"service_output\": 20,\n",
    "               \"service_output\": 30.0 ,\n",
    "#     \"service_output\": 20\n",
    "}\n",
    " \n",
    "\n",
    "\n",
    "model2.compile(loss=losses, loss_weights=lossWeights, optimizer=keras.optimizers.Adam(lr=1e-6  ), metrics=[f1_perRow,f1_perClass,'acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bidirectional LSTM :D \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Flatten\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import Input, merge, concatenate, Conv2D, MaxPooling2D, Activation, UpSampling2D, Dropout, Conv2DTranspose, UpSampling2D, Lambda\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization as bn\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.merge import add\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers.merge import add\n",
    "from keras.layers import Dense,Conv1D,Dropout,Activation,BatchNormalization,MaxPooling1D,Flatten,Masking,TimeDistributed\n",
    "from keras.layers.recurrent import LSTM,GRU,SimpleRNN\n",
    "from keras.models import Input,Sequential,Model\n",
    "from keras.layers.merge import add\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import MSE,MSLE\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def f1_perRow(y_true, y_pred):\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "\n",
    "def f1_perClass(y_true, y_pred):\n",
    "\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss_perClass(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "def f1_loss_perRow(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "\n",
    "\n",
    "inputs  = Input( ( dim_size,1 ) )\n",
    "\n",
    "\n",
    "\n",
    "# bi_d_1 = Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat') (inputs)\n",
    "lstm_1 =  LSTM(32 ,  recurrent_dropout=0.3, return_sequences=True)(inputs)\n",
    "lstm_2 = LSTM(32 ,  recurrent_dropout=0.3, return_sequences=True)(lstm_1)\n",
    "\n",
    "lstm_2=Flatten()(lstm_2)\n",
    "lstm_2 = Dense(256, activation='relu')(lstm_2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "service_output = Dense(len(classes  ), activation=\"sigmoid\", name = 'service_output')(lstm_2)\n",
    "\n",
    "\n",
    "\n",
    "model2 = Model(inputs=[inputs], outputs=[service_output])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model2.add(Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat'))\n",
    "# model2.add( LSTM(60 ,  recurrent_dropout=0.04, return_sequences=True))\n",
    "# model2.add( LSTM(30 ,  recurrent_dropout=0.14, return_sequences=True))\n",
    "# # model2.add(Bidirectional( LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True),merge_mode='concat'))\n",
    "# model2.add(TimeDistributed(Dense(256, activation='relu')))\n",
    "# model2.add(Dropout(0.1))\n",
    "# model2.add(Flatten())\n",
    "# model2.add(Dense(128, activation='relu'))\n",
    "# model2.add(Dropout(0.2))\n",
    "# model2.add(Dense(len(classes), activation='sigmoid'))\n",
    "\n",
    "# model2.compile(loss=weighted_categorical_crossentropy(weights=weights), optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "losses = {\n",
    "#     \"service_output\": f1_loss_perClass ,\n",
    "    \"service_output\": f1_loss_perRow ,\n",
    "    \"service_output\": \"categorical_crossentropy\",\n",
    "}\n",
    "lossWeights = {#\"service_output\": 20,\n",
    "               \"service_output\": 30.0 ,\n",
    "    \"service_output\": 20}\n",
    " \n",
    "\n",
    "\n",
    "model2.compile(loss=losses,loss_weights=lossWeights, optimizer='adam', metrics=[f1_perRow,f1_perClass,'acc'])\n",
    "# model2.compile(loss=losses, loss_weights=lossWeights, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('IoTDownNet', monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "print(model2.summary())\n",
    "\n",
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=100, batch_size=12500, shuffle=True, callbacks=callbacks_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split model\n",
    "## Bidirectional LSTM :D \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Flatten\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import Input, merge, concatenate, Conv2D, MaxPooling2D, Activation, UpSampling2D, Dropout, Conv2DTranspose, UpSampling2D, Lambda\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization as bn\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.merge import add\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers.merge import add\n",
    "from keras.layers import Dense,Conv1D,Dropout,Activation,BatchNormalization,MaxPooling1D,Flatten,Masking,TimeDistributed\n",
    "from keras.layers.recurrent import LSTM,GRU,SimpleRNN\n",
    "from keras.models import Input,Sequential,Model\n",
    "from keras.layers.merge import add\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import MSE,MSLE\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def f1_perRow(y_true, y_pred):\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "\n",
    "def f1_perClass(y_true, y_pred):\n",
    "\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss_perClass(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "def f1_loss_perRow(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "#splitting data\n",
    "X=x_lstm_prossed_train2\n",
    "y=y_lstm_prossed_train\n",
    "a,b,c,d,e,f,g,h,ii,jj,k,l,m,n,o,p=[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]\n",
    "print(len(y_lstm_prossed_train[0]))\n",
    "for i in range(0,len(y_lstm_prossed_train)):\n",
    "    a.append(y_lstm_prossed_train[i][0])\n",
    "    b.append(y_lstm_prossed_train[i][1])\n",
    "    c.append(y_lstm_prossed_train[i][2])\n",
    "    d.append(y_lstm_prossed_train[i][3])\n",
    "    e.append(y_lstm_prossed_train[i][4])\n",
    "    f.append(y_lstm_prossed_train[i][5])\n",
    "    g.append(y_lstm_prossed_train[i][6])\n",
    "    h.append(y_lstm_prossed_train[i][7])\n",
    "    ii.append(y_lstm_prossed_train[i][8])\n",
    "    jj.append(y_lstm_prossed_train[i][9])\n",
    "    k.append(y_lstm_prossed_train[i][10])\n",
    "    l.append(y_lstm_prossed_train[i][11])\n",
    "    m.append(y_lstm_prossed_train[i][12])\n",
    "    n.append(y_lstm_prossed_train[i][13])\n",
    "    o.append(y_lstm_prossed_train[i][14])\n",
    "    p.append(y_lstm_prossed_train[i][15])\n",
    "    \n",
    "zzzz=[]    \n",
    "zzzz.append(np.array(a))\n",
    "zzzz.append(np.array(b))\n",
    "zzzz.append(np.array(c))\n",
    "zzzz.append(np.array(d))\n",
    "zzzz.append(np.array(e))\n",
    "zzzz.append(np.array(f))\n",
    "zzzz.append(np.array(g))\n",
    "zzzz.append(np.array(h))\n",
    "zzzz.append(np.array(ii))\n",
    "zzzz.append(np.array(jj))\n",
    "zzzz.append(np.array(k))\n",
    "zzzz.append(np.array(l))\n",
    "zzzz.append(np.array(m))\n",
    "zzzz.append(np.array(n))\n",
    "zzzz.append(np.array(o))\n",
    "zzzz.append(np.array(p))\n",
    "\n",
    "\n",
    "for i in range(0,16):\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "    inputs  = Input( ( dim_size,1 ) )\n",
    "\n",
    "\n",
    "    out = Conv1D(128,3,padding='same')(inputs)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = Activation('relu')(out)\n",
    "    out = Dropout(0.2)(out)\n",
    "    out = Conv1D(128,3,padding='same')(out)\n",
    "    # bi_d_1 = Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat') (inputs)\n",
    "    lstm_1 =  LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True)(out)\n",
    "    # lstm_2 = LSTM(30 ,  recurrent_dropout=0.14, return_sequences=True)(lstm_1)\n",
    "\n",
    "    bi_d_1 =Dense(128, activation='relu')  (lstm_1)\n",
    "    lstm_1 =  Dense(128, activation='relu')(bi_d_1)\n",
    "    lstm_2 = Dense(128, activation='relu')(lstm_1)\n",
    "\n",
    "\n",
    "\n",
    "    # td_1    = TimeDistributed(Dense(256, activation='relu'))(lstm_2)\n",
    "    # dout_1  = Dropout(0.1)(td_1)\n",
    "    dout_1  = Dropout(0.1)(lstm_2)\n",
    "    flt_1   = Flatten()(dout_1)\n",
    "    dense_1 = Dense(128, activation='relu')(flt_1)\n",
    "    dout_2  = Dropout(0.2)(dense_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # bi_d_1 = Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat') (inputs)\n",
    "    lstm_1 =  LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True)(inputs)\n",
    "    # lstm_1 = LSTM(40 ,  recurrent_dropout=0.14, return_sequences=True)(lstm_1)\n",
    "\n",
    "    bi_d_raw_1 =Dense(128, activation='relu')  (lstm_1)\n",
    "    lstm_raw_1 =  Dense(128, activation='relu')(bi_d_raw_1)\n",
    "    lstm_raw_2 = Dense(128, activation='relu')(lstm_raw_1)\n",
    "\n",
    "    dout_1  = Dropout(0.1)(lstm_raw_2)\n",
    "    flt_1   = Flatten()(dout_1)\n",
    "    dense_1 = Dense(128, activation='relu')(flt_1)\n",
    "    dout_3  = Dropout(0.2)(dense_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    out = Conv1D(128,3,padding='same')(inputs)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = Activation('relu')(out)\n",
    "    out = Dropout(0.2)(out)\n",
    "    out = Conv1D(128,3,padding='same')(out)\n",
    "    # out = Flatten()(out)\n",
    "    # out = MaxPooling1D(2,padding='same', name ='pooling')(out)\n",
    "\n",
    "\n",
    "    out = Conv1D(128,3,padding='same')(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = Activation('relu')(out)\n",
    "    out = Dropout(0.2)(out)\n",
    "    out = Conv1D(128,3,padding='same')(out)\n",
    "    # out = Flatten()(out)\n",
    "    # out = MaxPooling1D(2,padding='same', name ='pooling2')(out)\n",
    "\n",
    "\n",
    "    out = Conv1D(128,3,padding='same')(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = Activation('relu')(out)\n",
    "    out = Dropout(0.2)(out)\n",
    "    out = Conv1D(128,3,padding='same')(out)\n",
    "    # out = Flatten()(out)\n",
    "    # out = MaxPooling1D(2,padding='same', name ='pooling')(out)\n",
    "\n",
    "\n",
    "\n",
    "    # fl_out_1 = Flatten()(dout_2)\n",
    "\n",
    "    fl_out_cnn = Flatten()(out)\n",
    "\n",
    "    # out_new = concatenate( [fl_out_1, fl_out_cnn] , name='mergerguy')\n",
    "    out_new = concatenate( [dout_2, fl_out_cnn,dout_3] , name='mergerguy')\n",
    "\n",
    "    dens_out_1 = Dense( 128, activation='relu' )(out_new)\n",
    "    dens_out_2 = Dense( 128, activation='relu' )(dens_out_1)\n",
    "    dens_out_3 = Dense( 128, activation='relu' )(dens_out_2)\n",
    "\n",
    "    # fl2  = Flatten()(out_new)\n",
    "\n",
    "    out_put_final = Dense(len(classes), activation='sigmoid', name='Event_output')(dens_out_3)\n",
    "\n",
    "    toService_1 = Dense( 130, name=\"to_service1\" )(dens_out_3)\n",
    "    toService_1 = Dense( 130, name=\"to_service2\" )(toService_1)\n",
    "\n",
    "    service_output = Dense(1, activation=\"sigmoid\", name = 'service_output')(toService_1)\n",
    "\n",
    "\n",
    "    losses = {\n",
    "    #     \"service_output\": f1_loss_perClass ,\n",
    "        \"service_output\": f1_loss_perRow ,\n",
    "        \"service_output\": \"binary_crossentropy\",\n",
    "    }\n",
    "    lossWeights = {#\"service_output\": 20,\n",
    "                   \"service_output\": 30.0 ,\n",
    "        \"service_output\": 20}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    model2 = Model(inputs=[inputs], outputs=[service_output])\n",
    "    model2.compile(loss=losses,loss_weights=lossWeights, optimizer='adam', metrics=[f1_perRow,f1_perClass,'acc'])\n",
    "    # model2.compile(loss=losses, loss_weights=lossWeights, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    checkpoint = ModelCheckpoint('IoTDownNet', monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "    hist2 = model2.fit(x_lstm_prossed_train2, zzzz[i], epochs=200, batch_size=12500, shuffle=True, callbacks=callbacks_list)\n",
    "    model2.save('number'+str(i)+'.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2.compile(loss=losses,loss_weights=lossWeights, optimizer=keras.optimizers.Adam(lr=5e-5  ), metrics=[f1_perRow,f1_perClass,'acc'])\n",
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=300, batch_size=16500, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss=f1_loss, optimizer=keras.optimizers.Adam(lr=8e-5  ), metrics=[f1,'acc'])\n",
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=300, batch_size=7500, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss=weighted_categorical_crossentropy(weights=weights), optimizer=keras.optimizers.Adam(lr=8e-5  ), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(lr=8e-5  ), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss=weighted_categorical_crossentropy(weights=weights), optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=300, batch_size=3500, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save( \"LSTM-sigmoid-withRemovedClasses\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist2.history['loss'], c='red')\n",
    "plt.plot(hist2.history['acc'], c='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist2.history['loss'], c='red')\n",
    "# plt.plot(hist2.history['acc'], c='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model2, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "# model2=load_model( \"LSTM_withSigmoid_LargeData_F1_E100_B500_MSE_False\"  \n",
    "#            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(lstm_tests)) :\n",
    "    print( \"==================HOME Case : %s =============\" % test_names[i])\n",
    "    makeReadable( classes=classes, confidance=0.5,data=lstm_tests[i][0],gt=lstm_tests[i][1],model=model2,path=test_names[i],x=lstm_tests[i][0])\n",
    "    \n",
    "#     lstm_pred= model2.predict( lstm_tests[i][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ------------- do not go any further :) ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred= model2.predict( lstm_tests[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred= model2.predict( lstm_tests[1][0])\n",
    "lstm_pred__ = np.array(list(lstm_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred__ = np.array(list(lstm_pred))\n",
    "print_info( lstm_tests[1][1], lstm_pred__, classes , confidance=0.43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in [1] :\n",
    "for i in range(len(lstm_tests)) :\n",
    "    print( \"==================HOME Case : %s =============\" % test_names[i])\n",
    "    lstm_pred= model2.predict( lstm_tests[i][0])\n",
    "    \n",
    "#     print_info( lstm_tests[i][1], lstm_pred, classes , confidance=0.7)\n",
    "    print_info( lstm_tests[i][1], lstm_pred, classes , confidance=0.5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in [1] :\n",
    "# for i in range(len(lstm_tests)) :\n",
    "    print( \"==================HOME Case : %s =============\" % test_names[i])\n",
    "    lstm_pred= model2.predict( lstm_tests[i][0])\n",
    "    \n",
    "#     print_info( lstm_tests[i][1], lstm_pred, classes , confidance=0.7)\n",
    "    print_info( lstm_tests[i][1], lstm_pred, classes , confidance=0.992)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred =model2.predict( x_lstm_prossed_test2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_info(y_lstm_prossed_test, lstm_pred, classes, confidance=0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred =model2.predict( x_lstm_prossed_test)\n",
    "print_info(y_lstm_prossed_test, lstm_pred, classes, confidance=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred = model2.predict( x_lstm_prossed_test)\n",
    "print_info(y_lstm_prossed_test, lstm_pred, classes, confidance=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_info(y_lstm_prossed_train, y_lstm_prossed_train, classes, confidance=0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x  in lstm_pred  if  np.sum(x) > 0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save( \"LSTM_withSigmoid_LargeData_F%s_E%d_B%d_M%s_%r\" %\n",
    "            (\n",
    "            FoldID,\n",
    "                Epoch_count,\n",
    "                Batch_size,\n",
    "                Mapper,\n",
    "                IgnoreEmpty\n",
    "            ) \n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred = model2.predict( x_lstm_prossed_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_info(y_lstm_prossed_test, lstm_pred, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_muhammed,y_train_muhammed, classes = pre_process_raw( x_train, y_train , dim_size, zero_pad=True, normalize=False)\n",
    "# x_test_muhammed,y_test_muhammed, classes = pre_process_raw( x_test, y_test , dim_size, zero_pad=True, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred = model2.predict( x_lstm_prossed_test )\n",
    "print_info(y_lstm_prossed_test, lstm_pred, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred = model2.predict( x_lstm_prossed_test )\n",
    "print_info(y_lstm_prossed_test, lstm_pred, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_info(y_lstm_prossed_test, pred, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len( y_lstm_prossed_train[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_size =160\n",
    "x_lstm_prossed_train,y_lstm_prossed_train, _ = pre_process_raw( x_train, y_train , dim_size, zero_pad=True, normalize=False,classes=classes)\n",
    "x_lstm_prossed_test,y_lstm_prossed_test, _ = pre_process_raw( x_test, y_test , dim_size, zero_pad=True, normalize=False,classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([x  for x  in y_lstm_prossed_test if x[21]==1 or x[20]==1]), len(y_lstm_prossed_test  ) , len([x  for x  in y_lstm_prossed_test if x[21]==1 or x[20]==1])/len(y_lstm_prossed_test  ) *1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ x for x  in  pred if np.sum(x) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_lstm_prossed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_for_raun( pred   ):\n",
    "    pp = pred\n",
    "    pp[pp>=0.5] = 1\n",
    "    pp[pp<0.5] = 0\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([x for x in pred if np.sum( do_for_raun(x) )==0 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([x for x in pred if  do_for_raun(x)[20] ==1 or do_for_raun(x)[21] ==1 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# np.save(\"../files/muhammed/x_train.json\" , x_train_muhammed)\n",
    "# np.save(\"../files/muhammed/y_train.json\", )\n",
    "\n",
    "\n",
    "# np.save( \"../files/muhammed/x_train.json\", x_train_muhammed )\n",
    "# np.save(\"../files/muhammed/y_train.json\",  y_train_muhammed )\n",
    "# np.save( \"../files/muhammed/x_test.json\",x_test_muhammed )\n",
    "# np.save( \"../files/muhammed/y_test.json\",y_test_muhammed )\n",
    "# np.save( \"../files/muhammed/classes.json\",  classes )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_lstm_prossed_test) + len(x_lstm_prossed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6(iot)",
   "language": "python",
   "name": "iot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
