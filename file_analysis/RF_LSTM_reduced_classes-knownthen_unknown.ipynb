{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import json \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import keras \n",
    "\n",
    "# import numpy\n",
    "# import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import glob\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadFromMerged=True\n",
    "loadFromIndexes= False\n",
    "Mapper='S'\n",
    "IgnoreEmpty= True\n",
    "FoldID =\"1\"\n",
    "Epoch_count=100\n",
    "Batch_size=5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data the old way\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_data= [] \n",
    "# y_data= [] \n",
    "\n",
    "\n",
    "# with open( '../files/txt/seq_mapping_large.txt' ) as f:\n",
    "#     x_data =   f.readlines()\n",
    "\n",
    "# with open( '../files/txt/command_mapping_large.txt' ) as f:\n",
    "#     y_data = f.readlines()\n",
    "    \n",
    "    \n",
    "# x_data =[ np.array([ int(y) for y in x.strip().split( ' ') ])   for x in  x_data ] \n",
    "# y_data =[ x.strip().split(' ') for x in  y_data ] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Load The Data The New Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  mapps the input records to a integer array for the input\n",
    "def mapping_x( inp, includeDirection = False , TrimAt= 15 ):\n",
    "    if includeDirection:\n",
    "        return np.array([ int(x[\"packet_length\"]) * (1 if x['packet_source']=='hub' else -1)  for x in inp ][:15])\n",
    "    else:\n",
    "        return np.array([ int(x[\"packet_length\"])  for x in inp ][:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping_y_service(inp):\n",
    "    return np.array(  list(set([x[\"event\"] for x in inp])) if (len(inp )>0) else [\"none\"] )\n",
    "\n",
    "def mapping_y_service_event(inp):\n",
    "    return np.array(  list(set([ \"%s-%s\"%( x[\"event\"] ,x[\"val\"] ) for x in inp])) if (len(inp )>0) else [\"none\"] )\n",
    "\n",
    "def mapping_y_device_service(inp):\n",
    "    return np.array(  list(set([ \"%s & %s\"%( x[\"device\"] ,x[\"event\"] ) for x in inp])) if (len(inp )>0) else [\"none\"] )\n",
    "\n",
    "def mapping_y_full(inp):\n",
    "    return np.array(  list(set([ \"%s & %s & %s\"%( x[\"device\"] ,x[\"event\"], x['val'] ) for x in inp])) if (len(inp )>0) else [\"none\"] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cleans the data removing emply nodes and turning the nodes into sarrays by calling the mapping function \n",
    "def clean_data( x_data, y_data , removeempty=True, Mapping='S'):\n",
    "    cleans = [] \n",
    "    cleans = (sorted([ x for x in y_data if (removeempty and len(y_data[x]) > 0) or not removeempty  ] ))\n",
    "    \n",
    "    ret_x  = [x_data[x] for x in cleans]\n",
    "    ret_y  = [y_data[x] for x in cleans] \n",
    "    \n",
    "    print( len(y_data), len(cleans) )\n",
    "    \n",
    "    ret_x  = [ mapping_x(x) for x in ret_x ] \n",
    "    ret_y_s = [ mapping_y_service(y) for y in ret_y ]\n",
    "    if Mapping=='S':\n",
    "        ret_y  = [ mapping_y_service(y) for y in ret_y ]\n",
    "    elif Mapping=='SE':\n",
    "        ret_y  = [ mapping_y_service_event(y) for y in ret_y ]\n",
    "    elif Mapping=='DS':\n",
    "        ret_y  = [ mapping_y_device_service(y) for y in ret_y ]\n",
    "    elif Mapping=='F':\n",
    "        ret_y  = [ mapping_y_full(y) for y in ret_y ]\n",
    "    return ret_x, ret_y, ret_y_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in load from merged\n",
      "58958 57867\n",
      "loading from test files\n",
      "found files :  4\n",
      "32069 32069\n",
      "19968 19968\n",
      "9109 9109\n",
      "6404 6404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(57867, 4)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= []\n",
    "y= []\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "y_test_service= []\n",
    "\n",
    "x_train = {}\n",
    "y_train = {}\n",
    "\n",
    "test_names = []\n",
    "\n",
    "add_to_trainig = [0,2]\n",
    "\n",
    "if loadFromMerged:\n",
    "    print(\"in load from merged\")\n",
    "    with open(  '../files/train/test/test_homes/final_upload/Merged_final_with_home/merged_hub_segments_final.json'  ) as f:\n",
    "        y_data = json.load(f)\n",
    "\n",
    "    with open(  '../files/train/test/test_homes/final_upload/Merged_final_with_home/merged_pcap_segments_final.json'  ) as f:\n",
    "        x_data = json.load(f)\n",
    "        \n",
    "#     with open(  '../files/train/merged/hub_segments_2.json'  ) as f:\n",
    "#         y_data = json.load(f)\n",
    "\n",
    "#     with open(  '../files/train/merged/pcap_segments_2.json'  ) as f:\n",
    "#         x_data = json.load(f)\n",
    "  \n",
    "    if len( y_data ) != len(x_data) :\n",
    "        print( pick )\n",
    "        \n",
    "    \n",
    "    x_train,y_train, y_train_service= clean_data( x_data, y_data, IgnoreEmpty , Mapping=Mapper )\n",
    "    \n",
    "    #     continue\n",
    "#     if loadFromIndexes:\n",
    "#         print(\"load from indexes\")\n",
    "#         with open(\"../files/train/merged/items_2_test-train_indexes.json\")  as f:\n",
    "#             index_info = json.load(f)\n",
    "\n",
    "\n",
    "#         for i in index_info[FoldID][\"test\"]:\n",
    "#             x_test[str(i)]=(x_data[str(i)] )\n",
    "#             y_test[str(i)]=(y_data[str(i)] )\n",
    "\n",
    "#         for i in index_info[FoldID][\"train\"]:\n",
    "#             x_train[str(i)]=(  x_data[str(i)] )\n",
    "#             y_train[str(i)]=(  y_data[str(i)] )\n",
    "        \n",
    "#         x_test_t,y_test_t= clean_data( x_test, y_test, IgnoreEmpty , Mapping=Mapper)\n",
    "#         x_test.append(x_test_t)\n",
    "#         y_test.append(y_test_t)\n",
    "    #     else :\n",
    "    print(\"loading from test files\")\n",
    "    test_files = sorted(glob.glob( '../files/train/test/test_homes/final_upload/usecases/pcap_segments_final_final/home*.json' ))\n",
    "    print( \"found files : \" , len(test_files) )\n",
    "    for pick  in test_files:\n",
    "        fname  = os.path.basename(pick)\n",
    "        test_names.append( fname )\n",
    "        with open( os.path.join( '../files/train/test/test_homes/final_upload/usecases/hub_segments_final_final/', fname) ) as f:\n",
    "            y_data_test = json.load(f)\n",
    "\n",
    "        with open( os.path.join('../files/train/test/test_homes/final_upload/usecases/pcap_segments_final_final/', fname) ) as f:\n",
    "            x_data_test = json.load(f)\n",
    "\n",
    "\n",
    "        t_x,t_y, t_z= clean_data( x_data_test, y_data_test, False , Mapping=Mapper )\n",
    "\n",
    "#         if test_files.index(pick) in add_to_trainig:\n",
    "#             x_test_t,y_test_t, y_test_service_t= clean_data( x_data_test, y_data_test, IgnoreEmpty , Mapping=Mapper)\n",
    "#             x_train.extend(x_test_t)\n",
    "#             y_train.extend(y_test_t)\n",
    "#             y_train_service.extend(y_test_service_t)\n",
    "\n",
    "\n",
    "        x_test.append(t_x)\n",
    "        y_test.append(t_y)\n",
    "        y_test_service.append(t_z)\n",
    "            \n",
    "#     x_test = x_data[ index_info[\"1\"][\"test\"]  ]\n",
    "#     y_test = y_data[ index_info[\"1\"][\"test\"]  ]\n",
    "    \n",
    "#     x_train = x_data[ index_info[\"1\"][\"train\"]  ]\n",
    "#     y_train = y_data[ index_info[\"1\"][\"train\"]  ]\n",
    "#     x.extend(t_x)\n",
    "#     y.extend(t_y)\n",
    "else:\n",
    "    for pick in sorted(glob.glob( '../files/train/hub_segments/*.json' )):\n",
    "        fname  = os.path.basename(pick)\n",
    "        test_names.append( fname )\n",
    "        with open( os.path.join( '../files/train/hub_segments/', fname) ) as f:\n",
    "            y_data = json.load(f)\n",
    "\n",
    "        with open( os.path.join('../files/train/pcap_segments/', fname) ) as f:\n",
    "            x_data = json.load(f)\n",
    "\n",
    "        if len( y_data ) != len(x_data) :\n",
    "            print( pick )\n",
    "            continue\n",
    "\n",
    "        t_x,t_y= clean_data( x_data, y_data, True )\n",
    "\n",
    "        x.extend( t_x)\n",
    "        y.extend(t_y)\n",
    "\n",
    "x= np.array(x)\n",
    "y= np.array(y)\n",
    "\n",
    "# x_train = np.append( x_train, x_test[0] , axis=0)\n",
    "# x_train = np.append( x_train, x_test[2] , axis=0)\n",
    "\n",
    "# y_train = np.append( y_train, y_test[0] , axis=0)\n",
    "# y_train = np.append( y_train, y_test[2] , axis=0)\n",
    "\n",
    "\n",
    "len(x_train), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Mittigation Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packet Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# days =[7,4,2,2]\n",
    "# t_sum =0\n",
    "# for ii in range(len(x_test)):\n",
    "#     p = x_test[ii]\n",
    "#     sizes = np.unique(np.concatenate(p), return_counts=True)\n",
    "#     sums= 0 \n",
    "#     for i in range(len(sizes[0])):\n",
    "# #         print( \"%d--> %d\" % ( sizes[0][i], sizes[1][i] ) )\n",
    "#         if  sizes[0][i] < 1000:\n",
    "#             sums+= (1000-sizes[0][i] )* sizes[1][i]\n",
    "#     t_sum +=(sums / days[ii] )/1000000 \n",
    "#     print ( (sums / days[ii] )/1000000)\n",
    "# print('--------')\n",
    "# print(t_sum/4)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# days =[7,4,2,2]\n",
    "# t_sum =0\n",
    "# d_sum = 0\n",
    "# import math\n",
    "# for ii in range(len(x_test)):\n",
    "\n",
    "#     p = x_test[ii]\n",
    "#     for i in p : \n",
    "#         t_sum += math.ceil(np.sum(i) / 2000)\n",
    "#         d_sum+= np.sum( i )\n",
    "# total_fixed  =  t_sum* 2000 / 15\n",
    "\n",
    "# print ( total_fixed , d_sum, d_sum-t_sum*2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packet Insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# days =[7,4,2,2]\n",
    "# t_sum =0\n",
    "# for ii in range(len(x_test)):\n",
    "#     p = x_test[ii]\n",
    "#     sizes = np.unique(np.concatenate(p), return_counts=True)\n",
    "#     sums= 0 \n",
    "#     for i in range(len(sizes[0])):\n",
    "# #         print( \"%d--> %d\" % ( sizes[0][i], sizes[1][i] ) )\n",
    "#         if  sizes[0][i] < 1000:\n",
    "#             sums+= (1000-sizes[0][i] )* sizes[1][i]\n",
    "#     t_sum +=(sums / days[ii] )/1000000 \n",
    "#     print ( (sums / days[ii] )/1000000)\n",
    "# print('--------')\n",
    "# print(t_sum/4)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sets the classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'acceleration'),\n",
       " (1, 'activity'),\n",
       " (2, 'battery'),\n",
       " (3, 'button'),\n",
       " (4, 'colorTemperature'),\n",
       " (5, 'contact'),\n",
       " (6, 'level'),\n",
       " (7, 'lock'),\n",
       " (8, 'motion'),\n",
       " (9, 'ping'),\n",
       " (10, 'status'),\n",
       " (11, 'switch'),\n",
       " (12, 'temperature'),\n",
       " (13, 'threeAxis'),\n",
       " (14, 'unknown'),\n",
       " (15, 'water')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = sorted(list(np.unique(  np.concatenate( y_train  ))))\n",
    "# print([ (i , classes[i]) for i in range( len(classes) ) ])\n",
    "\n",
    "service_classes = sorted(list(np.unique(  np.concatenate( y_train_service  ))))\n",
    "[ (i , service_classes[i]) for i in range( len(service_classes) ) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter the records by service/event types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_clean_event( inp, return_clean= True  ):\n",
    "    return is_clean(inp, return_clean=return_clean, to_keep=[ 'no_logs', 'lock-unlocked', 'on/off-XXX', 'raw-XXX', 'read_attr_-_raw-XXX' ] )\n",
    "#     if return_clean:\n",
    "#         return  'no_logs' not in inp and 'lock-unlocked' not in inp and 'on/off-XXX' not in inp and 'raw-XXX' not in inp and 'read_attr_-_raw-XXX' not in inp\n",
    "#     else:\n",
    "#         return  'lock-locked' in inp or 'lock-unlocked'  in inp or 'on/off-XXX' in inp or  'raw-XXX' in inp  or 'read_attr_-_raw-XXX' in inp \n",
    "     \n",
    "def is_clean_service( inp, return_clean= True  ):\n",
    "    return is_clean(inp, to_keep=['no_logs','unknown', 'read_attr_-_raw'], return_clean=return_clean )\n",
    "    \n",
    "#     if return_clean:\n",
    "#         return  'no_logs' not in inp and 'unknown' not in inp and 'read_attr_-_raw' not in inp #and 'ping' not in inp \n",
    "#     else:\n",
    "#         return  'no_logs' in inp or  'unknown' in inp  or 'read_attr_-_raw' in inp #or 'ping' in inp \n",
    "\n",
    "def is_clean(inp, to_keep=[], return_clean=True):\n",
    "    ret = False \n",
    "    \n",
    "    for x  in to_keep:\n",
    "        if x in inp:\n",
    "            ret = True\n",
    "            \n",
    "    if not return_clean:\n",
    "        ret = not ret\n",
    "        \n",
    "    return ret\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "choose services to keep : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "services_to_keep = ['acceleration', 'lock', 'temperature', 'motion', 'unknown'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find records which need change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indexes_to_change =  [ i for i in range(len(y_train)) if\n",
    "              is_clean( y_train[i],to_keep=services_to_keep, return_clean=False) ] \n",
    "\n",
    "y_train = [ ['unknown'] if i in train_indexes_to_change else y_train[i] \n",
    "           for i in range(len(y_train)) ] \n",
    "\n",
    "for t_index in range(len(y_test)):\n",
    "    test_indexes_to_change =  [ i for i in range(len(y_test[t_index])) if\n",
    "                  is_clean( y_test[t_index][i],to_keep=services_to_keep, return_clean=False) ] \n",
    "\n",
    "    y_test[t_index] = [ ['unknown'] if i in test_indexes_to_change else y_test[t_index][i] \n",
    "               for i in range(len(y_test[t_index])) ] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove classes that are ignored from the services list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = services_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toKeep = [ i for i in range(len(y_train)) if is_clean_event( y_train[i]) ] if Mapper=='SE' else [ i for i in range(len(y_train)) if is_clean_service( y_train[i]) ]\n",
    "# x_train= [ x_train[i] for i in toKeep ]\n",
    "# y_train= [ y_train[i] for i in toKeep ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for j in range(len(x_test)):\n",
    "#     toChange= [ i for i in range(len(y_test[j])) if is_clean_service( y_test[j][i], False) ]\n",
    "#     y_test[j] = [ (y_test[j][i] if i not in toChange else np.array( ['none'])) for i in range(len(y_test[j])) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes.remove('read_attr_-_raw-XXX')\n",
    "# classes.remove('on/off-XXX')\n",
    "# classes.remove('raw-XXX')\n",
    "# classes.remove('lock-unlocked')\n",
    "# classes.remove('lock-locked')\n",
    "\n",
    "\n",
    "# classes.remove('read_attr_-_raw')\n",
    "# classes.remove('on/off')\n",
    "# classes.remove('raw')\n",
    "# classes.remove('unknown')\n",
    "\n",
    "# classes.remove('lock')\n",
    "# # classes.remove('lock')\n",
    "\n",
    "\n",
    "# classes.remove('switch-on')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ===== end of unknown packet control====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_raw( x_data,y_data, dim_size = 128, zero_pad = False, normalize = False ,classes=None, twoD= False ):\n",
    "#  y data \n",
    "# \"\"\"\n",
    "# this functino is in charge of preprocessing the records , the sourc e json contains a lot of extra stuff, this function tailors\n",
    "# the data and it fixes their lenghth\n",
    "# \"\"\"\n",
    "    if classes is None:\n",
    "        classes  = sorted(list(np.unique(  np.concatenate( y_data  ))))\n",
    "    else :\n",
    "        classes = sorted(classes)\n",
    "    y_data_categorical = []  \n",
    "\n",
    "    for x in y_data:\n",
    "        temp = np.zeros( len(classes) )\n",
    "        for y in x : \n",
    "            if y in classes:\n",
    "                temp[ classes.index( y ) ] = 1\n",
    "        y_data_categorical.append( temp )\n",
    "    y_data_categorical = np.vstack(y_data_categorical)\n",
    "\n",
    "#     x_data = np.array( x_data) / 1500.0\n",
    "    \n",
    "    x_data_temp = [] \n",
    "    \n",
    "    if not zero_pad:\n",
    "        if twoD:\n",
    "            for x in x_data:\n",
    "                temp = [] #list(x)\n",
    "                lst = list(x)\n",
    "                while dim_size**2 - len(temp )   > len(lst):\n",
    "                    temp.extend(lst)\n",
    "\n",
    "                while len(temp) < dim_size**2:\n",
    "                    temp.append( 0 )\n",
    "\n",
    "                x_data_temp.append(np.array( temp).reshape(dim_size,dim_size))\n",
    "\n",
    "\n",
    "            x_data_temp = np.array( x_data_temp )\n",
    "            x_data_temp=x_data_temp.reshape(x_data_temp.shape+(1,))\n",
    "        else: \n",
    "            temp = [] \n",
    "            lst = list(x)\n",
    "            for x in x_data:\n",
    "                temp = [] #list(x)\n",
    "                lst = list(x)\n",
    "                while dim_size - len(temp )   > len(lst):\n",
    "                    temp.extend(lst)\n",
    "\n",
    "                while len(temp) < dim_size:\n",
    "                    temp.append( 0 )\n",
    "                \n",
    "                x_data_temp.append(np.array( temp))\n",
    "            \n",
    "    else :\n",
    "        x_data_temp = sequence.pad_sequences(x_data, maxlen=dim_size)\n",
    "    \n",
    "    \n",
    "    if normalize:\n",
    "        x_data_temp = np.array( x_data_temp) / (np.amax( x_data_temp) + 0.000000000001)\n",
    "    else :\n",
    "        x_data_temp = np.array(x_data_temp)\n",
    "    \n",
    "    \n",
    "    return x_data_temp ,y_data_categorical , classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,_=pre_process_raw( x_train, y_train , 15, zero_pad=True, normalize=True, classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9999999999999997, 0.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.amax(x), np.amin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_recall_shit( inp ):\n",
    "    tp = inp[1][1]\n",
    "    tn = inp[0][0]\n",
    "    fp = inp[0][1] \n",
    "    fn = inp[1][0]\n",
    "    \n",
    "    acc = (tp+tn)*1.0 / ( tp+tn+fp+fn)*1.0\n",
    "    recall = tp*1.0/ ( tp+fn ) *1.0\n",
    "    prec = tp*1.0 / ( tp+fp )*1.0\n",
    "    \n",
    "#     F= 2.0*( prec* recall )/ (prec+recall)\n",
    "    F= 2.0*( tp)/ (2*tp + fp + fn)\n",
    "    \n",
    "    return acc, recall, prec, F\n",
    "\n",
    "def acc_match( true, pred ):\n",
    "    \"\"\"\n",
    "    returns exact mathc accuracy\n",
    "    \"\"\"\n",
    " \n",
    "    return (len( [ x  for x  in  [np.sum(np.abs( true[i]- pred[i] )) for i in range(len(true))] if x  == 0]))*1.0 / len(true)\n",
    "\n",
    "\n",
    "# def acc_none_zero ( true, pred ):\n",
    "    \n",
    "\n",
    "def acc_match_wierd( true, pred ):\n",
    "    \"\"\"\n",
    "    returns exact mathc accuracy\n",
    "    \"\"\"\n",
    "    level = 6 \n",
    "    switch = 11\n",
    "    threeAxis=13\n",
    "    accel = 0 \n",
    "    status=10\n",
    "    contact=5\n",
    "    \n",
    "    counter  = 0 \n",
    "    for i in range( len (true) ):\n",
    "        if np.sum(np.abs( true[i]- pred[i] ))==0 :\n",
    "            counter+=1\n",
    "        else : \n",
    "            t_rec = np.array(list( pred[i]))\n",
    "            \n",
    "            if true[i][level]==1 and true[i][switch]==1 and t_rec[level]==1 :\n",
    "                t_rec[switch]=1\n",
    "            \n",
    "            if true[i][threeAxis]==1 and true[i][accel]==1 and t_rec[threeAxis]==1:\n",
    "                t_rec[accel] =1\n",
    "            \n",
    "            if true[i][status]==1 and true[i][contact]==1 and t_rec[status]==1:\n",
    "                t_rec[contact]=1\n",
    "#             print(t_rec , true[i])    \n",
    "            if np.sum(np.abs( true[i]- t_rec ))==0 :\n",
    "                counter+=1   \n",
    "            \n",
    "             \n",
    "            \n",
    "    \n",
    "    return counter*1.0 / len(true)\n",
    "\n",
    "\n",
    "def print_info(y_test, pred , classes , confidance=0.5 ):\n",
    "    \n",
    "    counts = np.sum( y_test.astype(int) , axis=0)\n",
    "    \n",
    "    pred[pred>=confidance] = 1\n",
    "    pred[pred<confidance] = 0\n",
    "    \n",
    "#     acc_wierd  =acc_match_wierd(y_test, pred)\n",
    "    \n",
    "    conf= multilabel_confusion_matrix( y_test , pred.astype(int), labels= range(len(classes)))\n",
    "    accs = [make_recall_shit(x) for x in conf]\n",
    "    print( \"%30s  %8s   %8s  %8s  %8s %8s %22s\"  %( \"Class\",\"Accuracy\", \"Recall\",\"Precision\",\"F Score\" , \"Count\", \"TP/TN/FP/FN\"))\n",
    "    print( \"------------------------------------------------------------------------\" )\n",
    "    \n",
    "    for index in range(len(classes)):\n",
    "        tp = conf[index][1][1]\n",
    "        tn = conf[index][0][0]\n",
    "        fp = conf[index][0][1] \n",
    "        fn = conf[index][1][0]\n",
    "        print( \"%30s  %8.3f   %8.3f  %8.3f  %8.3f  %8d %5d/%5d/%5d/%5d\"  %\n",
    "             (classes[index],\n",
    "              accs[index][0],\n",
    "              accs[index][1],\n",
    "              accs[index][2],\n",
    "              accs[index][3],\n",
    "              counts[index],\n",
    "                  tp ,\n",
    "                tn ,\n",
    "                fp ,\n",
    "                fn ))\n",
    "    n_zeros_true = len([ x  for x  in  [np.sum(np.abs( y_test[i] )) for i in range(len(y_test))] if x  == 0]  )\n",
    "    n_zeros_pred = len([ x  for x  in  [np.sum(np.abs( pred[i] )) for i in range(len(pred))] if x  == 0]  )\n",
    "    \n",
    "    accs = np.nan_to_num(accs)\n",
    "    \n",
    "    print (\"------------------------------------------------------------------------\")\n",
    "    print( \"%30s  %8.3f   %8.3f  %8.3f  %8.3f  %8d %5d/%5d/%5d/%5d\"  %\n",
    "             (\"AVERAGES\",\n",
    "              np.average( accs, axis=0)[0],\n",
    "              np.average( accs, axis=0)[1],\n",
    "              np.average( accs, axis=0)[2],\n",
    "              np.average( accs, axis=0)[3],\n",
    "              len(y_test),\n",
    "                  0 ,\n",
    "                0,\n",
    "                0 ,\n",
    "                0 ))\n",
    "    \n",
    "    print ( \"Exact Match ACC : %.5f \" % acc_match( y_test, pred )  )\n",
    "#     print ( \"Wierd Exact Match ACC : %.5f\" % acc_wierd)\n",
    "    print ( \"Total Records : %d \" % len(y_test)  )\n",
    "    print ( \"Total ZXeros in True : %d (%.3f)%%\" % (n_zeros_true ,  n_zeros_true * 1.0/ len(y_test)  ))\n",
    "    print ( \"Total ZXeros in Test : %d (%.3f)%%\" % (n_zeros_pred ,  n_zeros_pred * 1.0/ len(y_test)  ) )\n",
    "    print ('=============================================================================')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def f1_perRow(y_true, y_pred):\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "\n",
    "def f1_perClass(y_true, y_pred):\n",
    "\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss_perClass(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "def f1_loss_perRow(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "def make_readable_results ( inp , classes , conffidance=True):\n",
    "    ret = [] \n",
    "    inp =inp.astype(int)\n",
    "    for xx in range(len(inp)) :\n",
    "        u = inp[xx]\n",
    "        temp = []\n",
    "        for j in range(len(u)) : \n",
    "            if u[j] >0:\n",
    "                temp.append(classes[j])\n",
    "        ret.append(temp)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def makeReadable( model , data, gt, path , classes, x, confidance=0.5):\n",
    "    #collect across multi models\n",
    "    from keras.models import load_model\n",
    "    x=[]\n",
    "    for i in range(0,16):\n",
    "        model=load_model('number'+str(i)+'.h5',custom_objects={'f1_perRow':f1_perRow,'f1_perClass':f1_perClass,'f1_loss_perRow':f1_loss_perRow,'f1_loss_perClass':f1_loss_perClass})\n",
    "        x.append(model.predict(data))\n",
    "    x=np.array(x)\n",
    "    x=np.transpose(x)\n",
    "    x=np.squeeze(x)\n",
    "    print(x.shape)\n",
    "        \n",
    "        \n",
    "    pred_temp  = x\n",
    "    #pred_temp = model.predict(data)\n",
    "    print_info(gt, x, classes , confidance=confidance)\n",
    "#     print( len(classes ), len( pred_temp[0] ) )\n",
    "#     xcc= make_readable_results(pred_temp , classes)\n",
    "#     y_gt = make_readable_results( gt, classes )\n",
    "#     temp_dic = {} \n",
    "#     for pick in range(len(xcc)): \n",
    "#         temp_dic[ pick +1 ] =  { 'seq': str(data[pick]),\n",
    "#                                'pred': xcc[pick],\n",
    "#                                 'true':y_gt[pick]\n",
    "#                                }   \n",
    "\n",
    "#     with open(path , 'w') as f:\n",
    "#         json.dump(temp_dic , f, indent=4)\n",
    "\n",
    "\n",
    "# def makeReadable( model , data, gt, path , classes, confidance=0.7, x):\n",
    "#     pred_temp = model.predict( data)\n",
    "#     print_info(gt, pred_temp, classes , confidance=confidance)\n",
    "#     xcc= make_readable_results( pred_temp , classes )\n",
    "#     temp_dic = {} \n",
    "#     for pick in range(len(xcc)): \n",
    "#         temp_dic[ pick +1 ] = xcc[pick]  \n",
    "\n",
    "#     with open(path , 'w') as f:\n",
    "#         json.dump(temp_dic , f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcualte per class accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest baseline calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_size= 50\n",
    "x_random_forest_train,y_random_forest_train, _ = pre_process_raw( x_train, y_train , dim_size, zero_pad=True, normalize=False, classes=classes)\n",
    "# x_random_forest_test,y_random_forest_test, _ = pre_process_raw( x_test[0], y_test[0] , dim_size, zero_pad=True, normalize=False, classes=classes)\n",
    "rf_tests  = [ pre_process_raw( x_test[i], y_test[i] , dim_size, zero_pad=True, normalize=False, classes=classes) for i in range(len(x_test)) ] \n",
    "# x,y, classes = pre_process_raw( x_data, y_data , dim_size, zero_pad=True, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=960, max_depth=9050,\n",
    "                             random_state=0 )\n",
    "t_hist = clf.fit(x_random_forest_train, y_random_forest_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(clf.feature_importances_)\n",
    "\n",
    "# print(clf.predict([[0, 0, 0, 0]]))\n",
    "# from sklearn import metrics\n",
    "# scores = cross_val_score(clf, x_random_forest_train, y_random_forest_train, cv=10, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================HOME Case : home_muhammed_final.json =============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omid/.conda/envs/iot/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n",
      "/home/omid/.conda/envs/iot/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if __name__ == '__main__':\n",
      "/home/omid/.conda/envs/iot/lib/python3.6/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                  acceleration     1.000      0.500     0.750     0.600         6     3/32062/    1/    3\n",
      "                      activity     0.999      0.729     0.854     0.787        48    35/32015/    6/   13\n",
      "                       battery     1.000        nan     0.000     0.000         0     0/32067/    2/    0\n",
      "                        button     1.000      0.857     0.857     0.857         7     6/32061/    1/    1\n",
      "              colorTemperature     1.000        nan       nan       nan         0     0/32069/    0/    0\n",
      "                       contact     0.999      0.718     0.862     0.783        78    56/31982/    9/   22\n",
      "                         level     0.995      0.815     0.129     0.222        27    22/31893/  149/    5\n",
      "                          lock     0.999      0.786     0.234     0.361        14    11/32019/   36/    3\n",
      "                        motion     0.971      0.183     0.834     0.300      1099   201/30930/   40/  898\n",
      "                          ping     0.997      0.997     0.992     0.994      6975  6951/25037/   57/   24\n",
      "                        status     1.000      0.920     0.920     0.920        50    46/32015/    4/    4\n",
      "                        switch     1.000      0.957     1.000     0.978        23    22/32046/    0/    1\n",
      "                   temperature     0.955      0.095     0.706     0.168      1512   144/30497/   60/ 1368\n",
      "                     threeAxis     0.999      0.625     0.250     0.357         8     5/32046/   15/    3\n",
      "                       unknown     0.897      0.984     0.882     0.930     22337 21990/ 6785/ 2947/  347\n",
      "                         water     1.000        nan     0.000     0.000         0     0/32066/    3/    0\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.988      0.573     0.579     0.516     32069     0/    0/    0/    0\n",
      "Exact Match ACC : 0.89304 \n",
      "Total Records : 32069 \n",
      "Total ZXeros in True : 811 (0.025)%\n",
      "Total ZXeros in Test : 173 (0.005)%\n",
      "=============================================================================\n",
      "==================HOME Case : home_os_final.json =============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omid/.conda/envs/iot/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                  acceleration     0.998      0.298     0.850     0.442        57    17/19908/    3/   40\n",
      "                      activity     0.999      0.684     0.684     0.684        19    13/19943/    6/    6\n",
      "                       battery     0.999      0.000     0.000     0.000         7     0/19958/    3/    7\n",
      "                        button     0.999      0.000     0.000     0.000        14     0/19953/    1/   14\n",
      "              colorTemperature     1.000      0.667     1.000     0.800         6     4/19962/    0/    2\n",
      "                       contact     0.997      0.670     0.922     0.776       176   118/19782/   10/   58\n",
      "                         level     0.986      0.704     0.065     0.119        27    19/19668/  273/    8\n",
      "                          lock     0.997      0.771     0.386     0.514        35    27/19890/   43/    8\n",
      "                        motion     0.981      0.186     0.486     0.269       371    69/19524/   73/  302\n",
      "                          ping     0.997      0.994     0.992     0.993      4842  4814/15087/   39/   28\n",
      "                        status     0.998      0.723     0.851     0.782       119    86/19834/   15/   33\n",
      "                        switch     0.999      0.619     0.867     0.722        21    13/19945/    2/    8\n",
      "                   temperature     0.938      0.054     0.330     0.093      1168    63/18672/  128/ 1105\n",
      "                     threeAxis     0.998      0.460     0.744     0.569        63    29/19895/   10/   34\n",
      "                       unknown     0.883      0.951     0.883     0.915     13305 12649/ 4983/ 1680/  656\n",
      "                         water     1.000        nan     0.000     0.000         0     0/19964/    4/    0\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.986      0.486     0.566     0.480     19968     0/    0/    0/    0\n",
      "Exact Match ACC : 0.86704 \n",
      "Total Records : 19968 \n",
      "Total ZXeros in True : 452 (0.023)%\n",
      "Total ZXeros in Test : 396 (0.020)%\n",
      "=============================================================================\n",
      "==================HOME Case : home_os_final_reduced.json =============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omid/.conda/envs/iot/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if __name__ == '__main__':\n",
      "/home/omid/.conda/envs/iot/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n",
      "/home/omid/.conda/envs/iot/lib/python3.6/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                  acceleration     0.998      0.200     1.000     0.333        25     5/ 9084/    0/   20\n",
      "                      activity     0.998      0.381     0.800     0.516        21     8/ 9086/    2/   13\n",
      "                       battery     1.000      0.000       nan     0.000         4     0/ 9105/    0/    4\n",
      "                        button     1.000      0.000     0.000     0.000         3     0/ 9105/    1/    3\n",
      "              colorTemperature     1.000        nan       nan       nan         0     0/ 9109/    0/    0\n",
      "                       contact     0.996      0.615     0.923     0.738        78    48/ 9027/    4/   30\n",
      "                         level     0.986      0.583     0.053     0.097        12     7/ 8972/  125/    5\n",
      "                          lock     0.997      0.545     0.414     0.471        22    12/ 9070/   17/   10\n",
      "                        motion     0.976      0.156     0.507     0.239       218    34/ 8858/   33/  184\n",
      "                          ping     0.996      0.993     0.989     0.991      2237  2222/ 6848/   24/   15\n",
      "                        status     0.996      0.580     0.674     0.624        50    29/ 9045/   14/   21\n",
      "                        switch     1.000      0.500     0.750     0.600         6     3/ 9102/    1/    3\n",
      "                   temperature     0.929      0.044     0.378     0.080       630    28/ 8433/   46/  602\n",
      "                     threeAxis     0.998      0.440     0.786     0.564        25    11/ 9081/    3/   14\n",
      "                       unknown     0.873      0.955     0.865     0.908      5948  5679/ 2276/  885/  269\n",
      "                         water     1.000        nan     0.000     0.000         0     0/ 9108/    1/    0\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.984      0.375     0.509     0.385      9109     0/    0/    0/    0\n",
      "Exact Match ACC : 0.85772 \n",
      "Total Records : 9109 \n",
      "Total ZXeros in True : 168 (0.018)%\n",
      "Total ZXeros in Test : 158 (0.017)%\n",
      "=============================================================================\n",
      "==================HOME Case : home_sk_final.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                  acceleration     0.995      0.300     0.857     0.444        40    12/ 6362/    2/   28\n",
      "                      activity     1.000        nan       nan       nan         0     0/ 6404/    0/    0\n",
      "                       battery     1.000      0.000       nan     0.000         2     0/ 6402/    0/    2\n",
      "                        button     0.991      0.067     1.000     0.125        60     4/ 6344/    0/   56\n",
      "              colorTemperature     1.000      0.000       nan     0.000         2     0/ 6402/    0/    2\n",
      "                       contact     0.987      0.526     0.989     0.687       175    92/ 6228/    1/   83\n",
      "                         level     0.993      0.424     0.757     0.544        66    28/ 6329/    9/   38\n",
      "                          lock     0.999        nan     0.000     0.000         0     0/ 6399/    5/    0\n",
      "                        motion     0.980      0.248     0.692     0.365       145    36/ 6243/   16/  109\n",
      "                          ping     0.998      0.997     0.997     0.997      2307  2300/ 4089/    8/    7\n",
      "                        status     0.996      0.786     0.988     0.876       103    81/ 6300/    1/   22\n",
      "                        switch     0.997      0.500     0.706     0.585        24    12/ 6375/    5/   12\n",
      "                   temperature     0.967      0.064     0.351     0.108       203    13/ 6177/   24/  190\n",
      "                     threeAxis     0.996      0.543     0.806     0.649        46    25/ 6352/    6/   21\n",
      "                       unknown     0.926      0.984     0.894     0.937      3558  3500/ 2433/  413/   58\n",
      "                         water     1.000        nan       nan       nan         0     0/ 6404/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.989      0.340     0.565     0.395      6404     0/    0/    0/    0\n",
      "Exact Match ACC : 0.90256 \n",
      "Total Records : 6404 \n",
      "Total ZXeros in True : 80 (0.012)%\n",
      "Total ZXeros in Test : 105 (0.016)%\n",
      "=============================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omid/.conda/envs/iot/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n",
      "/home/omid/.conda/envs/iot/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if __name__ == '__main__':\n",
      "/home/omid/.conda/envs/iot/lib/python3.6/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(rf_tests)) :\n",
    "    print( \"==================HOME Case : %s =============\" % test_names[ i] )\n",
    "    rf_pred= clf.predict( rf_tests[i][0])\n",
    "    print_info( rf_tests[i][1], rf_pred, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================HOME Case : home_muhammed_final.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count      TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                  acceleration     1.000      0.500     0.750     0.600         6 3/32062/1/3\n",
      "                          lock     0.999      0.786     0.234     0.361        14 11/32019/36/3\n",
      "                   temperature     0.971      0.183     0.834     0.300      1099 201/30930/40/898\n",
      "                        motion     0.956      0.095     0.719     0.167      1512 143/30501/56/1369\n",
      "                       unknown     0.926      0.993     0.931     0.961     29465 29268/441/2163/197\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.970      0.511     0.694     0.478     32069 0/0/0/0\n",
      "Exact Match ACC : 0.92354 \n",
      "Total Records : 32069 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 151 (0.005)%\n",
      "=============================================================================\n",
      "==================HOME Case : home_os_final.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count      TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                  acceleration     0.998      0.333     0.864     0.481        57 19/19908/3/38\n",
      "                          lock     0.997      0.771     0.380     0.509        35 27/19889/44/8\n",
      "                   temperature     0.981      0.189     0.479     0.271       371 70/19521/76/301\n",
      "                        motion     0.938      0.054     0.333     0.093      1168 63/18674/126/1105\n",
      "                       unknown     0.918      0.979     0.935     0.956     18352 17973/357/1259/379\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.967      0.465     0.598     0.462     19968 0/0/0/0\n",
      "Exact Match ACC : 0.90880 \n",
      "Total Records : 19968 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 309 (0.015)%\n",
      "=============================================================================\n",
      "==================HOME Case : home_os_final_reduced.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count      TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                  acceleration     0.998      0.280     1.000     0.438        25 7/9084/0/18\n",
      "                          lock     0.997      0.545     0.400     0.462        22 12/9069/18/10\n",
      "                   temperature     0.976      0.156     0.507     0.239       218 34/8858/33/184\n",
      "                        motion     0.929      0.044     0.378     0.080       630 28/8433/46/602\n",
      "                       unknown     0.906      0.983     0.919     0.950      8236 8097/160/713/139\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.961      0.402     0.641     0.433      9109 0/0/0/0\n",
      "Exact Match ACC : 0.89757 \n",
      "Total Records : 9109 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 121 (0.013)%\n",
      "=============================================================================\n",
      "==================HOME Case : home_sk_final.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count      TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                  acceleration     0.995      0.275     0.846     0.415        40 11/6362/2/29\n",
      "                          lock     0.999        nan     0.000     0.000         0 0/6400/4/0\n",
      "                   temperature     0.980      0.248     0.692     0.365       145 36/6243/16/109\n",
      "                        motion     0.967      0.059     0.353     0.101       203 12/6179/22/191\n",
      "                       unknown     0.946      0.991     0.954     0.972      6025 5969/89/290/56\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.978      0.315     0.569     0.371      6404 0/0/0/0\n",
      "Exact Match ACC : 0.94066 \n",
      "Total Records : 6404 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 42 (0.007)%\n",
      "=============================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omid/.conda/envs/iot/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for i in range(len(rf_tests)) :\n",
    "    print( \"==================HOME Case : %s =============\" % test_names[ i] )\n",
    "    rf_pred= clf.predict( rf_tests[i][0])\n",
    "    print_info( rf_tests[i][1], rf_pred, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makeReadable( data=rf_tests[0][0], model=clf, classes=classes, confidance=0.7,gt=rf_tests[0][1], path='sk_home_out.json' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_info(y_random_forest_test, rf_pred, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_info(y_random_forest_test, rf_pred, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print ( \"mean : %f \\nstd: %f\\nmax:%f\" %( scores.mean(), scores.std(), scores.max()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnts  = np.unique(np.array([ len(x) for x  in x_train ]), return_counts=True)\n",
    "# # np.sort( cnts[1] )\n",
    "# cnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "dim_size =15\n",
    "x_lstm_prossed_train,y_lstm_prossed_train, _ = pre_process_raw( x_train, y_train , dim_size, zero_pad=False, normalize=False,classes=classes)\n",
    "_, y_s_lstm_processed_train ,_ =  pre_process_raw( x_train, y_train_service , dim_size, zero_pad=False, normalize=False,classes=service_classes)\n",
    "# x_lstm_prossed_test,y_lstm_prossed_test, _ = pre_process_raw( x_test, y_test_2 , dim_size, zero_pad=True, normalize=False,classes=classes)\n",
    "lstm_tests  = [ pre_process_raw( x_test[i], y_test[i] , dim_size, zero_pad=False, normalize=False, classes=classes) for i in range(len(x_test)) ] \n",
    "lstm_tests_services  = [ pre_process_raw( x_test[i], y_test_service[i] , dim_size, zero_pad=False, normalize=True, classes=service_classes) for i in range(len(x_test)) ] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32069 32069\n",
      "19968 19968\n",
      "9109 9109\n",
      "6404 6404\n"
     ]
    }
   ],
   "source": [
    "for i in range( len(lstm_tests) ):\n",
    "    print( len( lstm_tests[i][0] ), len( lstm_tests_services[i][0] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_lstm_prossed_test2 = np.expand_dims(x_lstm_prossed_test,axis=1)\n",
    "# x_lstm_prossed_train2 =np.expand_dims(x_lstm_prossed_train,axis=1)\n",
    "\n",
    "for tt  in range( len(lstm_tests) ):\n",
    "    lstm_tests[tt]= (lstm_tests[tt][0].reshape(len(lstm_tests[tt][0]),dim_size,1) , lstm_tests[tt][1],lstm_tests_services[tt][1] )\n",
    "# x_lstm_prossed_test2 = x_lstm_prossed_test.reshape(len(x_lstm_prossed_test),dim_size,1)\n",
    "x_lstm_prossed_train2 =x_lstm_prossed_train.reshape(len(x_lstm_prossed_train),dim_size,1)\n",
    "\n",
    "# y_lstm_prossed_test2 = y_lstm_prossed_test.reshape(len(y_lstm_prossed_test),len(classes),1)\n",
    "# y_lstm_prossed_train2 =y_lstm_prossed_train.reshape(len(y_lstm_prossed_train),len(classes),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y_s_lstm_processed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 15, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 15, 128)      512         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 15, 128)      512         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 15, 128)      0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 15, 128)      512         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 15, 128)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 15, 128)      512         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 15, 128)      49280       dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 15, 128)      0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 15, 128)      49280       conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 15, 128)      0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 15, 128)      512         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 15, 128)      49280       dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 15, 128)      0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 15, 100)      91600       conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 15, 128)      0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 15, 100)      40800       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 15, 128)      12928       lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 15, 128)      49280       dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 15, 128)      12928       lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 15, 128)      16512       dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 15, 128)      49280       conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 15, 128)      16512       dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 15, 128)      16512       dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 15, 128)      512         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 15, 128)      16512       dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 15, 128)      0           dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 15, 128)      0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 15, 128)      0           dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 1920)         0           dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 15, 128)      0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 1920)         0           dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 128)          245888      flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 15, 128)      49280       dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 128)          245888      flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 128)          0           dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 1920)         0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 128)          0           dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mergerguy (Concatenate)         (None, 2176)         0           dropout_11[0][0]                 \n",
      "                                                                 flatten_6[0][0]                  \n",
      "                                                                 dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 128)          278656      mergerguy[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 128)          16512       dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 128)          16512       dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "to_service1 (Dense)             (None, 130)          16770       dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "to_service2 (Dense)             (None, 130)          17030       to_service1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "service_output (Dense)          (None, 5)            655         to_service2[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,360,967\n",
      "Trainable params: 1,359,943\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "57867/57867 [==============================] - 7s 126us/step - loss: 14.3652 - f1_perRow: 0.1974 - f1_perClass: 0.7354 - acc: 0.8820\n",
      "Epoch 2/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 7.5872 - f1_perRow: 0.2262 - f1_perClass: 0.8678 - acc: 0.9172\n",
      "Epoch 3/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 6.7831 - f1_perRow: 0.2323 - f1_perClass: 0.8763 - acc: 0.9172\n",
      "Epoch 4/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 6.5687 - f1_perRow: 0.2327 - f1_perClass: 0.8895 - acc: 0.9172\n",
      "Epoch 5/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 6.3217 - f1_perRow: 0.2481 - f1_perClass: 0.8857 - acc: 0.9172\n",
      "Epoch 6/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 6.1229 - f1_perRow: 0.2698 - f1_perClass: 0.8878 - acc: 0.9172\n",
      "Epoch 7/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 6.0202 - f1_perRow: 0.2943 - f1_perClass: 0.8918 - acc: 0.9172\n",
      "Epoch 8/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 5.9412 - f1_perRow: 0.3064 - f1_perClass: 0.8912 - acc: 0.9172\n",
      "Epoch 9/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 5.8804 - f1_perRow: 0.3094 - f1_perClass: 0.8918 - acc: 0.9172\n",
      "Epoch 10/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 5.8207 - f1_perRow: 0.3120 - f1_perClass: 0.8915 - acc: 0.9172\n",
      "Epoch 11/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 5.7701 - f1_perRow: 0.3122 - f1_perClass: 0.8920 - acc: 0.9172\n",
      "Epoch 12/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 5.7323 - f1_perRow: 0.3172 - f1_perClass: 0.8934 - acc: 0.9172\n",
      "Epoch 13/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 5.6949 - f1_perRow: 0.3197 - f1_perClass: 0.8936 - acc: 0.9172\n",
      "Epoch 14/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 5.6431 - f1_perRow: 0.3281 - f1_perClass: 0.8936 - acc: 0.9172\n",
      "Epoch 15/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 5.5801 - f1_perRow: 0.3351 - f1_perClass: 0.8950 - acc: 0.9172\n",
      "Epoch 16/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 5.5191 - f1_perRow: 0.3418 - f1_perClass: 0.8954 - acc: 0.9172\n",
      "Epoch 17/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 5.4717 - f1_perRow: 0.3467 - f1_perClass: 0.8960 - acc: 0.9172\n",
      "Epoch 18/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 5.4608 - f1_perRow: 0.3586 - f1_perClass: 0.8961 - acc: 0.9172\n",
      "Epoch 19/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 5.4265 - f1_perRow: 0.3588 - f1_perClass: 0.8980 - acc: 0.9172\n",
      "Epoch 20/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 5.4008 - f1_perRow: 0.3686 - f1_perClass: 0.8980 - acc: 0.9172\n",
      "Epoch 21/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 5.4103 - f1_perRow: 0.3648 - f1_perClass: 0.8975 - acc: 0.9172\n",
      "Epoch 22/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 5.4272 - f1_perRow: 0.3688 - f1_perClass: 0.8965 - acc: 0.9172\n",
      "Epoch 23/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 5.4180 - f1_perRow: 0.3714 - f1_perClass: 0.8975 - acc: 0.9172\n",
      "Epoch 24/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 5.4186 - f1_perRow: 0.3648 - f1_perClass: 0.8982 - acc: 0.9172\n",
      "Epoch 25/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 5.3995 - f1_perRow: 0.3738 - f1_perClass: 0.8973 - acc: 0.9172\n",
      "Epoch 26/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 5.3698 - f1_perRow: 0.3816 - f1_perClass: 0.8983 - acc: 0.9172\n",
      "Epoch 27/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 5.3509 - f1_perRow: 0.3817 - f1_perClass: 0.9007 - acc: 0.9172\n",
      "Epoch 28/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 5.3313 - f1_perRow: 0.3780 - f1_perClass: 0.8985 - acc: 0.9172\n",
      "Epoch 29/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 5.3007 - f1_perRow: 0.3888 - f1_perClass: 0.8997 - acc: 0.9172\n",
      "Epoch 30/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 5.2945 - f1_perRow: 0.3909 - f1_perClass: 0.8990 - acc: 0.9172\n",
      "Epoch 31/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 5.3095 - f1_perRow: 0.3898 - f1_perClass: 0.8983 - acc: 0.9173\n",
      "Epoch 32/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 5.2825 - f1_perRow: 0.3919 - f1_perClass: 0.9001 - acc: 0.9172\n",
      "Epoch 33/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 5.2764 - f1_perRow: 0.3925 - f1_perClass: 0.8998 - acc: 0.9173\n",
      "Epoch 34/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 5.2888 - f1_perRow: 0.3929 - f1_perClass: 0.8994 - acc: 0.9178\n",
      "Epoch 35/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 5.2633 - f1_perRow: 0.4009 - f1_perClass: 0.9003 - acc: 0.9182\n",
      "Epoch 36/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 5.2552 - f1_perRow: 0.3943 - f1_perClass: 0.9009 - acc: 0.9183\n",
      "Epoch 37/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 5.2413 - f1_perRow: 0.4000 - f1_perClass: 0.9006 - acc: 0.9194\n",
      "Epoch 38/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 5.2559 - f1_perRow: 0.3966 - f1_perClass: 0.8993 - acc: 0.9180\n",
      "Epoch 39/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 5.2315 - f1_perRow: 0.4022 - f1_perClass: 0.9000 - acc: 0.9197\n",
      "Epoch 40/100\n",
      "57867/57867 [==============================] - 2s 39us/step - loss: 5.2667 - f1_perRow: 0.3990 - f1_perClass: 0.9005 - acc: 0.9191\n",
      "Epoch 41/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 5.2752 - f1_perRow: 0.4029 - f1_perClass: 0.9003 - acc: 0.9192\n",
      "Epoch 42/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 5.2155 - f1_perRow: 0.4003 - f1_perClass: 0.9011 - acc: 0.9197\n",
      "Epoch 43/100\n",
      "57867/57867 [==============================] - 2s 40us/step - loss: 5.2184 - f1_perRow: 0.4034 - f1_perClass: 0.9005 - acc: 0.9200\n",
      "Epoch 44/100\n",
      "57867/57867 [==============================] - 2s 38us/step - loss: 5.2254 - f1_perRow: 0.4051 - f1_perClass: 0.9007 - acc: 0.9194\n",
      "Epoch 45/100\n",
      "57867/57867 [==============================] - 2s 38us/step - loss: 5.2356 - f1_perRow: 0.3977 - f1_perClass: 0.9007 - acc: 0.9200\n",
      "Epoch 46/100\n",
      "57867/57867 [==============================] - 2s 39us/step - loss: 5.2678 - f1_perRow: 0.4086 - f1_perClass: 0.9007 - acc: 0.9201\n",
      "Epoch 47/100\n",
      "57867/57867 [==============================] - 2s 40us/step - loss: 5.3152 - f1_perRow: 0.3972 - f1_perClass: 0.8994 - acc: 0.9194\n",
      "Epoch 48/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 5.3042 - f1_perRow: 0.3964 - f1_perClass: 0.9009 - acc: 0.9201\n",
      "Epoch 49/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 5.2601 - f1_perRow: 0.3987 - f1_perClass: 0.8998 - acc: 0.9197\n",
      "Epoch 50/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 5.2282 - f1_perRow: 0.4052 - f1_perClass: 0.8995 - acc: 0.9203\n",
      "Epoch 51/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 5.2221 - f1_perRow: 0.4057 - f1_perClass: 0.9002 - acc: 0.9203\n",
      "Epoch 52/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 5.2013 - f1_perRow: 0.4010 - f1_perClass: 0.8998 - acc: 0.9202\n",
      "Epoch 53/100\n",
      "57867/57867 [==============================] - 2s 38us/step - loss: 5.2174 - f1_perRow: 0.4107 - f1_perClass: 0.9007 - acc: 0.9213\n",
      "Epoch 54/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 5.2324 - f1_perRow: 0.3995 - f1_perClass: 0.9000 - acc: 0.9211\n",
      "Epoch 55/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 5.1980 - f1_perRow: 0.4117 - f1_perClass: 0.9021 - acc: 0.9212\n",
      "Epoch 56/100\n",
      "57867/57867 [==============================] - 2s 40us/step - loss: 5.1982 - f1_perRow: 0.4037 - f1_perClass: 0.9002 - acc: 0.9200\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57867/57867 [==============================] - 2s 37us/step - loss: 5.1831 - f1_perRow: 0.4096 - f1_perClass: 0.8996 - acc: 0.9215\n",
      "Epoch 58/100\n",
      "57867/57867 [==============================] - 2s 40us/step - loss: 5.1650 - f1_perRow: 0.4091 - f1_perClass: 0.9008 - acc: 0.9209\n",
      "Epoch 59/100\n",
      "57867/57867 [==============================] - 2s 39us/step - loss: 5.1894 - f1_perRow: 0.4127 - f1_perClass: 0.9009 - acc: 0.9211\n",
      "Epoch 60/100\n",
      "57867/57867 [==============================] - 2s 38us/step - loss: 5.1781 - f1_perRow: 0.4072 - f1_perClass: 0.9003 - acc: 0.9233\n",
      "Epoch 61/100\n",
      "57867/57867 [==============================] - 2s 38us/step - loss: 5.2008 - f1_perRow: 0.4026 - f1_perClass: 0.8996 - acc: 0.9204\n",
      "Epoch 62/100\n",
      "57867/57867 [==============================] - 2s 39us/step - loss: 5.2252 - f1_perRow: 0.4075 - f1_perClass: 0.9027 - acc: 0.9212\n",
      "Epoch 63/100\n",
      "57867/57867 [==============================] - 2s 38us/step - loss: 5.2716 - f1_perRow: 0.4076 - f1_perClass: 0.8980 - acc: 0.9236\n",
      "Epoch 64/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 5.2377 - f1_perRow: 0.3994 - f1_perClass: 0.9005 - acc: 0.9208\n",
      "Epoch 65/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 5.1761 - f1_perRow: 0.4089 - f1_perClass: 0.9010 - acc: 0.9210\n",
      "Epoch 66/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 5.1891 - f1_perRow: 0.4029 - f1_perClass: 0.9000 - acc: 0.9246\n",
      "Epoch 67/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 5.1755 - f1_perRow: 0.4106 - f1_perClass: 0.9019 - acc: 0.9230\n",
      "Epoch 68/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 5.1562 - f1_perRow: 0.4134 - f1_perClass: 0.9014 - acc: 0.9225\n",
      "Epoch 69/100\n",
      "57867/57867 [==============================] - 2s 41us/step - loss: 5.1544 - f1_perRow: 0.4093 - f1_perClass: 0.8995 - acc: 0.9241\n",
      "Epoch 70/100\n",
      "57867/57867 [==============================] - 2s 41us/step - loss: 5.1383 - f1_perRow: 0.4075 - f1_perClass: 0.9000 - acc: 0.9237\n",
      "Epoch 71/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 5.1590 - f1_perRow: 0.4167 - f1_perClass: 0.9013 - acc: 0.9233\n",
      "Epoch 72/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 5.1611 - f1_perRow: 0.4091 - f1_perClass: 0.9016 - acc: 0.9233\n",
      "Epoch 73/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 5.1532 - f1_perRow: 0.4132 - f1_perClass: 0.9003 - acc: 0.9232\n",
      "Epoch 74/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 5.1201 - f1_perRow: 0.4102 - f1_perClass: 0.9003 - acc: 0.9224\n",
      "Epoch 75/100\n",
      "57867/57867 [==============================] - 2s 40us/step - loss: 5.1443 - f1_perRow: 0.4128 - f1_perClass: 0.9013 - acc: 0.9224\n",
      "Epoch 76/100\n",
      "57867/57867 [==============================] - 2s 39us/step - loss: 5.1916 - f1_perRow: 0.4136 - f1_perClass: 0.8994 - acc: 0.9218\n",
      "Epoch 77/100\n",
      "57867/57867 [==============================] - 2s 40us/step - loss: 5.1527 - f1_perRow: 0.4095 - f1_perClass: 0.9011 - acc: 0.9221\n",
      "Epoch 78/100\n",
      "57867/57867 [==============================] - 2s 41us/step - loss: 5.1453 - f1_perRow: 0.4085 - f1_perClass: 0.9006 - acc: 0.9225\n",
      "Epoch 79/100\n",
      "57867/57867 [==============================] - 2s 40us/step - loss: 5.1221 - f1_perRow: 0.4148 - f1_perClass: 0.9010 - acc: 0.9227\n",
      "Epoch 80/100\n",
      "57867/57867 [==============================] - 2s 38us/step - loss: 5.1266 - f1_perRow: 0.4155 - f1_perClass: 0.9011 - acc: 0.9226\n",
      "Epoch 81/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 5.1274 - f1_perRow: 0.4130 - f1_perClass: 0.9023 - acc: 0.9207\n",
      "Epoch 82/100\n",
      "57867/57867 [==============================] - 2s 38us/step - loss: 5.1384 - f1_perRow: 0.4126 - f1_perClass: 0.9009 - acc: 0.9222\n",
      "Epoch 83/100\n",
      "57867/57867 [==============================] - 2s 40us/step - loss: 5.1644 - f1_perRow: 0.4117 - f1_perClass: 0.9007 - acc: 0.9226\n",
      "Epoch 84/100\n",
      "57867/57867 [==============================] - 2s 40us/step - loss: 5.1244 - f1_perRow: 0.4113 - f1_perClass: 0.9007 - acc: 0.9221\n",
      "Epoch 85/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 5.1820 - f1_perRow: 0.4096 - f1_perClass: 0.8989 - acc: 0.9245\n",
      "Epoch 86/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 5.1570 - f1_perRow: 0.4128 - f1_perClass: 0.9019 - acc: 0.9230\n",
      "Epoch 87/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 5.1232 - f1_perRow: 0.4142 - f1_perClass: 0.8999 - acc: 0.9226\n",
      "Epoch 88/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 5.1391 - f1_perRow: 0.4162 - f1_perClass: 0.9008 - acc: 0.9229\n",
      "Epoch 89/100\n",
      "57867/57867 [==============================] - 2s 38us/step - loss: 5.1555 - f1_perRow: 0.4090 - f1_perClass: 0.9019 - acc: 0.9220\n",
      "Epoch 90/100\n",
      "57867/57867 [==============================] - 2s 39us/step - loss: 5.1621 - f1_perRow: 0.4112 - f1_perClass: 0.8996 - acc: 0.9218\n",
      "Epoch 91/100\n",
      "57867/57867 [==============================] - 2s 40us/step - loss: 5.1233 - f1_perRow: 0.4196 - f1_perClass: 0.9017 - acc: 0.9252\n",
      "Epoch 92/100\n",
      "57867/57867 [==============================] - 2s 40us/step - loss: 5.1088 - f1_perRow: 0.4148 - f1_perClass: 0.8998 - acc: 0.9234\n",
      "Epoch 93/100\n",
      "57867/57867 [==============================] - 2s 41us/step - loss: 5.1048 - f1_perRow: 0.4133 - f1_perClass: 0.9013 - acc: 0.9219\n",
      "Epoch 94/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 5.1082 - f1_perRow: 0.4193 - f1_perClass: 0.9013 - acc: 0.9217\n",
      "Epoch 95/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 5.1212 - f1_perRow: 0.4142 - f1_perClass: 0.9004 - acc: 0.9229\n",
      "Epoch 96/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 5.1074 - f1_perRow: 0.4109 - f1_perClass: 0.9019 - acc: 0.9251\n",
      "Epoch 97/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 5.2335 - f1_perRow: 0.4146 - f1_perClass: 0.8990 - acc: 0.9226\n",
      "Epoch 98/100\n",
      "57867/57867 [==============================] - 2s 38us/step - loss: 5.1370 - f1_perRow: 0.4102 - f1_perClass: 0.9015 - acc: 0.9246\n",
      "Epoch 99/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 5.1218 - f1_perRow: 0.4115 - f1_perClass: 0.9003 - acc: 0.9227\n",
      "Epoch 100/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 5.0933 - f1_perRow: 0.4190 - f1_perClass: 0.9012 - acc: 0.9223\n"
     ]
    }
   ],
   "source": [
    "## Bidirectional LSTM :D \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Flatten\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import Input, merge, concatenate, Conv2D, MaxPooling2D, Activation, UpSampling2D, Dropout, Conv2DTranspose, UpSampling2D, Lambda\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization as bn\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.merge import add\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers.merge import add\n",
    "from keras.layers import Dense,Conv1D,Dropout,Activation,BatchNormalization,MaxPooling1D,Flatten,Masking,TimeDistributed\n",
    "from keras.layers.recurrent import LSTM,GRU,SimpleRNN\n",
    "from keras.models import Input,Sequential,Model\n",
    "from keras.layers.merge import add\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import MSE,MSLE\n",
    "model2 = Sequential()\n",
    "\n",
    "\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def f1_perRow(y_true, y_pred):\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "\n",
    "def f1_perClass(y_true, y_pred):\n",
    "\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss_perClass(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "def f1_loss_perRow(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inputs  = Input( ( dim_size,1 ) )\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(inputs)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# bi_d_1 = Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat') (inputs)\n",
    "lstm_1 =  LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True)(out)\n",
    "# lstm_2 = LSTM(30 ,  recurrent_dropout=0.14, return_sequences=True)(lstm_1)\n",
    "\n",
    "bi_d_1 =Dense(128, activation='relu')  (lstm_1)\n",
    "lstm_1 =  Dense(128, activation='relu')(bi_d_1)\n",
    "lstm_2 = Dense(128, activation='relu')(lstm_1)\n",
    "\n",
    "\n",
    "\n",
    "# td_1    = TimeDistributed(Dense(256, activation='relu'))(lstm_2)\n",
    "# dout_1  = Dropout(0.1)(td_1)\n",
    "dout_1  = Dropout(0.1)(lstm_2)\n",
    "flt_1   = Flatten()(dout_1)\n",
    "dense_1 = Dense(128, activation='relu')(flt_1)\n",
    "dout_2  = Dropout(0.2)(dense_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# bi_d_1 = Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat') (inputs)\n",
    "lstm_1 =  LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True)(inputs)\n",
    "# lstm_1 = LSTM(40 ,  recurrent_dropout=0.14, return_sequences=True)(lstm_1)\n",
    "\n",
    "bi_d_raw_1 =Dense(128, activation='relu')  (lstm_1)\n",
    "lstm_raw_1 =  Dense(128, activation='relu')(bi_d_raw_1)\n",
    "lstm_raw_2 = Dense(128, activation='relu')(lstm_raw_1)\n",
    "\n",
    "dout_1  = Dropout(0.1)(lstm_raw_2)\n",
    "flt_1   = Flatten()(dout_1)\n",
    "dense_1 = Dense(128, activation='relu')(flt_1)\n",
    "dout_3  = Dropout(0.2)(dense_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(inputs)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# out = Flatten()(out)\n",
    "# out = MaxPooling1D(2,padding='same', name ='pooling')(out)\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# out = Flatten()(out)\n",
    "# out = MaxPooling1D(2,padding='same', name ='pooling2')(out)\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# out = Flatten()(out)\n",
    "# out = MaxPooling1D(2,padding='same', name ='pooling')(out)\n",
    "\n",
    "\n",
    "\n",
    "# fl_out_1 = Flatten()(dout_2)\n",
    "\n",
    "fl_out_cnn = Flatten()(out)\n",
    "\n",
    "# out_new = concatenate( [fl_out_1, fl_out_cnn] , name='mergerguy')\n",
    "out_new = concatenate( [dout_2, fl_out_cnn,dout_3] , name='mergerguy')\n",
    "\n",
    "dens_out_1 = Dense( 128, activation='relu' )(out_new)\n",
    "dens_out_2 = Dense( 128, activation='relu' )(dens_out_1)\n",
    "dens_out_3 = Dense( 128, activation='relu' )(dens_out_2)\n",
    "\n",
    "# fl2  = Flatten()(out_new)\n",
    "\n",
    "out_put_final = Dense(len(classes), activation='sigmoid', name='Event_output')(dens_out_3)\n",
    "\n",
    "toService_1 = Dense( 130, name=\"to_service1\" )(dens_out_3)\n",
    "toService_1 = Dense( 130, name=\"to_service2\" )(toService_1)\n",
    "\n",
    "service_output = Dense(len(classes  ), activation=\"sigmoid\", name = 'service_output')(toService_1)\n",
    "\n",
    "\n",
    "\n",
    "model2 = Model(inputs=[inputs], outputs=[service_output])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model2.add(Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat'))\n",
    "# model2.add( LSTM(60 ,  recurrent_dropout=0.04, return_sequences=True))\n",
    "# model2.add( LSTM(30 ,  recurrent_dropout=0.14, return_sequences=True))\n",
    "# # model2.add(Bidirectional( LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True),merge_mode='concat'))\n",
    "# model2.add(TimeDistributed(Dense(256, activation='relu')))\n",
    "# model2.add(Dropout(0.1))\n",
    "# model2.add(Flatten())\n",
    "# model2.add(Dense(128, activation='relu'))\n",
    "# model2.add(Dropout(0.2))\n",
    "# model2.add(Dense(len(classes), activation='sigmoid'))\n",
    "\n",
    " \n",
    "weights = [\n",
    "1.0/(57.0 / len(y_train)),\n",
    "1.0/(19.0 / len(y_train)),\n",
    "1.0/(7.0 / len(y_train)),\n",
    "1.0/(14.0 / len(y_train)),\n",
    "1.0/(6.0 / len(y_train)),\n",
    "1.0/(176.0 / len(y_train)),\n",
    "1.0/(27.0 / len(y_train)),\n",
    "1.0/(35.0 / len(y_train)),\n",
    "1.0/(371.0 / len(y_train)),\n",
    "1.0/(11111.0 / len(y_train)),\n",
    "1.0/(4842.0 / len(y_train)),\n",
    "1.0/(119.0 / len(y_train)),\n",
    "1.0/(21.0 / len(y_train)),\n",
    "1.0/(1168.0 / len(y_train)),\n",
    "1.0/(63.0 / len(y_train)),\n",
    "1.0/(13305.0 / len(y_train)),\n",
    "1.0/(11111.0 / len(y_train)),\n",
    "]\n",
    "    \n",
    "\n",
    "\n",
    "# model2.compile(loss=weighted_categorical_crossentropy(weights=weights), optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "losses = {\n",
    "#     \"service_output\": f1_loss_perClass ,\n",
    "    \"service_output\": f1_loss_perRow ,\n",
    "    \"service_output\": \"categorical_crossentropy\",\n",
    "}\n",
    "lossWeights = {#\"service_output\": 20,\n",
    "               \"service_output\": 30.0 ,\n",
    "    \"service_output\": 20}\n",
    " \n",
    "\n",
    "\n",
    "model2.compile(loss=losses,loss_weights=lossWeights, optimizer='adam', metrics=[f1_perRow,f1_perClass,'acc'])\n",
    "# model2.compile(loss=losses, loss_weights=lossWeights, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('IoTDownNet', monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "print(model2.summary())\n",
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=100, batch_size=12500, shuffle=True, callbacks=callbacks_list)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================HOME Case : home_muhammed_final.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                  acceleration     0.999      0.167     0.062     0.091         6     1/32048/   15/    5\n",
      "                          lock     0.996      0.571     0.066     0.119        14     8/31942/  113/    6\n",
      "                   temperature     0.966      0.005     0.833     0.009      1099     5/30969/    1/ 1094\n",
      "                        motion     0.953      0.001     0.500     0.001      1512     1/30556/    1/ 1511\n",
      "                       unknown     0.919      1.000     0.919     0.958     29465 29465/    0/ 2604/    0\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.967      0.349     0.476     0.235     32069     0/    0/    0/    0\n",
      "Exact Match ACC : 0.91493 \n",
      "Total Records : 32069 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 0 (0.000)%\n",
      "=============================================================================\n",
      "==================HOME Case : home_os_final.json =============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omid/.conda/envs/iot/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                  acceleration     0.996      0.351     0.351     0.351        57    20/19874/   37/   37\n",
      "                          lock     0.996      0.771     0.255     0.383        35    27/19854/   79/    8\n",
      "                   temperature     0.981      0.000     0.000     0.000       371     0/19595/    2/  371\n",
      "                        motion     0.942      0.000       nan     0.000      1168     0/18800/    0/ 1168\n",
      "                       unknown     0.919      1.000     0.919     0.958     18352 18352/    0/ 1616/    0\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.967      0.424     0.305     0.338     19968     0/    0/    0/    0\n",
      "Exact Match ACC : 0.91361 \n",
      "Total Records : 19968 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 0 (0.000)%\n",
      "=============================================================================\n",
      "==================HOME Case : home_os_final_reduced.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                  acceleration     0.996      0.280     0.259     0.269        25     7/ 9064/   20/   18\n",
      "                          lock     0.995      0.545     0.245     0.338        22    12/ 9050/   37/   10\n",
      "                   temperature     0.976      0.000     0.000     0.000       218     0/ 8890/    1/  218\n",
      "                        motion     0.931      0.000       nan     0.000       630     0/ 8479/    0/  630\n",
      "                       unknown     0.904      1.000     0.904     0.950      8236  8236/    0/  873/    0\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.960      0.365     0.282     0.311      9109     0/    0/    0/    0\n",
      "Exact Match ACC : 0.89856 \n",
      "Total Records : 9109 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 0 (0.000)%\n",
      "=============================================================================\n",
      "==================HOME Case : home_sk_final.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                  acceleration     0.993      0.300     0.400     0.343        40    12/ 6346/   18/   28\n",
      "                          lock     0.998        nan     0.000     0.000         0     0/ 6388/   16/    0\n",
      "                   temperature     0.977      0.000       nan     0.000       145     0/ 6259/    0/  145\n",
      "                        motion     0.968      0.000       nan     0.000       203     0/ 6201/    0/  203\n",
      "                       unknown     0.941      1.000     0.941     0.970      6025  6025/    0/  379/    0\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.975      0.260     0.268     0.262      6404     0/    0/    0/    0\n",
      "Exact Match ACC : 0.93582 \n",
      "Total Records : 6404 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 0 (0.000)%\n",
      "=============================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omid/.conda/envs/iot/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for i in range(len(lstm_tests)) :\n",
    "    print( \"==================HOME Case : %s =============\" % test_names[i])\n",
    "    lstm_pred= model2.predict( lstm_tests[i][0])\n",
    "    \n",
    "#     print_info( lstm_tests[i][1], lstm_pred, classes , confidance=0.7)\n",
    "    print_info( lstm_tests[i][1], lstm_pred, classes , confidance=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fae0bbef518>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist2.history['f1_perClass'], c='red')\n",
    "plt.plot(hist2.history['loss'], c='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================HOME Case : home_muhammed_final.json =============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omid/.conda/envs/iot/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n",
      "/home/omid/.conda/envs/iot/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if __name__ == '__main__':\n",
      "/home/omid/.conda/envs/iot/lib/python3.6/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                  acceleration     1.000      0.000     0.000     0.000         6     0/32058/    5/    6\n",
      "                      activity     0.999      0.479     0.742     0.582        48    23/32013/    8/   25\n",
      "                       battery     1.000        nan       nan       nan         0     0/32069/    0/    0\n",
      "                        button     0.579      0.429     0.000     0.000         7     3/18566/13496/    4\n",
      "              colorTemperature     1.000        nan     0.000     0.000         0     0/32065/    4/    0\n",
      "                       contact     0.998      0.603     0.723     0.657        78    47/31973/   18/   31\n",
      "                         level     0.999      0.333     0.643     0.439        27     9/32037/    5/   18\n",
      "                          lock     0.989      0.500     0.020     0.039        14     7/31716/  339/    7\n",
      "                        motion     0.966      0.003     0.600     0.005      1099     3/30968/    2/ 1096\n",
      "                          ping     0.987      0.994     0.949     0.971      6975  6935/24725/  369/   40\n",
      "                        status     0.999      0.820     0.732     0.774        50    41/32004/   15/    9\n",
      "                        switch     1.000      0.696     0.941     0.800        23    16/32045/    1/    7\n",
      "                   temperature     0.953      0.002     0.750     0.004      1512     3/30556/    1/ 1509\n",
      "                     threeAxis     0.999      0.500     0.143     0.222         8     4/32037/   24/    4\n",
      "                       unknown     0.837      0.917     0.859     0.887     22337 20488/ 6362/ 3370/ 1849\n",
      "                         water     1.000        nan       nan       nan         0     0/32069/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.957      0.392     0.444     0.336     32069     0/    0/    0/    0\n",
      "Exact Match ACC : 0.46793 \n",
      "Total Records : 32069 \n",
      "Total ZXeros in True : 811 (0.025)%\n",
      "Total ZXeros in Test : 1911 (0.060)%\n",
      "=============================================================================\n",
      "==================HOME Case : home_os_final.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                  acceleration     0.997      0.053     0.176     0.081        57     3/19897/   14/   54\n",
      "                      activity     1.000      0.632     0.857     0.727        19    12/19947/    2/    7\n",
      "                       battery     1.000      0.000       nan     0.000         7     0/19961/    0/    7\n",
      "                        button     0.547      0.071     0.000     0.000        14     1/10913/ 9041/   13\n",
      "              colorTemperature     1.000      0.667     0.800     0.727         6     4/19961/    1/    2\n",
      "                       contact     0.995      0.585     0.851     0.694       176   103/19774/   18/   73\n",
      "                         level     0.999      0.222     0.462     0.300        27     6/19934/    7/   21\n",
      "                          lock     0.988      0.743     0.101     0.177        35    26/19701/  232/    9\n",
      "                        motion     0.981      0.000     0.000     0.000       371     0/19593/    4/  371\n",
      "                          ping     0.990      0.997     0.962     0.979      4842  4829/14933/  193/   13\n",
      "                        status     0.997      0.773     0.786     0.780       119    92/19824/   25/   27\n",
      "                        switch     1.000      0.619     0.929     0.743        21    13/19946/    1/    8\n",
      "                   temperature     0.942      0.000       nan     0.000      1168     0/18800/    0/ 1168\n",
      "                     threeAxis     0.996      0.492     0.425     0.456        63    31/19863/   42/   32\n",
      "                       unknown     0.847      0.925     0.856     0.889     13305 12309/ 4598/ 2065/  996\n",
      "                         water     1.000        nan       nan       nan         0     0/19968/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.955      0.424     0.450     0.410     19968     0/    0/    0/    0\n",
      "Exact Match ACC : 0.44666 \n",
      "Total Records : 19968 \n",
      "Total ZXeros in True : 452 (0.023)%\n",
      "Total ZXeros in Test : 1006 (0.050)%\n",
      "=============================================================================\n",
      "==================HOME Case : home_os_final_reduced.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                  acceleration     0.996      0.000     0.000     0.000        25     0/ 9077/    7/   25\n",
      "                      activity     0.998      0.381     0.889     0.533        21     8/ 9087/    1/   13\n",
      "                       battery     1.000      0.000       nan     0.000         4     0/ 9105/    0/    4\n",
      "                        button     0.552      0.000     0.000     0.000         3     0/ 5028/ 4078/    3\n",
      "              colorTemperature     1.000        nan       nan       nan         0     0/ 9109/    0/    0\n",
      "                       contact     0.995      0.551     0.860     0.672        78    43/ 9024/    7/   35\n",
      "                         level     0.999      0.250     1.000     0.400        12     3/ 9097/    0/    9\n",
      "                          lock     0.986      0.500     0.085     0.146        22    11/ 8969/  118/   11\n",
      "                        motion     0.976      0.000     0.000     0.000       218     0/ 8888/    3/  218\n",
      "                          ping     0.988      0.996     0.955     0.975      2237  2229/ 6768/  104/    8\n",
      "                        status     0.996      0.640     0.653     0.646        50    32/ 9042/   17/   18\n",
      "                        switch     1.000      0.500     1.000     0.667         6     3/ 9103/    0/    3\n",
      "                   temperature     0.931      0.000       nan     0.000       630     0/ 8479/    0/  630\n",
      "                     threeAxis     0.996      0.440     0.355     0.393        25    11/ 9064/   20/   14\n",
      "                       unknown     0.840      0.928     0.842     0.883      5948  5522/ 2128/ 1033/  426\n",
      "                         water     1.000        nan       nan       nan         0     0/ 9109/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.953      0.324     0.415     0.332      9109     0/    0/    0/    0\n",
      "Exact Match ACC : 0.44637 \n",
      "Total Records : 9109 \n",
      "Total ZXeros in True : 168 (0.018)%\n",
      "Total ZXeros in Test : 450 (0.049)%\n",
      "=============================================================================\n",
      "==================HOME Case : home_sk_final.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                  acceleration     0.993      0.025     0.250     0.045        40     1/ 6361/    3/   39\n",
      "                      activity     1.000        nan       nan       nan         0     0/ 6404/    0/    0\n",
      "                       battery     1.000      0.000       nan     0.000         2     0/ 6402/    0/    2\n",
      "                        button     0.628      0.067     0.002     0.003        60     4/ 4017/ 2327/   56\n",
      "              colorTemperature     1.000      0.000       nan     0.000         2     0/ 6402/    0/    2\n",
      "                       contact     0.986      0.503     1.000     0.669       175    88/ 6229/    0/   87\n",
      "                         level     0.990      0.045     0.600     0.085        66     3/ 6336/    2/   63\n",
      "                          lock     0.988        nan     0.000     0.000         0     0/ 6326/   78/    0\n",
      "                        motion     0.977      0.000       nan     0.000       145     0/ 6259/    0/  145\n",
      "                          ping     0.991      0.997     0.980     0.988      2307  2299/ 4049/   48/    8\n",
      "                        status     0.997      0.835     1.000     0.910       103    86/ 6301/    0/   17\n",
      "                        switch     0.996      0.042     0.250     0.071        24     1/ 6377/    3/   23\n",
      "                   temperature     0.968      0.000       nan     0.000       203     0/ 6201/    0/  203\n",
      "                     threeAxis     0.994      0.478     0.564     0.518        46    22/ 6341/   17/   24\n",
      "                       unknown     0.858      0.901     0.851     0.875      3558  3205/ 2287/  559/  353\n",
      "                         water     1.000        nan       nan       nan         0     0/ 6404/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.960      0.243     0.344     0.260      6404     0/    0/    0/    0\n",
      "Exact Match ACC : 0.52217 \n",
      "Total Records : 6404 \n",
      "Total ZXeros in True : 80 (0.012)%\n",
      "Total ZXeros in Test : 389 (0.061)%\n",
      "=============================================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(lstm_tests)) :\n",
    "    print( \"==================HOME Case : %s =============\" % test_names[i])\n",
    "    lstm_pred= model2.predict( lstm_tests[i][0])\n",
    "    \n",
    "#     print_info( lstm_tests[i][1], lstm_pred, classes , confidance=0.7)\n",
    "    print_info( lstm_tests[i][1], lstm_pred, classes , confidance=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fae0bc17e80>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist2.history['f1_perClass'], c='red')\n",
    "plt.plot(hist2.history['loss'], c='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bidirectional LSTM :D \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Flatten\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import Input, merge, concatenate, Conv2D, MaxPooling2D, Activation, UpSampling2D, Dropout, Conv2DTranspose, UpSampling2D, Lambda\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization as bn\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.merge import add\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers.merge import add\n",
    "from keras.layers import Dense,Conv1D,Dropout,Activation,BatchNormalization,MaxPooling1D,Flatten,Masking,TimeDistributed\n",
    "from keras.layers.recurrent import LSTM,GRU,SimpleRNN\n",
    "from keras.models import Input,Sequential,Model\n",
    "from keras.layers.merge import add\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import MSE,MSLE\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def f1_perRow(y_true, y_pred):\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "\n",
    "def f1_perClass(y_true, y_pred):\n",
    "\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss_perClass(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "def f1_loss_perRow(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "\n",
    "\n",
    "inputs  = Input( ( dim_size,1 ) )\n",
    "\n",
    "\n",
    "\n",
    "# bi_d_1 = Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat') (inputs)\n",
    "lstm_1 =  LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True)(inputs)\n",
    "# lstm_2 = LSTM(30 ,  recurrent_dropout=0.14, return_sequences=True)(lstm_1)\n",
    "\n",
    "bi_d_1 =Dense(128, activation='relu')  (lstm_1)\n",
    "lstm_1 =  Dense(128, activation='relu')(bi_d_1)\n",
    "lstm_2 = Dense(128, activation='relu')(lstm_1)\n",
    "\n",
    "\n",
    "\n",
    "# td_1    = TimeDistributed(Dense(256, activation='relu'))(lstm_2)\n",
    "# dout_1  = Dropout(0.1)(td_1)\n",
    "dout_1  = Dropout(0.1)(lstm_2)\n",
    "flt_1   = Flatten()(dout_1)\n",
    "dense_1 = Dense(128, activation='relu')(flt_1)\n",
    "dout_2  = Dropout(0.2)(dense_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dens_out_3 = Dense( 128, activation='relu' )(dout_2)\n",
    "\n",
    "\n",
    "# fl2  = Flatten()(out_new)\n",
    "\n",
    "out_put_final = Dense(len(classes), activation='sigmoid', name='Event_output')(dens_out_3)\n",
    "\n",
    "toService_1 = Dense( 130, name=\"to_service1\" )(dens_out_3)\n",
    "toService_1 = Dense( 130, name=\"to_service2\" )(toService_1)\n",
    "\n",
    "service_output = Dense(len(classes  ), activation=\"sigmoid\", name = 'service_output')(toService_1)\n",
    "\n",
    "\n",
    "\n",
    "model2 = Model(inputs=[inputs], outputs=[service_output])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model2.add(Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat'))\n",
    "# model2.add( LSTM(60 ,  recurrent_dropout=0.04, return_sequences=True))\n",
    "# model2.add( LSTM(30 ,  recurrent_dropout=0.14, return_sequences=True))\n",
    "# # model2.add(Bidirectional( LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True),merge_mode='concat'))\n",
    "# model2.add(TimeDistributed(Dense(256, activation='relu')))\n",
    "# model2.add(Dropout(0.1))\n",
    "# model2.add(Flatten())\n",
    "# model2.add(Dense(128, activation='relu'))\n",
    "# model2.add(Dropout(0.2))\n",
    "# model2.add(Dense(len(classes), activation='sigmoid'))\n",
    "\n",
    "# model2.compile(loss=weighted_categorical_crossentropy(weights=weights), optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "losses = {\n",
    "#     \"service_output\": f1_loss_perClass ,\n",
    "    \"service_output\": f1_loss_perRow ,\n",
    "    \"service_output\": \"categorical_crossentropy\",\n",
    "}\n",
    "lossWeights = {#\"service_output\": 20,\n",
    "               \"service_output\": 30.0 ,\n",
    "    \"service_output\": 20}\n",
    " \n",
    "\n",
    "\n",
    "#model2.compile(loss=losses,loss_weights=lossWeights, optimizer='adam', metrics=[f1_perRow,f1_perClass,'acc'])\n",
    "model2.compile(loss=losses, loss_weights=lossWeights, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('IoTDownNet', monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "print(model2.summary())\n",
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=1000, batch_size=12500, shuffle=True, callbacks=callbacks_list)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bidirectional LSTM :D \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Flatten\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import Input, merge, concatenate, Conv2D, MaxPooling2D, Activation, UpSampling2D, Dropout, Conv2DTranspose, UpSampling2D, Lambda\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization as bn\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.merge import add\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers.merge import add\n",
    "from keras.layers import Dense,Conv1D,Dropout,Activation,BatchNormalization,MaxPooling1D,Flatten,Masking,TimeDistributed\n",
    "from keras.layers.recurrent import LSTM,GRU,SimpleRNN\n",
    "from keras.models import Input,Sequential,Model\n",
    "from keras.layers.merge import add\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import MSE,MSLE\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def f1_perRow(y_true, y_pred):\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "\n",
    "def f1_perClass(y_true, y_pred):\n",
    "\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss_perClass(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "def f1_loss_perRow(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "\n",
    "\n",
    "inputs  = Input( ( dim_size,1 ) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(inputs)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# out = Flatten()(out)\n",
    "# out = MaxPooling1D(2,padding='same', name ='pooling')(out)\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# out = Flatten()(out)\n",
    "# out = MaxPooling1D(2,padding='same', name ='pooling2')(out)\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# out = Flatten()(out)\n",
    "# out = MaxPooling1D(2,padding='same', name ='pooling')(out)\n",
    "\n",
    "\n",
    "\n",
    "# fl_out_1 = Flatten()(dout_2)\n",
    "\n",
    "fl_out_cnn = Flatten()(out)\n",
    "\n",
    "\n",
    "dens_out_3 = Dense( 128, activation='relu' )(fl_out_cnn)\n",
    "\n",
    "# fl2  = Flatten()(out_new)\n",
    "\n",
    "out_put_final = Dense(len(classes), activation='sigmoid', name='Event_output')(dens_out_3)\n",
    "\n",
    "toService_1 = Dense( 130, name=\"to_service1\" )(dens_out_3)\n",
    "toService_1 = Dense( 130, name=\"to_service2\" )(toService_1)\n",
    "\n",
    "service_output = Dense(len(classes  ), activation=\"sigmoid\", name = 'service_output')(toService_1)\n",
    "\n",
    "\n",
    "\n",
    "model2 = Model(inputs=[inputs], outputs=[service_output])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model2.add(Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat'))\n",
    "# model2.add( LSTM(60 ,  recurrent_dropout=0.04, return_sequences=True))\n",
    "# model2.add( LSTM(30 ,  recurrent_dropout=0.14, return_sequences=True))\n",
    "# # model2.add(Bidirectional( LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True),merge_mode='concat'))\n",
    "# model2.add(TimeDistributed(Dense(256, activation='relu')))\n",
    "# model2.add(Dropout(0.1))\n",
    "# model2.add(Flatten())\n",
    "# model2.add(Dense(128, activation='relu'))\n",
    "# model2.add(Dropout(0.2))\n",
    "# model2.add(Dense(len(classes), activation='sigmoid'))\n",
    "\n",
    "# model2.compile(loss=weighted_categorical_crossentropy(weights=weights), optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "losses = {\n",
    "#     \"service_output\": f1_loss_perClass ,\n",
    "    \"service_output\": f1_loss_perRow ,\n",
    "    \"service_output\": \"categorical_crossentropy\",\n",
    "}\n",
    "lossWeights = {#\"service_output\": 20,\n",
    "               \"service_output\": 30.0 ,\n",
    "    \"service_output\": 20}\n",
    " \n",
    "\n",
    "\n",
    "model2.compile(loss=losses,loss_weights=lossWeights, optimizer='adam', metrics=[f1_perRow,f1_perClass,'acc'])\n",
    "# model2.compile(loss=losses, loss_weights=lossWeights, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('IoTDownNet', monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "print(model2.summary())\n",
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=100, batch_size=12500, shuffle=True, callbacks=callbacks_list)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bidirectional LSTM :D \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Flatten\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import Input, merge, concatenate, Conv2D, MaxPooling2D, Activation, UpSampling2D, Dropout, Conv2DTranspose, UpSampling2D, Lambda\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization as bn\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.merge import add\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers.merge import add\n",
    "from keras.layers import Dense,Conv1D,Dropout,Activation,BatchNormalization,MaxPooling1D,Flatten,Masking,TimeDistributed\n",
    "from keras.layers.recurrent import LSTM,GRU,SimpleRNN\n",
    "from keras.models import Input,Sequential,Model\n",
    "from keras.layers.merge import add\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import MSE,MSLE\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def f1_perRow(y_true, y_pred):\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "\n",
    "def f1_perClass(y_true, y_pred):\n",
    "\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss_perClass(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1)) \n",
    "\n",
    "def f1_loss_perRow(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))\n",
    "\n",
    "\n",
    "\n",
    "inputs  = Input( ( dim_size,1 ) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# bi_d_1 = Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat') (inputs)\n",
    "lstm_1 =  LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True)(inputs)\n",
    "# lstm_1 = LSTM(40 ,  recurrent_dropout=0.14, return_sequences=True)(lstm_1)\n",
    "\n",
    "bi_d_raw_1 =Dense(128, activation='relu')  (lstm_1)\n",
    "lstm_raw_1 =  Dense(128, activation='relu')(bi_d_raw_1)\n",
    "lstm_raw_2 = Dense(128, activation='relu')(lstm_raw_1)\n",
    "\n",
    "dout_1  = Dropout(0.1)(lstm_raw_2)\n",
    "flt_1   = Flatten()(dout_1)\n",
    "dense_1 = Dense(128, activation='relu')(flt_1)\n",
    "dout_3  = Dropout(0.2)(dense_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(inputs)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# out = Flatten()(out)\n",
    "# out = MaxPooling1D(2,padding='same', name ='pooling')(out)\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# out = Flatten()(out)\n",
    "# out = MaxPooling1D(2,padding='same', name ='pooling2')(out)\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# out = Flatten()(out)\n",
    "# out = MaxPooling1D(2,padding='same', name ='pooling')(out)\n",
    "\n",
    "\n",
    "\n",
    "# fl_out_1 = Flatten()(dout_2)\n",
    "\n",
    "fl_out_cnn = Flatten()(out)\n",
    "\n",
    "# out_new = concatenate( [fl_out_1, fl_out_cnn] , name='mergerguy')\n",
    "out_new = concatenate( [ fl_out_cnn,dout_3] , name='mergerguy')\n",
    "\n",
    "dens_out_1 = Dense( 128, activation='relu' )(out_new)\n",
    "dens_out_2 = Dense( 128, activation='relu' )(dens_out_1)\n",
    "dens_out_3 = Dense( 128, activation='relu' )(dens_out_2)\n",
    "\n",
    "\n",
    "\n",
    "service_output = Dense(len(classes  ), activation=\"sigmoid\", name = 'service_output')(dens_out_3)\n",
    "\n",
    "\n",
    "\n",
    "model2 = Model(inputs=[inputs], outputs=[service_output])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model2.add(Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat'))\n",
    "# model2.add( LSTM(60 ,  recurrent_dropout=0.04, return_sequences=True))\n",
    "# model2.add( LSTM(30 ,  recurrent_dropout=0.14, return_sequences=True))\n",
    "# # model2.add(Bidirectional( LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True),merge_mode='concat'))\n",
    "# model2.add(TimeDistributed(Dense(256, activation='relu')))\n",
    "# model2.add(Dropout(0.1))\n",
    "# model2.add(Flatten())\n",
    "# model2.add(Dense(128, activation='relu'))\n",
    "# model2.add(Dropout(0.2))\n",
    "# model2.add(Dense(len(classes), activation='sigmoid'))\n",
    "\n",
    "# model2.compile(loss=weighted_categorical_crossentropy(weights=weights), optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "losses = {\n",
    "#     \"service_output\": f1_loss_perClass ,\n",
    "    \"service_output\": f1_loss_perRow ,\n",
    "    \"service_output\": \"categorical_crossentropy\",\n",
    "}\n",
    "lossWeights = {#\"service_output\": 20,\n",
    "               \"service_output\": 30.0 ,\n",
    "    \"service_output\": 20}\n",
    " \n",
    "\n",
    "\n",
    "model2.compile(loss=losses,loss_weights=lossWeights, optimizer='adam', metrics=[f1_perRow,f1_perClass,'acc'])\n",
    "# model2.compile(loss=losses, loss_weights=lossWeights, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('IoTDownNet', monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "print(model2.summary())\n",
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=100, batch_size=12500, shuffle=True, callbacks=callbacks_list)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {\n",
    "#     \"service_output\": f1_loss_perClass ,\n",
    "    \"service_output\": f1_loss_perClass ,\n",
    "#     \"service_output\": \"categorical_crossentropy\",\n",
    "}\n",
    "lossWeights = {#\"service_output\": 20,\n",
    "               \"service_output\": 30.0 ,\n",
    "#     \"service_output\": 20\n",
    "}\n",
    " \n",
    "\n",
    "\n",
    "model2.compile(loss=losses, loss_weights=lossWeights, optimizer=keras.optimizers.Adam(lr=1e-6  ), metrics=[f1_perRow,f1_perClass,'acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bidirectional LSTM :D \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Flatten\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import Input, merge, concatenate, Conv2D, MaxPooling2D, Activation, UpSampling2D, Dropout, Conv2DTranspose, UpSampling2D, Lambda\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization as bn\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.merge import add\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers.merge import add\n",
    "from keras.layers import Dense,Conv1D,Dropout,Activation,BatchNormalization,MaxPooling1D,Flatten,Masking,TimeDistributed\n",
    "from keras.layers.recurrent import LSTM,GRU,SimpleRNN\n",
    "from keras.models import Input,Sequential,Model\n",
    "from keras.layers.merge import add\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import MSE,MSLE\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def f1_perRow(y_true, y_pred):\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "\n",
    "def f1_perClass(y_true, y_pred):\n",
    "\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss_perClass(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "def f1_loss_perRow(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "\n",
    "\n",
    "inputs  = Input( ( dim_size,1 ) )\n",
    "\n",
    "\n",
    "\n",
    "# bi_d_1 = Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat') (inputs)\n",
    "lstm_1 =  LSTM(32 ,  recurrent_dropout=0.3, return_sequences=True)(inputs)\n",
    "lstm_2 = LSTM(32 ,  recurrent_dropout=0.3, return_sequences=True)(lstm_1)\n",
    "\n",
    "lstm_2=Flatten()(lstm_2)\n",
    "lstm_2 = Dense(256, activation='relu')(lstm_2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "service_output = Dense(len(classes  ), activation=\"sigmoid\", name = 'service_output')(lstm_2)\n",
    "\n",
    "\n",
    "\n",
    "model2 = Model(inputs=[inputs], outputs=[service_output])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model2.add(Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat'))\n",
    "# model2.add( LSTM(60 ,  recurrent_dropout=0.04, return_sequences=True))\n",
    "# model2.add( LSTM(30 ,  recurrent_dropout=0.14, return_sequences=True))\n",
    "# # model2.add(Bidirectional( LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True),merge_mode='concat'))\n",
    "# model2.add(TimeDistributed(Dense(256, activation='relu')))\n",
    "# model2.add(Dropout(0.1))\n",
    "# model2.add(Flatten())\n",
    "# model2.add(Dense(128, activation='relu'))\n",
    "# model2.add(Dropout(0.2))\n",
    "# model2.add(Dense(len(classes), activation='sigmoid'))\n",
    "\n",
    "# model2.compile(loss=weighted_categorical_crossentropy(weights=weights), optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "losses = {\n",
    "#     \"service_output\": f1_loss_perClass ,\n",
    "    \"service_output\": f1_loss_perRow ,\n",
    "    \"service_output\": \"categorical_crossentropy\",\n",
    "}\n",
    "lossWeights = {#\"service_output\": 20,\n",
    "               \"service_output\": 30.0 ,\n",
    "    \"service_output\": 20}\n",
    " \n",
    "\n",
    "\n",
    "model2.compile(loss=losses,loss_weights=lossWeights, optimizer='adam', metrics=[f1_perRow,f1_perClass,'acc'])\n",
    "# model2.compile(loss=losses, loss_weights=lossWeights, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('IoTDownNet', monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "print(model2.summary())\n",
    "\n",
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=100, batch_size=12500, shuffle=True, callbacks=callbacks_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split model\n",
    "## Bidirectional LSTM :D \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Flatten\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import Input, merge, concatenate, Conv2D, MaxPooling2D, Activation, UpSampling2D, Dropout, Conv2DTranspose, UpSampling2D, Lambda\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization as bn\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.merge import add\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers.merge import add\n",
    "from keras.layers import Dense,Conv1D,Dropout,Activation,BatchNormalization,MaxPooling1D,Flatten,Masking,TimeDistributed\n",
    "from keras.layers.recurrent import LSTM,GRU,SimpleRNN\n",
    "from keras.models import Input,Sequential,Model\n",
    "from keras.layers.merge import add\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import MSE,MSLE\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def f1_perRow(y_true, y_pred):\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "\n",
    "def f1_perClass(y_true, y_pred):\n",
    "\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss_perClass(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "def f1_loss_perRow(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "#splitting data\n",
    "X=x_lstm_prossed_train2\n",
    "y=y_lstm_prossed_train\n",
    "a,b,c,d,e,f,g,h,ii,jj,k,l,m,n,o,p=[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]\n",
    "print(len(y_lstm_prossed_train[0]))\n",
    "for i in range(0,len(y_lstm_prossed_train)):\n",
    "    a.append(y_lstm_prossed_train[i][0])\n",
    "    b.append(y_lstm_prossed_train[i][1])\n",
    "    c.append(y_lstm_prossed_train[i][2])\n",
    "    d.append(y_lstm_prossed_train[i][3])\n",
    "    e.append(y_lstm_prossed_train[i][4])\n",
    "    f.append(y_lstm_prossed_train[i][5])\n",
    "    g.append(y_lstm_prossed_train[i][6])\n",
    "    h.append(y_lstm_prossed_train[i][7])\n",
    "    ii.append(y_lstm_prossed_train[i][8])\n",
    "    jj.append(y_lstm_prossed_train[i][9])\n",
    "    k.append(y_lstm_prossed_train[i][10])\n",
    "    l.append(y_lstm_prossed_train[i][11])\n",
    "    m.append(y_lstm_prossed_train[i][12])\n",
    "    n.append(y_lstm_prossed_train[i][13])\n",
    "    o.append(y_lstm_prossed_train[i][14])\n",
    "    p.append(y_lstm_prossed_train[i][15])\n",
    "    \n",
    "zzzz=[]    \n",
    "zzzz.append(np.array(a))\n",
    "zzzz.append(np.array(b))\n",
    "zzzz.append(np.array(c))\n",
    "zzzz.append(np.array(d))\n",
    "zzzz.append(np.array(e))\n",
    "zzzz.append(np.array(f))\n",
    "zzzz.append(np.array(g))\n",
    "zzzz.append(np.array(h))\n",
    "zzzz.append(np.array(ii))\n",
    "zzzz.append(np.array(jj))\n",
    "zzzz.append(np.array(k))\n",
    "zzzz.append(np.array(l))\n",
    "zzzz.append(np.array(m))\n",
    "zzzz.append(np.array(n))\n",
    "zzzz.append(np.array(o))\n",
    "zzzz.append(np.array(p))\n",
    "\n",
    "\n",
    "for i in range(0,16):\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "    inputs  = Input( ( dim_size,1 ) )\n",
    "\n",
    "\n",
    "    out = Conv1D(128,3,padding='same')(inputs)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = Activation('relu')(out)\n",
    "    out = Dropout(0.2)(out)\n",
    "    out = Conv1D(128,3,padding='same')(out)\n",
    "    # bi_d_1 = Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat') (inputs)\n",
    "    lstm_1 =  LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True)(out)\n",
    "    # lstm_2 = LSTM(30 ,  recurrent_dropout=0.14, return_sequences=True)(lstm_1)\n",
    "\n",
    "    bi_d_1 =Dense(128, activation='relu')  (lstm_1)\n",
    "    lstm_1 =  Dense(128, activation='relu')(bi_d_1)\n",
    "    lstm_2 = Dense(128, activation='relu')(lstm_1)\n",
    "\n",
    "\n",
    "\n",
    "    # td_1    = TimeDistributed(Dense(256, activation='relu'))(lstm_2)\n",
    "    # dout_1  = Dropout(0.1)(td_1)\n",
    "    dout_1  = Dropout(0.1)(lstm_2)\n",
    "    flt_1   = Flatten()(dout_1)\n",
    "    dense_1 = Dense(128, activation='relu')(flt_1)\n",
    "    dout_2  = Dropout(0.2)(dense_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # bi_d_1 = Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat') (inputs)\n",
    "    lstm_1 =  LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True)(inputs)\n",
    "    # lstm_1 = LSTM(40 ,  recurrent_dropout=0.14, return_sequences=True)(lstm_1)\n",
    "\n",
    "    bi_d_raw_1 =Dense(128, activation='relu')  (lstm_1)\n",
    "    lstm_raw_1 =  Dense(128, activation='relu')(bi_d_raw_1)\n",
    "    lstm_raw_2 = Dense(128, activation='relu')(lstm_raw_1)\n",
    "\n",
    "    dout_1  = Dropout(0.1)(lstm_raw_2)\n",
    "    flt_1   = Flatten()(dout_1)\n",
    "    dense_1 = Dense(128, activation='relu')(flt_1)\n",
    "    dout_3  = Dropout(0.2)(dense_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    out = Conv1D(128,3,padding='same')(inputs)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = Activation('relu')(out)\n",
    "    out = Dropout(0.2)(out)\n",
    "    out = Conv1D(128,3,padding='same')(out)\n",
    "    # out = Flatten()(out)\n",
    "    # out = MaxPooling1D(2,padding='same', name ='pooling')(out)\n",
    "\n",
    "\n",
    "    out = Conv1D(128,3,padding='same')(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = Activation('relu')(out)\n",
    "    out = Dropout(0.2)(out)\n",
    "    out = Conv1D(128,3,padding='same')(out)\n",
    "    # out = Flatten()(out)\n",
    "    # out = MaxPooling1D(2,padding='same', name ='pooling2')(out)\n",
    "\n",
    "\n",
    "    out = Conv1D(128,3,padding='same')(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = Activation('relu')(out)\n",
    "    out = Dropout(0.2)(out)\n",
    "    out = Conv1D(128,3,padding='same')(out)\n",
    "    # out = Flatten()(out)\n",
    "    # out = MaxPooling1D(2,padding='same', name ='pooling')(out)\n",
    "\n",
    "\n",
    "\n",
    "    # fl_out_1 = Flatten()(dout_2)\n",
    "\n",
    "    fl_out_cnn = Flatten()(out)\n",
    "\n",
    "    # out_new = concatenate( [fl_out_1, fl_out_cnn] , name='mergerguy')\n",
    "    out_new = concatenate( [dout_2, fl_out_cnn,dout_3] , name='mergerguy')\n",
    "\n",
    "    dens_out_1 = Dense( 128, activation='relu' )(out_new)\n",
    "    dens_out_2 = Dense( 128, activation='relu' )(dens_out_1)\n",
    "    dens_out_3 = Dense( 128, activation='relu' )(dens_out_2)\n",
    "\n",
    "    # fl2  = Flatten()(out_new)\n",
    "\n",
    "    out_put_final = Dense(len(classes), activation='sigmoid', name='Event_output')(dens_out_3)\n",
    "\n",
    "    toService_1 = Dense( 130, name=\"to_service1\" )(dens_out_3)\n",
    "    toService_1 = Dense( 130, name=\"to_service2\" )(toService_1)\n",
    "\n",
    "    service_output = Dense(1, activation=\"sigmoid\", name = 'service_output')(toService_1)\n",
    "\n",
    "\n",
    "    losses = {\n",
    "    #     \"service_output\": f1_loss_perClass ,\n",
    "        \"service_output\": f1_loss_perRow ,\n",
    "        \"service_output\": \"binary_crossentropy\",\n",
    "    }\n",
    "    lossWeights = {#\"service_output\": 20,\n",
    "                   \"service_output\": 30.0 ,\n",
    "        \"service_output\": 20}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    model2 = Model(inputs=[inputs], outputs=[service_output])\n",
    "    model2.compile(loss=losses,loss_weights=lossWeights, optimizer='adam', metrics=[f1_perRow,f1_perClass,'acc'])\n",
    "    # model2.compile(loss=losses, loss_weights=lossWeights, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    checkpoint = ModelCheckpoint('IoTDownNet', monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "    hist2 = model2.fit(x_lstm_prossed_train2, zzzz[i], epochs=200, batch_size=12500, shuffle=True, callbacks=callbacks_list)\n",
    "    model2.save('number'+str(i)+'.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2.compile(loss=losses,loss_weights=lossWeights, optimizer=keras.optimizers.Adam(lr=5e-5  ), metrics=[f1_perRow,f1_perClass,'acc'])\n",
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=300, batch_size=16500, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss=f1_loss, optimizer=keras.optimizers.Adam(lr=8e-5  ), metrics=[f1,'acc'])\n",
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=300, batch_size=7500, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss=weighted_categorical_crossentropy(weights=weights), optimizer=keras.optimizers.Adam(lr=8e-5  ), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(lr=8e-5  ), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss=weighted_categorical_crossentropy(weights=weights), optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=300, batch_size=3500, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save( \"LSTM-sigmoid-withRemovedClasses\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist2.history['loss'], c='red')\n",
    "plt.plot(hist2.history['acc'], c='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist2.history['loss'], c='red')\n",
    "# plt.plot(hist2.history['acc'], c='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model2, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "# model2=load_model( \"LSTM_withSigmoid_LargeData_F1_E100_B500_MSE_False\"  \n",
    "#            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(lstm_tests)) :\n",
    "    print( \"==================HOME Case : %s =============\" % test_names[i])\n",
    "    makeReadable( classes=classes, confidance=0.5,data=lstm_tests[i][0],gt=lstm_tests[i][1],model=model2,path=test_names[i],x=lstm_tests[i][0])\n",
    "    \n",
    "#     lstm_pred= model2.predict( lstm_tests[i][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ------------- do not go any further :) ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred= model2.predict( lstm_tests[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred= model2.predict( lstm_tests[1][0])\n",
    "lstm_pred__ = np.array(list(lstm_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred__ = np.array(list(lstm_pred))\n",
    "print_info( lstm_tests[1][1], lstm_pred__, classes , confidance=0.43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in [1] :\n",
    "for i in range(len(lstm_tests)) :\n",
    "    print( \"==================HOME Case : %s =============\" % test_names[i])\n",
    "    lstm_pred= model2.predict( lstm_tests[i][0])\n",
    "    \n",
    "#     print_info( lstm_tests[i][1], lstm_pred, classes , confidance=0.7)\n",
    "    print_info( lstm_tests[i][1], lstm_pred, classes , confidance=0.5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in [1] :\n",
    "# for i in range(len(lstm_tests)) :\n",
    "    print( \"==================HOME Case : %s =============\" % test_names[i])\n",
    "    lstm_pred= model2.predict( lstm_tests[i][0])\n",
    "    \n",
    "#     print_info( lstm_tests[i][1], lstm_pred, classes , confidance=0.7)\n",
    "    print_info( lstm_tests[i][1], lstm_pred, classes , confidance=0.992)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred =model2.predict( x_lstm_prossed_test2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_info(y_lstm_prossed_test, lstm_pred, classes, confidance=0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred =model2.predict( x_lstm_prossed_test)\n",
    "print_info(y_lstm_prossed_test, lstm_pred, classes, confidance=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred = model2.predict( x_lstm_prossed_test)\n",
    "print_info(y_lstm_prossed_test, lstm_pred, classes, confidance=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_info(y_lstm_prossed_train, y_lstm_prossed_train, classes, confidance=0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x  in lstm_pred  if  np.sum(x) > 0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save( \"LSTM_withSigmoid_LargeData_F%s_E%d_B%d_M%s_%r\" %\n",
    "            (\n",
    "            FoldID,\n",
    "                Epoch_count,\n",
    "                Batch_size,\n",
    "                Mapper,\n",
    "                IgnoreEmpty\n",
    "            ) \n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred = model2.predict( x_lstm_prossed_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_info(y_lstm_prossed_test, lstm_pred, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_muhammed,y_train_muhammed, classes = pre_process_raw( x_train, y_train , dim_size, zero_pad=True, normalize=False)\n",
    "# x_test_muhammed,y_test_muhammed, classes = pre_process_raw( x_test, y_test , dim_size, zero_pad=True, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred = model2.predict( x_lstm_prossed_test )\n",
    "print_info(y_lstm_prossed_test, lstm_pred, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred = model2.predict( x_lstm_prossed_test )\n",
    "print_info(y_lstm_prossed_test, lstm_pred, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_info(y_lstm_prossed_test, pred, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len( y_lstm_prossed_train[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_size =160\n",
    "x_lstm_prossed_train,y_lstm_prossed_train, _ = pre_process_raw( x_train, y_train , dim_size, zero_pad=True, normalize=False,classes=classes)\n",
    "x_lstm_prossed_test,y_lstm_prossed_test, _ = pre_process_raw( x_test, y_test , dim_size, zero_pad=True, normalize=False,classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([x  for x  in y_lstm_prossed_test if x[21]==1 or x[20]==1]), len(y_lstm_prossed_test  ) , len([x  for x  in y_lstm_prossed_test if x[21]==1 or x[20]==1])/len(y_lstm_prossed_test  ) *1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ x for x  in  pred if np.sum(x) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_lstm_prossed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_for_raun( pred   ):\n",
    "    pp = pred\n",
    "    pp[pp>=0.5] = 1\n",
    "    pp[pp<0.5] = 0\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([x for x in pred if np.sum( do_for_raun(x) )==0 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([x for x in pred if  do_for_raun(x)[20] ==1 or do_for_raun(x)[21] ==1 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# np.save(\"../files/muhammed/x_train.json\" , x_train_muhammed)\n",
    "# np.save(\"../files/muhammed/y_train.json\", )\n",
    "\n",
    "\n",
    "# np.save( \"../files/muhammed/x_train.json\", x_train_muhammed )\n",
    "# np.save(\"../files/muhammed/y_train.json\",  y_train_muhammed )\n",
    "# np.save( \"../files/muhammed/x_test.json\",x_test_muhammed )\n",
    "# np.save( \"../files/muhammed/y_test.json\",y_test_muhammed )\n",
    "# np.save( \"../files/muhammed/classes.json\",  classes )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_lstm_prossed_test) + len(x_lstm_prossed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6(iot)",
   "language": "python",
   "name": "iot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
