{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%run \"./utils.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadFromMerged=True\n",
    "loadFromIndexes= False\n",
    "Mapper='S'\n",
    "IgnoreEmpty= True\n",
    "FoldID =\"1\"\n",
    "Epoch_count=100\n",
    "Batch_size=5000\n",
    "INCLUDE_DIRECTION = True\n",
    "\n",
    "LOAD_OLD_DATA_TRAIN = False\n",
    "LOAD_OLD_DATA_TEST = False\n",
    "MERGE_TESTS=False\n",
    "\n",
    "TEST_CLASS_CAP =150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in load from merged\n",
      "2522 2335\n",
      "0 0\n",
      "7 6\n",
      "0 0\n",
      "9 9\n",
      "3161 3047\n",
      "2703 2229\n",
      "17 17\n",
      "29 17\n",
      "2960 2778\n",
      "1775 1697\n",
      "15 12\n",
      "191 143\n",
      "0 0\n",
      "10 9\n",
      "7114 6606\n",
      "983 652\n",
      "918 890\n",
      "1703 1540\n",
      "7 7\n",
      "2199 2179\n",
      "loading from test files\n",
      "found files :  5\n",
      "home_os_final.json\n",
      "18308 18308\n",
      "home_sk_final.json\n",
      "5837 5837\n",
      "test_data_light.json\n",
      "233 233\n",
      "test_data_lock.json\n",
      "807 807\n",
      "test_data_motion_2.json\n",
      "8133 8133\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(24173, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= []\n",
    "y= []\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "y_test_service= []\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "y_train_service = []\n",
    "\n",
    "test_names = []\n",
    "\n",
    "add_to_trainig = [0,2]\n",
    "\n",
    "if loadFromMerged:\n",
    "    print(\"in load from merged\")\n",
    "    if LOAD_OLD_DATA_TRAIN:\n",
    "        with open(  '../files/train/test/test_homes/final_upload/Merged_final_with_home/merged_hub_segments_final.json'  ) as f:\n",
    "            y_data = json.load(f)\n",
    "\n",
    "        with open(  '../files/train/test/test_homes/final_upload/Merged_final_with_home/merged_pcap_segments_final.json'  ) as f:\n",
    "            x_data = json.load(f)\n",
    "\n",
    "    #     with open(  '../files/train/merged/hub_segments_2.json'  ) as f:\n",
    "    #         y_data = json.load(f)\n",
    "\n",
    "    #     with open(  '../files/train/merged/pcap_segments_2.json'  ) as f:\n",
    "    #         x_data = json.load(f)\n",
    "\n",
    "        if len( y_data ) != len(x_data) :\n",
    "            print( pick )\n",
    "\n",
    "\n",
    "        x_train,y_train, y_train_service= clean_data( x_data, y_data, IgnoreEmpty , \n",
    "                                                     Mapping=Mapper, include_direction=INCLUDE_DIRECTION )\n",
    "    else:\n",
    "        \n",
    "        for pick in sorted(glob.glob( '../files/iot_data_2020/train/hub_segments/*.json' )):\n",
    "            fname  = os.path.basename(pick)\n",
    "#             test_names.append( fname )\n",
    "            with open( os.path.join( '../files/iot_data_2020/train/hub_segments/', fname) ) as f:\n",
    "                y_data = json.load(f)\n",
    "\n",
    "            with open( os.path.join('../files/iot_data_2020/train/pcap_segments/', fname) ) as f:\n",
    "                x_data = json.load(f)\n",
    "\n",
    "            if len( y_data ) != len(x_data) :\n",
    "                print( pick )\n",
    "                continue\n",
    "\n",
    "            x_t,y_t, y_t_s= clean_data( x_data, y_data, IgnoreEmpty , \n",
    "                                                     Mapping=Mapper, include_direction=INCLUDE_DIRECTION )\n",
    "    \n",
    "            x_train.extend(x_t)\n",
    "            y_train.extend(y_t)\n",
    "            y_train_service.extend(y_t_s)\n",
    "            \n",
    "\n",
    "    #     continue\n",
    "#     if loadFromIndexes:\n",
    "#         print(\"load from indexes\")\n",
    "#         with open(\"../files/train/merged/items_2_test-train_indexes.json\")  as f:\n",
    "#             index_info = json.load(f)\n",
    "\n",
    "\n",
    "#         for i in index_info[FoldID][\"test\"]:\n",
    "#             x_test[str(i)]=(x_data[str(i)] )\n",
    "#             y_test[str(i)]=(y_data[str(i)] )\n",
    "\n",
    "#         for i in index_info[FoldID][\"train\"]:\n",
    "#             x_train[str(i)]=(  x_data[str(i)] )\n",
    "#             y_train[str(i)]=(  y_data[str(i)] )\n",
    "        \n",
    "#         x_test_t,y_test_t= clean_data( x_test, y_test, IgnoreEmpty , Mapping=Mapper)\n",
    "#         x_test.append(x_test_t)\n",
    "#         y_test.append(y_test_t)\n",
    "    #     else :\n",
    "    print(\"loading from test files\")\n",
    "    files_path =  '../files/train/test/test_homes/final_upload/usecases/pcap_segments_final_final/home*.json' if LOAD_OLD_DATA_TEST else '../files/iot_data_2020/usecases/pcap_segments_final/*.json'\n",
    "    test_y_dir = '../files/train/test/test_homes/final_upload/usecases/hub_segments_final_final/' if LOAD_OLD_DATA_TEST else '../files/iot_data_2020/usecases/hub_segments_final/'\n",
    "    test_x_dir = '../files/train/test/test_homes/final_upload/usecases/pcap_segments_final_final/' if LOAD_OLD_DATA_TEST else '../files/iot_data_2020/usecases/pcap_segments_final/'\n",
    "    \n",
    "    test_files = sorted(glob.glob(files_path))\n",
    "    print( \"found files : \" , len(test_files) )\n",
    "    \n",
    "    \n",
    "    for pick  in test_files:\n",
    "        fname  = os.path.basename(pick)\n",
    "        test_names.append( fname )\n",
    "        print(fname)\n",
    "        with open( os.path.join(test_y_dir , fname) ) as f:\n",
    "            y_data_test = json.load(f)\n",
    "\n",
    "        with open( os.path.join(test_x_dir, fname) ) as f:\n",
    "            x_data_test = json.load(f)\n",
    "\n",
    "\n",
    "        t_x,t_y, t_z= clean_data( x_data_test, y_data_test, False , Mapping=Mapper, include_direction= INCLUDE_DIRECTION )\n",
    "\n",
    "#         if test_files.index(pick) in add_to_trainig:\n",
    "#             x_test_t,y_test_t, y_test_service_t= clean_data( x_data_test, y_data_test, IgnoreEmpty , Mapping=Mapper)\n",
    "#             x_train.extend(x_test_t)\n",
    "#             y_train.extend(y_test_t)\n",
    "#             y_train_service.extend(y_test_service_t)\n",
    "\n",
    "                \n",
    "        if MERGE_TESTS:\n",
    "            x_test.extend(t_x)\n",
    "            y_test.extend(t_y)\n",
    "            y_test_service.extend(t_z)\n",
    "        else:\n",
    "            x_test.append(t_x)\n",
    "            y_test.append(t_y)\n",
    "            y_test_service.append(t_z)\n",
    "if MERGE_TESTS:\n",
    "    x_test = [x_test]\n",
    "    y_test = [y_test]\n",
    "    y_test_service = [y_test_service]\n",
    "    test_names =['MERGED']\n",
    "#     x_test = x_data[ index_info[\"1\"][\"test\"]  ]\n",
    "#     y_test = y_data[ index_info[\"1\"][\"test\"]  ]\n",
    "    \n",
    "#     x_train = x_data[ index_info[\"1\"][\"train\"]  ]\n",
    "#     y_train = y_data[ index_info[\"1\"][\"train\"]  ]\n",
    "#     x.extend(t_x)\n",
    "#     y.extend(t_y)\n",
    "# else:\n",
    "#     for pick in sorted(glob.glob( '../files/train/hub_segments/*.json' )):\n",
    "#         fname  = os.path.basename(pick)\n",
    "#         test_names.append( fname )\n",
    "#         with open( os.path.join( '../files/train/hub_segments/', fname) ) as f:\n",
    "#             y_data = json.load(f)\n",
    "\n",
    "#         with open( os.path.join('../files/train/pcap_segments/', fname) ) as f:\n",
    "#             x_data = json.load(f)\n",
    "\n",
    "#         if len( y_data ) != len(x_data) :\n",
    "#             print( pick )\n",
    "#             continue\n",
    "\n",
    "#         t_x,t_y= clean_data( x_data, y_data, True, include_direction=INCLUDE_DIRECTION )\n",
    "\n",
    "#         x.extend( t_x)\n",
    "#         y.extend(t_y)\n",
    "\n",
    "x= np.array(x)\n",
    "y= np.array(y)\n",
    "\n",
    "# x_train = np.append( x_train, x_test[0] , axis=0)\n",
    "# x_train = np.append( x_train, x_test[2] , axis=0)\n",
    "\n",
    "# y_train = np.append( y_train, y_test[0] , axis=0)\n",
    "# y_train = np.append( y_train, y_test[2] , axis=0)\n",
    "\n",
    "\n",
    "len(x_train), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sets the classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'acceleration'),\n",
       " (1, 'activity'),\n",
       " (2, 'battery'),\n",
       " (3, 'button'),\n",
       " (4, 'colorTemperature'),\n",
       " (5, 'contact'),\n",
       " (6, 'level'),\n",
       " (7, 'lock'),\n",
       " (8, 'motion'),\n",
       " (9, 'no_logs'),\n",
       " (10, 'ping'),\n",
       " (11, 'status'),\n",
       " (12, 'switch'),\n",
       " (13, 'temperature'),\n",
       " (14, 'threeAxis'),\n",
       " (15, 'unknown'),\n",
       " (16, 'water')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = sorted(list(np.unique(  np.concatenate( y_train  ))))\n",
    "# print([ (i , classes[i]) for i in range( len(classes) ) ])\n",
    "\n",
    "service_classes = sorted(list(np.unique(  np.concatenate( y_train_service  ))))\n",
    "[ (i , service_classes[i]) for i in range( len(service_classes) ) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acceleration',\n",
       " 'activity',\n",
       " 'battery',\n",
       " 'button',\n",
       " 'colorTemperature',\n",
       " 'contact',\n",
       " 'level',\n",
       " 'lock',\n",
       " 'motion',\n",
       " 'no_logs',\n",
       " 'ping',\n",
       " 'status',\n",
       " 'switch',\n",
       " 'temperature',\n",
       " 'threeAxis',\n",
       " 'unknown',\n",
       " 'water']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter the records by service/event types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "choose services to keep : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Mapper=='SE':\n",
    "    services_to_keep = [\n",
    "        \"colorTemperature-XXX\",\n",
    "\"contact-closed\",\n",
    "\"contact-open\",\n",
    "\"level-XXX\",\n",
    "\"lock-locked\",\n",
    "\"lock-unlocked\",\n",
    "\"motion-active\",\n",
    "\"motion-inactive\",\n",
    "\"ping-ping\",\n",
    "\"status-closed\",\n",
    "\"status-open\",\n",
    "\"switch-off\",\n",
    "\"switch-on\",\n",
    "\"temperature-XXX\"\n",
    "    ] \n",
    "else:\n",
    "     services_to_keep =[\"button\",\n",
    "\"colorTemperature\",\n",
    "\"contact\",\n",
    "\"level\",\n",
    "\"lock\",\n",
    "\"motion\",\n",
    "\"ping\",\n",
    "\"status\",\n",
    "\"switch\",\n",
    "\"temperature\"]\n",
    "\n",
    "# keep all ? \n",
    "# services_to_keep= classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find records which need change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indexes_to_change =  [ i for i in range(len(y_train)) if\n",
    "              is_clean( y_train[i],to_keep=services_to_keep, return_clean=False) ] \n",
    "\n",
    "y_train = [ ['unknown'] if i in train_indexes_to_change else y_train[i] \n",
    "           for i in range(len(y_train)) ] \n",
    "\n",
    "for t_index in range(len(y_test)):\n",
    "    test_indexes_to_change =  [ i for i in range(len(y_test[t_index])) if\n",
    "                  is_clean( y_test[t_index][i],to_keep=services_to_keep, return_clean=False) ] \n",
    "\n",
    "    y_test[t_index] = [ ['unknown'] if i in test_indexes_to_change else y_test[t_index][i] \n",
    "               for i in range(len(y_test[t_index])) ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_known_unknown(inp, unknown_ind):\n",
    "    return [ [1,0] if (x[unknown_ind] == 1 ) else [0,1]   for x in inp ]\n",
    "\n",
    "known_unknown_y_train = [ [1,0] if (len(x) == 1 and (\"unknown\" in x or 'unknown-' in x)) else [0,1]   for x in y_train ]\n",
    "\n",
    "known_unknown_y_test= [] \n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    \n",
    "    known_unknown_y_test.append( [ [1,0] if (len(x) == 1 and \"unknown\" in x) else [0,1]   for x in y_test[i] ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove classes that are ignored from the services list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['button',\n",
       " 'colorTemperature',\n",
       " 'contact',\n",
       " 'level',\n",
       " 'lock',\n",
       " 'motion',\n",
       " 'ping',\n",
       " 'status',\n",
       " 'switch',\n",
       " 'temperature',\n",
       " 'unknown']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = services_to_keep\n",
    "classes.append('unknown')\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,_=pre_process_raw( x_train, y_train , 15, zero_pad=True, normalize=True, classes=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the data and preprocess train and test portions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_size= 20\n",
    "x_random_forest_train,y_random_forest_train, _ = pre_process_raw( x_train, y_train , dim_size, zero_pad=False, normalize=False, classes=classes, as_string=True)\n",
    "rf_tests  = [ pre_process_raw( x_test[i], y_test[i] , dim_size, zero_pad=False, normalize=False, classes=classes, as_string=True, class_cap=TEST_CLASS_CAP) for i in range(len(x_test)) ] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vectorize the X vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 5))\n",
    "x_xgb_train = vectorizer.fit_transform(x_random_forest_train)\n",
    "xgb_test = []\n",
    "for x in range(len(rf_tests)):\n",
    "    xgb_test.append( ( vectorizer.transform(rf_tests[x][0]),\n",
    "                    rf_tests[x][1],\n",
    "                    rf_tests[x][2]\n",
    "                  ))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xgb_known_unknown_separator_classifier = OneVsRestClassifier(XGBClassifier())\n",
    "\n",
    "xgb_known_unknown_separator_classifier.fit(x_xgb_train, np.array(known_unknown_y_train))\n",
    "\n",
    "train_known_unknown_pred=xgb_known_unknown_separator_classifier.predict_proba(x_xgb_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================HOME Case : home_os_final.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                       unknown     0.766      0.669     0.988     0.798     11851  7924/ 5231/   94/ 3927\n",
      "                         known     0.766      0.982     0.571     0.722      5325  5231/ 7924/ 3927/   94\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.766      0.825     0.780     0.760     17176     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "             Weighted AVERAGES     0.766      0.766     0.859     0.774     17176     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "       known Weighted AVERAGES     0.766      0.982     0.571     0.722     17176     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "       non zero count AVERAGES     0.766      0.825     0.780     0.760     17176     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "Macro F1 : 0.759994\n",
      "sample F1 : 0.765894\n",
      "weighted F1 : 0.774292\n",
      "Exact Match ACC : 0.76589 \n",
      "Total Records : 17176 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 0 (0.000)%\n",
      "=============================================================================\n",
      "==================HOME Case : home_sk_final.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                       unknown     0.794      0.622     0.969     0.757      2997  1863/ 2732/   60/ 1134\n",
      "                         known     0.794      0.979     0.707     0.821      2792  2732/ 1863/ 1134/   60\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.794      0.800     0.838     0.789      5789     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "             Weighted AVERAGES     0.794      0.794     0.842     0.788      5789     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "       known Weighted AVERAGES     0.794      0.979     0.707     0.821      5789     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "       non zero count AVERAGES     0.794      0.800     0.838     0.789      5789     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "Macro F1 : 0.788992\n",
      "sample F1 : 0.793747\n",
      "weighted F1 : 0.787870\n",
      "Exact Match ACC : 0.79375 \n",
      "Total Records : 5789 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 0 (0.000)%\n",
      "=============================================================================\n",
      "==================HOME Case : test_data_light.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                       unknown     0.725      0.400     0.977     0.568       105    42/  127/    1/   63\n",
      "                         known     0.725      0.992     0.668     0.799       128   127/   42/   63/    1\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.725      0.696     0.823     0.683       233     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "             Weighted AVERAGES     0.725      0.725     0.807     0.695       233     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "       known Weighted AVERAGES     0.725      0.992     0.668     0.799       233     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "       non zero count AVERAGES     0.725      0.696     0.823     0.683       233     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "Macro F1 : 0.683155\n",
      "sample F1 : 0.725322\n",
      "weighted F1 : 0.694565\n",
      "Exact Match ACC : 0.72532 \n",
      "Total Records : 233 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 0 (0.000)%\n",
      "=============================================================================\n",
      "==================HOME Case : test_data_lock.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                       unknown     0.570      0.391     0.973     0.558       560   219/  241/    6/  341\n",
      "                         known     0.570      0.976     0.414     0.581       247   241/  219/  341/    6\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.570      0.683     0.694     0.570       807     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "             Weighted AVERAGES     0.570      0.570     0.802     0.565       807     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "       known Weighted AVERAGES     0.570      0.976     0.414     0.581       807     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "       non zero count AVERAGES     0.570      0.683     0.694     0.570       807     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "Macro F1 : 0.569693\n",
      "sample F1 : 0.570012\n",
      "weighted F1 : 0.565143\n",
      "Exact Match ACC : 0.57001 \n",
      "Total Records : 807 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 0 (0.000)%\n",
      "=============================================================================\n",
      "==================HOME Case : test_data_motion_2.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                       unknown     0.679      0.514     0.943     0.665      4953  2546/ 2877/  153/ 2407\n",
      "                         known     0.679      0.950     0.544     0.692      3030  2877/ 2546/ 2407/  153\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.679      0.732     0.744     0.679      7983     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "             Weighted AVERAGES     0.679      0.679     0.792     0.676      7983     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "       known Weighted AVERAGES     0.679      0.950     0.544     0.692      7983     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "       non zero count AVERAGES     0.679      0.732     0.744     0.679      7983     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "Macro F1 : 0.678766\n",
      "sample F1 : 0.679319\n",
      "weighted F1 : 0.675558\n",
      "Exact Match ACC : 0.67932 \n",
      "Total Records : 7983 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 0 (0.000)%\n",
      "=============================================================================\n"
     ]
    }
   ],
   "source": [
    "test_known_unknown_predicted = []\n",
    "for i in range(len(xgb_test)) :\n",
    "    print( \"==================HOME Case : %s =============\" % test_names[ i] )\n",
    "    xgb_pred=xgb_known_unknown_separator_classifier.predict( xgb_test[i][0])\n",
    "    test_known_unknown_predicted.append(xgb_pred)\n",
    "    print_info( np.array( make_known_unknown(xgb_test[i][1], classes.index('unknown'))), xgb_pred, [\"unknown\",\"known\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17176, 3, (17176, 11))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_known_unknown_predicted[0]), len(xgb_test[0]), (xgb_test[0][1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_indexes_train  = [ i for i in range(len(train_known_unknown_pred)) if train_known_unknown_pred[i][1] ==1 ]\n",
    "\n",
    "x_train_known = x_xgb_train[known_indexes_train]\n",
    "y_train_known = y_random_forest_train[known_indexes_train]\n",
    "\n",
    "xgb_test_known = [] \n",
    "\n",
    "for test_index in range(len(test_known_unknown_predicted)):\n",
    "    known_indexes= [ i for i in range(len(test_known_unknown_predicted[test_index])) if test_known_unknown_predicted[test_index][i][1] ==1 ]\n",
    "    \n",
    "    xgb_test_known.append(  (xgb_test[test_index][0][known_indexes], \n",
    "                            xgb_test[test_index][1][known_indexes],\n",
    "                            xgb_test[test_index][2],\n",
    "                            known_indexes\n",
    "                           ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                            colsample_bylevel=None,\n",
       "                                            colsample_bynode=None,\n",
       "                                            colsample_bytree=None, gamma=None,\n",
       "                                            gpu_id=None, importance_type='gain',\n",
       "                                            interaction_constraints=None,\n",
       "                                            learning_rate=None,\n",
       "                                            max_delta_step=None, max_depth=None,\n",
       "                                            min_child_weight=None, missing=nan,\n",
       "                                            monotone_constraints=None,\n",
       "                                            n_estimators=100, n_jobs=None,\n",
       "                                            num_parallel_tree=None,\n",
       "                                            objective='binary:logistic',\n",
       "                                            random_state=None, reg_alpha=None,\n",
       "                                            reg_lambda=None,\n",
       "                                            scale_pos_weight=None,\n",
       "                                            subsample=None, tree_method=None,\n",
       "                                            validate_parameters=None,\n",
       "                                            verbosity=None),\n",
       "                    n_jobs=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_classifier = OneVsRestClassifier(XGBClassifier())\n",
    "\n",
    "xgb_classifier.fit(x_xgb_train, np.array(y_random_forest_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================HOME Case : home_os_final.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                        button     1.000      0.750     0.692     0.720        12     9/17160/    4/    3\n",
      "              colorTemperature     1.000      0.800     1.000     0.889         5     4/17171/    0/    1\n",
      "                       contact     0.993      0.740     0.561     0.638       150   111/16939/   87/   39\n",
      "                         level     0.985      0.750     0.056     0.103        20    15/16901/  255/    5\n",
      "                          lock     1.000      0.941     0.889     0.914        34    32/17138/    4/    2\n",
      "                        motion     0.980      0.460     0.212     0.291       150    69/16770/  256/   81\n",
      "                          ping     0.986      0.950     0.999     0.974      4813  4570/12359/    4/  243\n",
      "                        status     0.996      0.812     0.704     0.754       117    95/17019/   40/   22\n",
      "                        switch     0.999      0.619     0.812     0.703        21    13/17152/    3/    8\n",
      "                   temperature     0.819      0.747     0.035     0.067       150   112/13955/ 3071/   38\n",
      "                       unknown     0.766      0.669     0.988     0.798     11851  7924/ 5231/   94/ 3927\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.957      0.749     0.632     0.623     17176     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "             Weighted AVERAGES     0.834      0.748     0.969     0.833     17176     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "       known Weighted AVERAGES     0.982      0.919     0.927     0.911     17176     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      " known -ping Weighted AVERAGES     0.952      0.698     0.403     0.453     17176     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "       non zero count AVERAGES     0.957      0.749     0.632     0.623     17176     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "Macro F1 : 0.622752\n",
      "sample F1 : 0.747747\n",
      "weighted F1 : 0.833433\n",
      "Exact Match ACC : 0.74686 \n",
      "Total Records : 17176 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 555 (0.032)%\n",
      "=============================================================================\n",
      "==================HOME Case : home_sk_final.json =============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/omid/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n",
      "/opt/anaconda3/envs/omid/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if __name__ == '__main__':\n",
      "/opt/anaconda3/envs/omid/lib/python3.6/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if sys.path[0] == '':\n",
      "/opt/anaconda3/envs/omid/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1515: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                        button     1.000      1.000     0.961     0.980        49    49/ 5738/    2/    0\n",
      "              colorTemperature     1.000      1.000     0.500     0.667         1     1/ 5787/    1/    0\n",
      "                       contact     0.989      0.733     0.821     0.775       150   110/ 5615/   24/   40\n",
      "                         level     0.996      0.811     0.754     0.782        53    43/ 5722/   14/   10\n",
      "                          lock     1.000        nan       nan       nan         0     0/ 5789/    0/    0\n",
      "                        motion     0.961      0.609     0.316     0.416       133    81/ 5481/  175/   52\n",
      "                          ping     0.984      0.961     0.997     0.979      2301  2212/ 3482/    6/   89\n",
      "                        status     0.997      0.863     0.967     0.912       102    88/ 5684/    3/   14\n",
      "                        switch     0.998      0.444     0.889     0.593        18     8/ 5770/    1/   10\n",
      "                   temperature     0.831      0.787     0.111     0.194       150   118/ 4692/  947/   32\n",
      "                       unknown     0.794      0.622     0.969     0.757      2997  1863/ 2732/   60/ 1134\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.959      0.712     0.662     0.641      5789     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "             Weighted AVERAGES     0.884      0.768     0.938     0.826      5789     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "       known Weighted AVERAGES     0.976      0.916     0.906     0.895      5789     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      " known -ping Weighted AVERAGES     0.950      0.759     0.585     0.601      5789     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "       non zero count AVERAGES     0.955      0.783     0.729     0.705      5789     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "Macro F1 : 0.641330\n",
      "sample F1 : 0.766523\n",
      "weighted F1 : 0.825803\n",
      "Exact Match ACC : 0.76179 \n",
      "Total Records : 5789 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 126 (0.022)%\n",
      "=============================================================================\n",
      "==================HOME Case : test_data_light.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                        button     1.000        nan       nan       nan         0     0/  233/    0/    0\n",
      "              colorTemperature     0.966      0.839     0.897     0.867        31    26/  199/    3/    5\n",
      "                       contact     1.000        nan       nan       nan         0     0/  233/    0/    0\n",
      "                         level     0.863      0.756     0.838     0.795        82    62/  139/   12/   20\n",
      "                          lock     1.000        nan       nan       nan         0     0/  233/    0/    0\n",
      "                        motion     0.974        nan     0.000     0.000         0     0/  227/    6/    0\n",
      "                          ping     1.000      1.000     1.000     1.000        26    26/  207/    0/    0\n",
      "                        status     1.000        nan       nan       nan         0     0/  233/    0/    0\n",
      "                        switch     0.953      0.500     0.909     0.645        20    10/  212/    1/   10\n",
      "                   temperature     0.901      0.500     0.043     0.080         2     1/  209/   22/    1\n",
      "                       unknown     0.725      0.400     0.977     0.568       105    42/  127/    1/   63\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.944      0.363     0.424     0.359       233     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "             Weighted AVERAGES     0.841      0.628     0.915     0.717       233     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "       known Weighted AVERAGES     0.916      0.776     0.874     0.814       233     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      " known -ping Weighted AVERAGES     0.900      0.733     0.850     0.779       233     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "       non zero count AVERAGES     0.901      0.666     0.777     0.659       233     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "Macro F1 : 0.359479\n",
      "sample F1 : 0.612303\n",
      "weighted F1 : 0.716932\n",
      "Exact Match ACC : 0.54936 \n",
      "Total Records : 233 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 47 (0.202)%\n",
      "=============================================================================\n",
      "==================HOME Case : test_data_lock.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                        button     0.998        nan     0.000     0.000         0     0/  805/    2/    0\n",
      "              colorTemperature     1.000        nan       nan       nan         0     0/  807/    0/    0\n",
      "                       contact     1.000        nan       nan       nan         0     0/  807/    0/    0\n",
      "                         level     0.999        nan     0.000     0.000         0     0/  806/    1/    0\n",
      "                          lock     0.840      0.000       nan     0.000       129     0/  678/    0/  129\n",
      "                        motion     0.968        nan     0.000     0.000         0     0/  781/   26/    0\n",
      "                          ping     0.994      0.950     1.000     0.974       100    95/  707/    0/    5\n",
      "                        status     1.000        nan       nan       nan         0     0/  807/    0/    0\n",
      "                        switch     1.000        nan       nan       nan         0     0/  807/    0/    0\n",
      "                   temperature     0.944      0.611     0.224     0.328        18    11/  751/   38/    7\n",
      "                       unknown     0.570      0.391     0.973     0.558       560   219/  241/    6/  341\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.937      0.177     0.200     0.169       807     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "             Weighted AVERAGES     0.674      0.403     0.804     0.515       807     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "       known Weighted AVERAGES     0.910      0.429     0.421     0.418       807     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      " known -ping Weighted AVERAGES     0.853      0.075     0.027     0.040       807     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "       non zero count AVERAGES     0.837      0.488     0.549     0.465       807     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "Macro F1 : 0.169153\n",
      "sample F1 : 0.402726\n",
      "weighted F1 : 0.515248\n",
      "Exact Match ACC : 0.40273 \n",
      "Total Records : 807 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 409 (0.507)%\n",
      "=============================================================================\n",
      "==================HOME Case : test_data_motion_2.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                        button     0.990        nan     0.000     0.000         0     0/ 7903/   80/    0\n",
      "              colorTemperature     1.000      0.333     1.000     0.500         3     1/ 7980/    0/    2\n",
      "                       contact     0.983      0.015     0.400     0.029       131     2/ 7849/    3/  129\n",
      "                         level     0.996      1.000     0.143     0.250         5     5/ 7948/   30/    0\n",
      "                          lock     0.999        nan     0.000     0.000         0     0/ 7972/   11/    0\n",
      "                        motion     0.917      0.280     0.071     0.113       150    42/ 7282/  551/  108\n",
      "                          ping     0.978      0.946     0.990     0.968      2708  2561/ 5250/   25/  147\n",
      "                        status     1.000        nan     0.000     0.000         0     0/ 7982/    1/    0\n",
      "                        switch     1.000      0.500     1.000     0.667         2     1/ 7981/    0/    1\n",
      "                   temperature     0.880      0.800     0.041     0.077        50    40/ 6987/  946/   10\n",
      "                       unknown     0.679      0.514     0.943     0.665      4953  2546/ 2877/  153/ 2407\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.948      0.399     0.417     0.297      7983     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "             Weighted AVERAGES     0.792      0.650     0.928     0.743      7983     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "       known Weighted AVERAGES     0.974      0.870     0.903     0.869      7983     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      " known -ping Weighted AVERAGES     0.940      0.267     0.208     0.084      7983     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "       non zero count AVERAGES     0.929      0.549     0.573     0.409      7983     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "Macro F1 : 0.297210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample F1 : 0.650862\n",
      "weighted F1 : 0.742906\n",
      "Exact Match ACC : 0.65063 \n",
      "Total Records : 7983 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 989 (0.124)%\n",
      "=============================================================================\n"
     ]
    }
   ],
   "source": [
    "xg_boost_results = []\n",
    "xgb_preds = []\n",
    "for i in range(len(xgb_test_known)) :\n",
    "    print( \"==================HOME Case : %s =============\" % test_names[ i] )\n",
    "    xgb_pred= xgb_classifier.predict( xgb_test_known[i][0])\n",
    "    xgb_pred = add_unknowns_back(xgb_pred,xgb_test[i][1], xgb_test_known[i][3] , classes)\n",
    "    xgb_preds.append(xgb_pred)\n",
    "    xg_boost_results.append(print_info( xgb_test[i][1], xgb_pred, classes, confidance=0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(rf_test_known)) :\n",
    "#     print( \"==================HOME Case : %s =============\" % test_names[ i] )\n",
    "#     rf_pred= xgb_classifier.predict( rf_test_known[i][0])\n",
    "#     print_info( rf_test_known[i][1], rf_pred, classes, confidance=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## string encoding test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "knownity_rf =  RandomForestClassifier(n_estimators=100, max_depth=400,\n",
    "                             random_state=0 )\n",
    "knownity_rf_results = knownity_rf.fit(x_xgb_train, known_unknown_y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_known_unknown_pred=knownity_rf.predict(x_xgb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================HOME Case : home_os_final.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                       unknown     0.761      0.662     0.987     0.793     11851  7851/ 5224/  101/ 4000\n",
      "                         known     0.761      0.981     0.566     0.718      5325  5224/ 7851/ 4000/  101\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.761      0.822     0.777     0.756     17176     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "             Weighted AVERAGES     0.761      0.761     0.857     0.770     17176     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "       known Weighted AVERAGES     0.761      0.981     0.566     0.718     17176     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "       non zero count AVERAGES     0.761      0.822     0.777     0.756     17176     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "Macro F1 : 0.755518\n",
      "sample F1 : 0.761237\n",
      "weighted F1 : 0.769725\n",
      "Exact Match ACC : 0.76124 \n",
      "Total Records : 17176 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 0 (0.000)%\n",
      "=============================================================================\n",
      "==================HOME Case : home_sk_final.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                       unknown     0.797      0.626     0.972     0.762      2997  1877/ 2738/   54/ 1120\n",
      "                         known     0.797      0.981     0.710     0.823      2792  2738/ 1877/ 1120/   54\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.797      0.803     0.841     0.793      5789     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "             Weighted AVERAGES     0.797      0.797     0.846     0.792      5789     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "       known Weighted AVERAGES     0.797      0.981     0.710     0.823      5789     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "       non zero count AVERAGES     0.797      0.803     0.841     0.793      5789     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "Macro F1 : 0.792614\n",
      "sample F1 : 0.797202\n",
      "weighted F1 : 0.791522\n",
      "Exact Match ACC : 0.79720 \n",
      "Total Records : 5789 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 0 (0.000)%\n",
      "=============================================================================\n",
      "==================HOME Case : test_data_light.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                       unknown     0.670      0.333     0.833     0.476       105    35/  121/    7/   70\n",
      "                         known     0.670      0.945     0.634     0.759       128   121/   35/   70/    7\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.670      0.639     0.733     0.617       233     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "             Weighted AVERAGES     0.670      0.670     0.724     0.631       233     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "       known Weighted AVERAGES     0.670      0.945     0.634     0.759       233     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "       non zero count AVERAGES     0.670      0.639     0.733     0.617       233     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "Macro F1 : 0.617406\n",
      "sample F1 : 0.669528\n",
      "weighted F1 : 0.631345\n",
      "Exact Match ACC : 0.66953 \n",
      "Total Records : 233 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 0 (0.000)%\n",
      "=============================================================================\n",
      "==================HOME Case : test_data_lock.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                       unknown     0.498      0.284     0.975     0.440       560   159/  243/    4/  401\n",
      "                         known     0.498      0.984     0.377     0.545       247   243/  159/  401/    4\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.498      0.634     0.676     0.493       807     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "             Weighted AVERAGES     0.498      0.498     0.792     0.472       807     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "       known Weighted AVERAGES     0.498      0.984     0.377     0.545       807     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "       non zero count AVERAGES     0.498      0.634     0.676     0.493       807     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "Macro F1 : 0.492644\n",
      "sample F1 : 0.498141\n",
      "weighted F1 : 0.472161\n",
      "Exact Match ACC : 0.49814 \n",
      "Total Records : 807 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 0 (0.000)%\n",
      "=============================================================================\n",
      "==================HOME Case : test_data_motion_2.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                       unknown     0.631      0.427     0.950     0.590      4953  2117/ 2919/  111/ 2836\n",
      "                         known     0.631      0.963     0.507     0.665      3030  2919/ 2117/ 2836/  111\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.631      0.695     0.729     0.627      7983     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "             Weighted AVERAGES     0.631      0.631     0.782     0.618      7983     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "       known Weighted AVERAGES     0.631      0.963     0.507     0.665      7983     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "       non zero count AVERAGES     0.631      0.695     0.729     0.627      7983     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "Macro F1 : 0.627077\n",
      "sample F1 : 0.630841\n",
      "weighted F1 : 0.618052\n",
      "Exact Match ACC : 0.63084 \n",
      "Total Records : 7983 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 0 (0.000)%\n",
      "=============================================================================\n"
     ]
    }
   ],
   "source": [
    "test_known_unknown_predicted = []\n",
    "for i in range(len(rf_tests)) :\n",
    "    print( \"==================HOME Case : %s =============\" % test_names[ i] )\n",
    "    rf_pred=knownity_rf.predict( xgb_test[i][0])\n",
    "    test_known_unknown_predicted.append(rf_pred)\n",
    "    print_info( np.array( make_known_unknown(rf_tests[i][1], classes.index('unknown'))), rf_pred, [\"unknown\",\"known\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rf_tests[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_indexes_train  = [ i for i in range(len(train_known_unknown_pred)) if train_known_unknown_pred[i][1] ==1 ]\n",
    "\n",
    "x_train_known = x_xgb_train[known_indexes_train]\n",
    "y_train_known = y_random_forest_train[known_indexes_train]\n",
    "\n",
    "rf_test_known = [] \n",
    "\n",
    "for test_index in range(len(test_known_unknown_predicted)):\n",
    "    known_indexes= [ i for i in range(len(test_known_unknown_predicted[test_index])) if test_known_unknown_predicted[test_index][i][1] ==1 ]\n",
    "    \n",
    "    rf_test_known.append(  (xgb_test[test_index][0][known_indexes], \n",
    "                            rf_tests[test_index][1][known_indexes],\n",
    "                            rf_tests[test_index][2],\n",
    "                            known_indexes\n",
    "                           ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=200, max_depth=400,\n",
    "                             random_state=0 )\n",
    "t_hist = clf.fit(x_train_known, y_train_known)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================HOME Case : home_os_final.json =============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/omid/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                        button     0.999      0.000       nan     0.000        12     0/17164/    0/   12\n",
      "              colorTemperature     1.000      1.000     1.000     1.000         5     5/17171/    0/    0\n",
      "                       contact     0.990      0.760     0.456     0.570       150   114/16890/  136/   36\n",
      "                         level     0.984      0.750     0.051     0.096        20    15/16879/  277/    5\n",
      "                          lock     1.000      0.941     0.865     0.901        34    32/17137/    5/    2\n",
      "                        motion     0.978      0.453     0.190     0.268       150    68/16736/  290/   82\n",
      "                          ping     0.986      0.951     0.998     0.974      4813  4579/12352/   11/  234\n",
      "                        status     0.996      0.846     0.688     0.759       117    99/17014/   45/   18\n",
      "                        switch     0.999      0.810     0.548     0.654        21    17/17141/   14/    4\n",
      "                   temperature     0.829      0.647     0.032     0.062       150    97/14134/ 2892/   53\n",
      "                       unknown     0.761      0.662     0.987     0.793     11851  7851/ 5224/  101/ 4000\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.957      0.711     0.529     0.552     17176     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "             Weighted AVERAGES     0.831      0.743     0.966     0.829     17176     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "       known Weighted AVERAGES     0.982      0.918     0.919     0.907     17176     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      " known -ping Weighted AVERAGES     0.953      0.678     0.348     0.417     17176     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "       non zero count AVERAGES     0.957      0.711     0.529     0.552     17176     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "Macro F1 : 0.552400\n",
      "sample F1 : 0.743161\n",
      "weighted F1 : 0.828919\n",
      "Exact Match ACC : 0.74226 \n",
      "Total Records : 17176 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 690 (0.040)%\n",
      "=============================================================================\n",
      "==================HOME Case : home_sk_final.json =============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/omid/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n",
      "/opt/anaconda3/envs/omid/lib/python3.6/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                        button     0.992      0.020     1.000     0.040        49     1/ 5740/    0/   48\n",
      "              colorTemperature     1.000      0.000       nan     0.000         1     0/ 5788/    0/    1\n",
      "                       contact     0.988      0.747     0.789     0.767       150   112/ 5609/   30/   38\n",
      "                         level     0.996      0.849     0.726     0.783        53    45/ 5719/   17/    8\n",
      "                          lock     1.000        nan     0.000     0.000         0     0/ 5787/    2/    0\n",
      "                        motion     0.961      0.594     0.313     0.410       133    79/ 5483/  173/   54\n",
      "                          ping     0.985      0.964     0.997     0.980      2301  2218/ 3482/    6/   83\n",
      "                        status     0.998      0.912     0.959     0.935       102    93/ 5683/    4/    9\n",
      "                        switch     0.998      0.500     0.900     0.643        18     9/ 5770/    1/    9\n",
      "                   temperature     0.831      0.727     0.104     0.182       150   109/ 4700/  939/   41\n",
      "                       unknown     0.797      0.626     0.972     0.762      2997  1877/ 2738/   54/ 1120\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.959      0.540     0.615     0.500      5789     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "             Weighted AVERAGES     0.886      0.763     0.938     0.821      5789     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "       known Weighted AVERAGES     0.977      0.902     0.904     0.880      5789     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      " known -ping Weighted AVERAGES     0.949      0.683     0.575     0.529      5789     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "       non zero count AVERAGES     0.954      0.594     0.676     0.550      5789     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "Macro F1 : 0.500157\n",
      "sample F1 : 0.765290\n",
      "weighted F1 : 0.820635\n",
      "Exact Match ACC : 0.75954 \n",
      "Total Records : 5789 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 136 (0.023)%\n",
      "=============================================================================\n",
      "==================HOME Case : test_data_light.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                        button     1.000        nan       nan       nan         0     0/  233/    0/    0\n",
      "              colorTemperature     0.876      0.065     1.000     0.121        31     2/  202/    0/   29\n",
      "                       contact     0.996        nan     0.000     0.000         0     0/  232/    1/    0\n",
      "                         level     0.820      0.902     0.685     0.779        82    74/  117/   34/    8\n",
      "                          lock     1.000        nan       nan       nan         0     0/  233/    0/    0\n",
      "                        motion     0.974        nan     0.000     0.000         0     0/  227/    6/    0\n",
      "                          ping     1.000      1.000     1.000     1.000        26    26/  207/    0/    0\n",
      "                        status     0.996        nan     0.000     0.000         0     0/  232/    1/    0\n",
      "                        switch     0.927      0.550     0.579     0.564        20    11/  205/    8/    9\n",
      "                   temperature     0.906      0.500     0.045     0.083         2     1/  210/   21/    1\n",
      "                       unknown     0.670      0.333     0.833     0.476       105    35/  121/    7/   70\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.924      0.305     0.377     0.275       233     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "             Weighted AVERAGES     0.793      0.560     0.798     0.583       233     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "       known Weighted AVERAGES     0.874      0.708     0.775     0.653       233     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      " known -ping Weighted AVERAGES     0.850      0.652     0.732     0.586       233     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "       non zero count AVERAGES     0.866      0.558     0.690     0.504       233     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "Macro F1 : 0.274890\n",
      "sample F1 : 0.558655\n",
      "weighted F1 : 0.583007\n",
      "Exact Match ACC : 0.49785 \n",
      "Total Records : 233 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 23 (0.099)%\n",
      "=============================================================================\n",
      "==================HOME Case : test_data_lock.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                        button     1.000        nan       nan       nan         0     0/  807/    0/    0\n",
      "              colorTemperature     1.000        nan       nan       nan         0     0/  807/    0/    0\n",
      "                       contact     0.996        nan     0.000     0.000         0     0/  804/    3/    0\n",
      "                         level     0.999        nan     0.000     0.000         0     0/  806/    1/    0\n",
      "                          lock     0.840      0.000       nan     0.000       129     0/  678/    0/  129\n",
      "                        motion     0.973        nan     0.000     0.000         0     0/  785/   22/    0\n",
      "                          ping     0.994      0.950     1.000     0.974       100    95/  707/    0/    5\n",
      "                        status     0.996        nan     0.000     0.000         0     0/  804/    3/    0\n",
      "                        switch     1.000        nan       nan       nan         0     0/  807/    0/    0\n",
      "                   temperature     0.943      0.667     0.231     0.343        18    12/  749/   40/    6\n",
      "                       unknown     0.498      0.284     0.975     0.440       560   159/  243/    4/  401\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.931      0.173     0.201     0.160       807     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "             Weighted AVERAGES     0.624      0.330     0.806     0.434       807     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "       known Weighted AVERAGES     0.910      0.433     0.422     0.419       807     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      " known -ping Weighted AVERAGES     0.853      0.082     0.028     0.042       807     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "       non zero count AVERAGES     0.819      0.475     0.552     0.439       807     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "Macro F1 : 0.159732\n",
      "sample F1 : 0.329203\n",
      "weighted F1 : 0.433599\n",
      "Exact Match ACC : 0.32838 \n",
      "Total Records : 807 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 474 (0.587)%\n",
      "=============================================================================\n",
      "==================HOME Case : test_data_motion_2.json =============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/omid/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1515: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                        button     0.999        nan     0.000     0.000         0     0/ 7979/    4/    0\n",
      "              colorTemperature     1.000      0.667     1.000     0.800         3     2/ 7980/    0/    1\n",
      "                       contact     0.978      0.000     0.000     0.000       131     0/ 7806/   46/  131\n",
      "                         level     0.995      1.000     0.122     0.217         5     5/ 7942/   36/    0\n",
      "                          lock     0.999        nan     0.000     0.000         0     0/ 7974/    9/    0\n",
      "                        motion     0.915      0.300     0.072     0.117       150    45/ 7257/  576/  105\n",
      "                          ping     0.989      0.977     0.991     0.984      2708  2645/ 5250/   25/   63\n",
      "                        status     0.995        nan     0.000     0.000         0     0/ 7944/   39/    0\n",
      "                        switch     1.000      0.500     0.500     0.500         2     1/ 7980/    1/    1\n",
      "                   temperature     0.882      0.700     0.036     0.069        50    35/ 7008/  925/   15\n",
      "                       unknown     0.631      0.427     0.950     0.590      4953  2117/ 2919/  111/ 2836\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.944      0.416     0.334     0.298      7983     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "             Weighted AVERAGES     0.765      0.606     0.926     0.701      7983     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "       known Weighted AVERAGES     0.983      0.896     0.886     0.882      7983     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      " known -ping Weighted AVERAGES     0.937      0.258     0.051     0.075      7983     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "       non zero count AVERAGES     0.924      0.571     0.459     0.410      7983     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "Macro F1 : 0.297880\n",
      "sample F1 : 0.607103\n",
      "weighted F1 : 0.701012\n",
      "Exact Match ACC : 0.60654 \n",
      "Total Records : 7983 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 1404 (0.176)%\n",
      "=============================================================================\n"
     ]
    }
   ],
   "source": [
    "rf_results = []\n",
    "rf_preds = [] \n",
    "for i in range(len(rf_test_known)) :\n",
    "    print( \"==================HOME Case : %s =============\" % test_names[ i] )\n",
    "    rf_pred= clf.predict( rf_test_known[i][0])\n",
    "    rf_pred = add_unknowns_back(rf_pred,rf_tests[i][1], rf_test_known[i][3] , classes)\n",
    "    rf_preds.append(rf_pred)\n",
    "    rf_results.append(print_info( rf_tests[i][1], rf_pred, classes, confidance=0.09))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(rf_test_known)) :\n",
    "#     print( \"==================HOME Case : %s =============\" % test_names[ i] )\n",
    "#     rf_pred= clf.predict( rf_test_known[i][0])\n",
    "#     print_info( rf_test_known[i][1], rf_pred, classes, confidance=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(for_classes, classes):\n",
    "    ret = np.zeros(len(classes))\n",
    "    \n",
    "    for x in for_classes:\n",
    "        ret[classes.index(x)] = 1\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., 0.])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_mask([\"a\",\"c\"], [\"d\",\"c\",\"a\",\"b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_classes = [\n",
    " 'button',\n",
    " 'colorTemperature',\n",
    " 'contact',\n",
    " 'level',\n",
    " 'lock',\n",
    " 'motion',\n",
    " \n",
    " 'status',\n",
    " \n",
    " 'temperature',\n",
    " 'unknown']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classes =[\n",
    "   'ping',\n",
    "    'switch',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_mask =create_mask(rf_classes, classes)\n",
    "xgb_mask = create_mask(xgb_classes, classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================HOME Case : home_os_final.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                        button     1.000      0.750     0.692     0.720        12     9/17160/    4/    3\n",
      "              colorTemperature     1.000      0.800     1.000     0.889         5     4/17171/    0/    1\n",
      "                       contact     0.993      0.740     0.561     0.638       150   111/16939/   87/   39\n",
      "                         level     0.985      0.750     0.056     0.103        20    15/16901/  255/    5\n",
      "                          lock     1.000      0.941     0.889     0.914        34    32/17138/    4/    2\n",
      "                        motion     0.980      0.460     0.212     0.291       150    69/16770/  256/   81\n",
      "                          ping     0.986      0.951     0.998     0.974      4813  4579/12352/   11/  234\n",
      "                        status     0.996      0.812     0.704     0.754       117    95/17019/   40/   22\n",
      "                        switch     0.999      0.810     0.548     0.654        21    17/17141/   14/    4\n",
      "                   temperature     0.819      0.747     0.035     0.067       150   112/13955/ 3071/   38\n",
      "                       unknown     0.766      0.669     0.988     0.798     11851  7924/ 5231/   94/ 3927\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.957      0.766     0.608     0.618     17176     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "             Weighted AVERAGES     0.834      0.749     0.968     0.833     17176     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "       known Weighted AVERAGES     0.982      0.922     0.925     0.911     17176     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      " known -ping Weighted AVERAGES     0.952      0.704     0.394     0.451     17176     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "       non zero count AVERAGES     0.957      0.766     0.608     0.618     17176     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "Macro F1 : 0.618334\n",
      "sample F1 : 0.748224\n",
      "weighted F1 : 0.833445\n",
      "Exact Match ACC : 0.74691 \n",
      "Total Records : 17176 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 540 (0.031)%\n",
      "=============================================================================\n",
      "==================HOME Case : home_sk_final.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                        button     1.000      1.000     0.961     0.980        49    49/ 5738/    2/    0\n",
      "              colorTemperature     1.000      1.000     0.500     0.667         1     1/ 5787/    1/    0\n",
      "                       contact     0.989      0.733     0.821     0.775       150   110/ 5615/   24/   40\n",
      "                         level     0.996      0.811     0.754     0.782        53    43/ 5722/   14/   10\n",
      "                          lock     1.000        nan       nan       nan         0     0/ 5789/    0/    0\n",
      "                        motion     0.961      0.609     0.316     0.416       133    81/ 5481/  175/   52\n",
      "                          ping     0.985      0.964     0.997     0.980      2301  2218/ 3482/    6/   83\n",
      "                        status     0.997      0.863     0.967     0.912       102    88/ 5684/    3/   14\n",
      "                        switch     0.998      0.500     0.900     0.643        18     9/ 5770/    1/    9\n",
      "                   temperature     0.831      0.787     0.111     0.194       150   118/ 4692/  947/   32\n",
      "                       unknown     0.794      0.622     0.969     0.757      2997  1863/ 2732/   60/ 1134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/omid/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n",
      "/opt/anaconda3/envs/omid/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if __name__ == '__main__':\n",
      "/opt/anaconda3/envs/omid/lib/python3.6/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if sys.path[0] == '':\n",
      "/opt/anaconda3/envs/omid/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1515: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.959      0.717     0.663     0.646      5789     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "             Weighted AVERAGES     0.885      0.769     0.938     0.826      5789     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "       known Weighted AVERAGES     0.977      0.919     0.906     0.897      5789     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      " known -ping Weighted AVERAGES     0.950      0.761     0.586     0.603      5789     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "       non zero count AVERAGES     0.955      0.789     0.730     0.711      5789     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "Macro F1 : 0.646022\n",
      "sample F1 : 0.767594\n",
      "weighted F1 : 0.826479\n",
      "Exact Match ACC : 0.76300 \n",
      "Total Records : 5789 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 120 (0.021)%\n",
      "=============================================================================\n",
      "==================HOME Case : test_data_light.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                        button     1.000        nan       nan       nan         0     0/  233/    0/    0\n",
      "              colorTemperature     0.966      0.839     0.897     0.867        31    26/  199/    3/    5\n",
      "                       contact     1.000        nan       nan       nan         0     0/  233/    0/    0\n",
      "                         level     0.863      0.756     0.838     0.795        82    62/  139/   12/   20\n",
      "                          lock     1.000        nan       nan       nan         0     0/  233/    0/    0\n",
      "                        motion     0.974        nan     0.000     0.000         0     0/  227/    6/    0\n",
      "                          ping     1.000      1.000     1.000     1.000        26    26/  207/    0/    0\n",
      "                        status     1.000        nan       nan       nan         0     0/  233/    0/    0\n",
      "                        switch     0.927      0.550     0.579     0.564        20    11/  205/    8/    9\n",
      "                   temperature     0.901      0.500     0.043     0.080         2     1/  209/   22/    1\n",
      "                       unknown     0.725      0.400     0.977     0.568       105    42/  127/    1/   63\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.941      0.368     0.394     0.352       233     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "             Weighted AVERAGES     0.839      0.632     0.890     0.711       233     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "       known Weighted AVERAGES     0.913      0.783     0.833     0.804       233     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      " known -ping Weighted AVERAGES     0.896      0.741     0.801     0.767       233     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "       non zero count AVERAGES     0.897      0.674     0.722     0.646       233     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "Macro F1 : 0.352110\n",
      "sample F1 : 0.609156\n",
      "weighted F1 : 0.710838\n",
      "Exact Match ACC : 0.54077 \n",
      "Total Records : 233 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 46 (0.197)%\n",
      "=============================================================================\n",
      "==================HOME Case : test_data_lock.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                        button     0.998        nan     0.000     0.000         0     0/  805/    2/    0\n",
      "              colorTemperature     1.000        nan       nan       nan         0     0/  807/    0/    0\n",
      "                       contact     1.000        nan       nan       nan         0     0/  807/    0/    0\n",
      "                         level     0.999        nan     0.000     0.000         0     0/  806/    1/    0\n",
      "                          lock     0.840      0.000       nan     0.000       129     0/  678/    0/  129\n",
      "                        motion     0.968        nan     0.000     0.000         0     0/  781/   26/    0\n",
      "                          ping     0.994      0.950     1.000     0.974       100    95/  707/    0/    5\n",
      "                        status     1.000        nan       nan       nan         0     0/  807/    0/    0\n",
      "                        switch     1.000        nan       nan       nan         0     0/  807/    0/    0\n",
      "                   temperature     0.944      0.611     0.224     0.328        18    11/  751/   38/    7\n",
      "                       unknown     0.570      0.391     0.973     0.558       560   219/  241/    6/  341\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.937      0.177     0.200     0.169       807     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "             Weighted AVERAGES     0.674      0.403     0.804     0.515       807     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "       known Weighted AVERAGES     0.910      0.429     0.421     0.418       807     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      " known -ping Weighted AVERAGES     0.853      0.075     0.027     0.040       807     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "       non zero count AVERAGES     0.837      0.488     0.549     0.465       807     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "Macro F1 : 0.169153\n",
      "sample F1 : 0.402726\n",
      "weighted F1 : 0.515248\n",
      "Exact Match ACC : 0.40273 \n",
      "Total Records : 807 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 409 (0.507)%\n",
      "=============================================================================\n",
      "==================HOME Case : test_data_motion_2.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count            TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                        button     0.990        nan     0.000     0.000         0     0/ 7903/   80/    0\n",
      "              colorTemperature     1.000      0.333     1.000     0.500         3     1/ 7980/    0/    2\n",
      "                       contact     0.983      0.015     0.400     0.029       131     2/ 7849/    3/  129\n",
      "                         level     0.996      1.000     0.143     0.250         5     5/ 7948/   30/    0\n",
      "                          lock     0.999        nan     0.000     0.000         0     0/ 7972/   11/    0\n",
      "                        motion     0.917      0.280     0.071     0.113       150    42/ 7282/  551/  108\n",
      "                          ping     0.989      0.977     0.991     0.984      2708  2645/ 5250/   25/   63\n",
      "                        status     1.000        nan     0.000     0.000         0     0/ 7982/    1/    0\n",
      "                        switch     1.000      0.500     0.500     0.500         2     1/ 7980/    1/    1\n",
      "                   temperature     0.880      0.800     0.041     0.077        50    40/ 6987/  946/   10\n",
      "                       unknown     0.679      0.514     0.943     0.665      4953  2546/ 2877/  153/ 2407\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.949      0.402     0.372     0.284      7983     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "             Weighted AVERAGES     0.795      0.660     0.928     0.748      7983     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "       known Weighted AVERAGES     0.983      0.897     0.903     0.883      7983     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      " known -ping Weighted AVERAGES     0.940      0.267     0.205     0.083      7983     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n",
      "       non zero count AVERAGES     0.931      0.552     0.511     0.390      7983     0/    0/    0/    0\n",
      "------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1 : 0.283525\n",
      "sample F1 : 0.661218\n",
      "weighted F1 : 0.748322\n",
      "Exact Match ACC : 0.66065 \n",
      "Total Records : 7983 \n",
      "Total ZXeros in True : 0 (0.000)%\n",
      "Total ZXeros in Test : 908 (0.114)%\n",
      "=============================================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(rf_test_known)) :\n",
    "    print( \"==================HOME Case : %s =============\" % test_names[ i] )\n",
    "    the_pred = rf_preds[i] * rf_mask + xgb_preds[i] * xgb_mask\n",
    "#     rf_pred = add_unknowns_back(rf_pred,rf_tests[i][1], rf_test_known[i][3] , classes)\n",
    "#     rf_preds.append(rf_pred)\n",
    "    print_info( rf_tests[i][1], the_pred, classes, confidance=0.09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show comparative resutls for xgb and RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, desc = print_info( rf_test_known[i][1], rf_pred, classes, confidance=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_nan(inp):\n",
    "    if np.isnan(inp):\n",
    "        return 0\n",
    "    else:\n",
    "        return inp\n",
    "desc = xg_boost_results[0][1]\n",
    "index = 0 \n",
    "for index in range(len(test_names)):\n",
    "    plt.figure(figsize=(14,4))\n",
    "    plt.plot( [x[0] for x in xg_boost_results[index][0]] , \n",
    "         [fix_nan(x[desc.index(\"F Score\")]) for x in xg_boost_results[index][0]],\n",
    "        label=\"XGBoost\",\n",
    "         color=\"red\" ,\n",
    "             alpha=0.5\n",
    "        )\n",
    "    plt.plot( [x[0] for x in rf_results[index][0]] , \n",
    "         [fix_nan(x[desc.index(\"F Score\")]) for x in rf_results[index][0]],\n",
    "        label=\"RF\",\n",
    "         color=\"blue\" ,\n",
    "             alpha=0.5\n",
    "        )\n",
    "    \n",
    "    plt.plot( [x[0] for x in xg_boost_results[index][0]] , \n",
    "         [fix_nan(x[desc.index(\"Precision\")]) for x in xg_boost_results[index][0]],\n",
    "        label=\"XGBoost - Precision\",\n",
    "         color=\"red\" ,\n",
    "             marker=\"s\",\n",
    "             alpha=0.5\n",
    "        )\n",
    "    plt.plot( [x[0] for x in rf_results[index][0]] , \n",
    "         [fix_nan(x[desc.index(\"Precision\")]) for x in rf_results[index][0]],\n",
    "        label=\"RF - Precision\",\n",
    "         color=\"blue\" ,\n",
    "             marker=\"s\",\n",
    "             alpha=0.5\n",
    "        )\n",
    "    plt.plot( [x[0] for x in xg_boost_results[index][0]] , \n",
    "         [fix_nan(x[desc.index(\"Recall\")]) for x in xg_boost_results[index][0]],\n",
    "        label=\"XGBoost - Recall\",\n",
    "         color=\"red\" ,\n",
    "             marker=\"+\",\n",
    "             alpha=0.5\n",
    "        )\n",
    "    plt.plot( [x[0] for x in rf_results[index][0]] , \n",
    "         [fix_nan(x[desc.index(\"Recall\")]) for x in rf_results[index][0]],\n",
    "        label=\"RF - Recall\",\n",
    "         color=\"blue\" ,\n",
    "             marker=\"+\",\n",
    "             alpha=0.5\n",
    "        )\n",
    "\n",
    "    # plt.plot( )\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=90, ha='right')\n",
    "    plt.title(test_names[ index] )\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok ... bye bye now ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest baseline calc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preproicess the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_size= 50\n",
    "x_random_forest_train,y_random_forest_train, _ = pre_process_raw( x_train, y_train , dim_size, zero_pad=True, normalize=False, classes=classes)\n",
    "rf_tests  = [ pre_process_raw( x_test[i], y_test[i] , dim_size, zero_pad=True, normalize=False, classes=classes, class_cap=TEST_CLASS_CAP) for i in range(len(x_test)) ] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first stage RF will learn if it is a known or unknown instance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "knownity_rf =  RandomForestClassifier(n_estimators=960, max_depth=9050,\n",
    "                             random_state=0 )\n",
    "knownity_rf_results = knownity_rf.fit(x_random_forest_train, known_unknown_y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_known_unknown_pred=knownity_rf.predict(x_random_forest_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_known_unknown_predicted = []\n",
    "for i in range(len(rf_tests)) :\n",
    "    print( \"==================HOME Case : %s =============\" % test_names[ i] )\n",
    "    rf_pred=knownity_rf.predict( rf_tests[i][0])\n",
    "    test_known_unknown_predicted.append(rf_pred)\n",
    "    print_info( np.array( known_unknown_y_test[i]), rf_pred, [\"unknown\",\"known\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_indexes_train  = [ i for i in range(len(train_known_unknown_pred)) if train_known_unknown_pred[i][1] ==1 ]\n",
    "\n",
    "x_train_known = x_random_forest_train[known_indexes_train]\n",
    "y_train_known = y_random_forest_train[known_indexes_train]\n",
    "\n",
    "rf_test_known = [] \n",
    "\n",
    "for test_index in range(len(rf_tests)):\n",
    "    known_indexes= [ i for i in range(len(test_known_unknown_predicted[test_index])) if test_known_unknown_predicted[test_index][i][1] ==1 ]\n",
    "    \n",
    "    rf_test_known.append(  (rf_tests[test_index][0][known_indexes], \n",
    "                            rf_tests[test_index][1][known_indexes],\n",
    "                            rf_tests[test_index][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=960, max_depth=9050,\n",
    "                             random_state=0 )\n",
    "t_hist = clf.fit(x_train_known, y_train_known)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(rf_test_known)) :\n",
    "    print( \"==================HOME Case : %s =============\" % test_names[ i] )\n",
    "    rf_pred= clf.predict( rf_test_known[i][0])\n",
    "    print_info( rf_test_known[i][1], rf_pred, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "dim_size =20\n",
    "x_lstm_prossed_train,y_lstm_prossed_train, _ = pre_process_raw( x_train, y_train , dim_size, zero_pad=False, normalize=False,classes=classes, as_string=True)\n",
    "_, y_s_lstm_processed_train ,_ =  pre_process_raw( x_train, y_train_service , dim_size, zero_pad=False, normalize=False,classes=service_classes, as_string=True)\n",
    "# x_lstm_prossed_test,y_lstm_prossed_test, _ = pre_process_raw( x_test, y_test_2 , dim_size, zero_pad=True, normalize=False,classes=classes)\n",
    "lstm_tests  = [ pre_process_raw( x_test[i], y_test[i] , dim_size, zero_pad=False, normalize=False, classes=classes, class_cap=TEST_CLASS_CAP) for i in range(len(x_test)) ] \n",
    "lstm_tests_services  = [ pre_process_raw( x_test[i], y_test_service[i] , dim_size, zero_pad=False, normalize=True, classes=service_classes, class_cap=TEST_CLASS_CAP) for i in range(len(x_test)) ] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " x_random_forest_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x_lstm_prossed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_indexes_train  = [ i for i in range(len(train_known_unknown_pred)) if train_known_unknown_pred[i][1] ==1 ]\n",
    "\n",
    "# x_lstm_prossed_train = x_lstm_prossed_train[known_indexes_train]\n",
    "y_lstm_prossed_train = y_lstm_prossed_train[known_indexes_train]\n",
    "\n",
    "lstm_tests_known = [] \n",
    "\n",
    "for test_index in range(len(rf_tests)):\n",
    "    known_indexes= [ i for i in range(len(test_known_unknown_predicted[test_index])) if test_known_unknown_predicted[test_index][i][1] ==1 ]\n",
    "    \n",
    "    lstm_tests_known.append(  (lstm_tests[test_index][0][known_indexes], \n",
    "                            lstm_tests[test_index][1][known_indexes],\n",
    "                            lstm_tests[test_index][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_lstm_prossed_test2 = np.expand_dims(x_lstm_prossed_test,axis=1)\n",
    "# x_lstm_prossed_train2 =np.expand_dims(x_random_forest_train,axis=1)\n",
    "# x_lstm_prossed_train2 =x_random_forest_train\n",
    "\n",
    "\n",
    "# x_lstm_prossed_test2 = x_lstm_prossed_test.reshape(len(x_lstm_prossed_test),dim_size,1)\n",
    "x_lstm_prossed_train2 =x_train_known.toarray().reshape((x_train_known.shape[0],x_train_known.shape[1],1))\n",
    "\n",
    "# y_lstm_prossed_test2 = y_lstm_prossed_test.reshape(len(y_lstm_prossed_test),len(classes),1)\n",
    "# y_lstm_prossed_train2 =y_lstm_prossed_train.reshape(len(y_lstm_prossed_train),len(classes),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tt  in range( len(rf_test_known) ):\n",
    "    rf_test_known[tt]= (rf_test_known[tt][0].toarray().reshape(rf_test_known[tt][0].shape[0],\n",
    "                                                     rf_test_known[tt][0].shape[1],\n",
    "                                                     1) ,\n",
    "                           rf_test_known[tt][1],\n",
    "                           rf_test_known[tt][1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes.index('ping'), classes.index('unknown'), len(classes), len(y_lstm_prossed_train[0])\n",
    "ping=classes.index('ping')\n",
    "unknown =classes.index('unknown')\n",
    "key = np.ones_like(y_lstm_prossed_train[0])\n",
    "key[unknown]=0\n",
    "key[ping]=0\n",
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bidirectional LSTM :D \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Flatten\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import Input, merge, concatenate, Conv2D, MaxPooling2D, Activation, UpSampling2D, Dropout, Conv2DTranspose, UpSampling2D, Lambda\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization as bn\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.merge import add\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers.merge import add\n",
    "from keras.layers import Dense,Conv1D,Dropout,Activation,BatchNormalization,MaxPooling1D,Flatten,Masking,TimeDistributed\n",
    "from keras.layers.recurrent import LSTM,GRU,SimpleRNN\n",
    "from keras.models import Input,Sequential,Model\n",
    "from keras.layers.merge import add\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import MSE,MSLE\n",
    "model2 = Sequential()\n",
    "\n",
    "\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def f1_perClass(y_true, y_pred):\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "\n",
    "def f1_perRow(y_true, y_pred):\n",
    "\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss_perRow(y_true, y_pred):\n",
    "    \n",
    "#     y_true = y_true * \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1) \n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1) \n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1) \n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1) \n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "#     f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "def f1_loss_perClass(y_true, y_pred):\n",
    "    key = K.variable([1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.])\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = (2*p*r / (p+r+K.epsilon()))*key\n",
    "    \n",
    "#     f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lstm_prossed_train2[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs  = Input(  (x_lstm_prossed_train2[0].shape[0]  ,1) )\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(inputs)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(64,3,padding='same')(out)\n",
    "\n",
    "dout_1  = Dropout(0.2)(out)\n",
    "# flt_1   = Flatten()(dout_1)\n",
    "dense_1 = Dense(32, activation='relu')(dout_1)\n",
    "# dout_3  = Dropout(0.2)(dense_1)\n",
    "flt_1   = Flatten()(dense_1)\n",
    "\n",
    "out_put_final = Dense(len(classes), activation='sigmoid', name='Event_output')(flt_1)\n",
    "\n",
    "\n",
    "model2 = Model(inputs=[inputs], outputs=[out_put_final])\n",
    "\n",
    "\n",
    "losses = {\n",
    "    \"Event_output\": f1_loss_perClass ,\n",
    "#     \"Event_output\": f1_loss_perRow \n",
    "    \"Event_output\": \"categorical_crossentropy\",\n",
    "}\n",
    "lossWeights = {\"Event_output\": 200,\n",
    "#                \"Event_output\": 30.0 \n",
    "    \"Event_output\": 5\n",
    "              }\n",
    " \n",
    "\n",
    "\n",
    "model2.compile(loss=losses,loss_weights=lossWeights, optimizer='adam', metrics=[f1_perRow,f1_perClass,'acc'])\n",
    "# model2.compile(loss=losses, loss_weights=lossWeights, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('IoTDownNet_cnn_newloss', monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "print(model2.summary())\n",
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=70, batch_size=2000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "losses = {\n",
    "    \"Event_output\": f1_loss_perClass ,\n",
    "    \"Event_output\": f1_loss_perRow ,\n",
    "    \"Event_output\": \"categorical_crossentropy\",\n",
    "}\n",
    "lossWeights = {\n",
    "    \"Event_output\": 300,\n",
    "               \"Event_output\": 300.0 ,\n",
    "    \"Event_output\": 20\n",
    "              }\n",
    " \n",
    "\n",
    "\n",
    "model2.compile(loss=losses,loss_weights=lossWeights, optimizer='adam', metrics=[f1_perRow,f1_perClass,'acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=100, batch_size=7010, shuffle=True, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=200, batch_size=7000, shuffle=True, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=300, batch_size=7000, shuffle=True, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.load_weights('IoTDownNet_cnn_nocca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lstm_tests_known)) :\n",
    "    print( \"==================HOME Case : %s =============\" % test_names[i])\n",
    "    lstm_pred= model2.predict( lstm_tests_known[i][0])\n",
    "    \n",
    "#     print_info( lstm_tests[i][1], lstm_pred, classes , confidance=0.7)\n",
    "    print_info( lstm_tests_known[i][1], lstm_pred, classes , confidance=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_tests_known[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_output(classes, instance):\n",
    "    ret = [] \n",
    "    for x in range(len(instance)):\n",
    "        if instance[x] > 0.6:\n",
    "            ret.append(classes[x])\n",
    "    return ret\n",
    "def save_resutls(inp, y, y_hat, classes,the_name):\n",
    "    items = [] \n",
    "    for i in range(len(inp)):\n",
    "#         print(describe_output(classes, y[i]))\n",
    "        items.append({'inp': str(list(inp[i])),\n",
    "                     'true': list(describe_output(classes, y[i])),\n",
    "                     'pred': list(describe_output(classes, y_hat[i]))\n",
    "                     })\n",
    "#         print(items[-1])\n",
    "#         return\n",
    "    with open('cnn_for_karthika_'+the_name +'.json', 'w') as outfile:\n",
    "        json.dump(items, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_output(['a','b'],[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred = []\n",
    "for i in range(len(lstm_tests_known)) :\n",
    "    print( \"==================HOME Case : %s =============\" % test_names[i])\n",
    "    lstm_pred= model2.predict( rf_test_known[i][0])\n",
    "#     save_resutls( lstm_tests_known[i][0], rf_test_known[i][1], lstm_pred,  classes, test_names[i] )\n",
    "#     break \n",
    "#     print_info( lstm_tests[i][1], lstm_pred, classes , confidance=0.7)\n",
    "    print_info( rf_test_known[i][1], lstm_pred, classes , confidance=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_resutls( lstm_tests_known[0][0], rf_test_known[0][1], lstm_pred, classes, test_names[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_output(classes, rf_test_known[0][1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs  = Input(  (x_lstm_prossed_train2[0].shape[0]  ,1) )\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(inputs)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(64,3,padding='same')(out)\n",
    "# bi_d_1 = Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat') (inputs)\n",
    "lstm_1 =  LSTM(30 ,  recurrent_dropout=0.04, return_sequences=True)(out)\n",
    "# lstm_2 = LSTM(30 ,  recurrent_dropout=0.14, return_sequences=True)(lstm_1)\n",
    "\n",
    "bi_d_1 =Dense(64, activation='relu')  (lstm_1)\n",
    "lstm_1 =  Dense(32, activation='relu')(bi_d_1)\n",
    "lstm_2 = Dense(16, activation='relu')(lstm_1)\n",
    "\n",
    "\n",
    "\n",
    "# td_1    = TimeDistributed(Dense(256, activation='relu'))(lstm_2)\n",
    "# dout_1  = Dropout(0.1)(td_1)\n",
    "dout_1  = Dropout(0.1)(lstm_2)\n",
    "flt_1   = Flatten()(dout_1)\n",
    "dense_1 = Dense(16, activation='relu')(flt_1)\n",
    "dout_2  = Dropout(0.2)(dense_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # bi_d_1 = Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat') (inputs)\n",
    "# lstm_1 =  LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True)(inputs)\n",
    "# # lstm_1 = LSTM(40 ,  recurrent_dropout=0.14, return_sequences=True)(lstm_1)\n",
    "\n",
    "# bi_d_raw_1 =Dense(128, activation='relu')  (lstm_1)\n",
    "# lstm_raw_1 =  Dense(128, activation='relu')(bi_d_raw_1)\n",
    "# lstm_raw_2 = Dense(128, activation='relu')(lstm_raw_1)\n",
    "\n",
    "# dout_1  = Dropout(0.1)(lstm_raw_2)\n",
    "# flt_1   = Flatten()(dout_1)\n",
    "# dense_1 = Dense(128, activation='relu')(flt_1)\n",
    "# dout_3  = Dropout(0.2)(dense_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# out = Conv1D(128,3,padding='same')(inputs)\n",
    "# out = BatchNormalization()(out)\n",
    "# out = Activation('relu')(out)\n",
    "# out = Dropout(0.2)(out)\n",
    "# out = Conv1D(128,3,padding='same')(out)\n",
    "# # out = Flatten()(out)\n",
    "# # out = MaxPooling1D(2,padding='same', name ='pooling')(out)\n",
    "\n",
    "\n",
    "# out = Conv1D(128,3,padding='same')(out)\n",
    "# out = BatchNormalization()(out)\n",
    "# out = Activation('relu')(out)\n",
    "# out = Dropout(0.2)(out)\n",
    "# out = Conv1D(128,3,padding='same')(out)\n",
    "# # out = Flatten()(out)\n",
    "# # out = MaxPooling1D(2,padding='same', name ='pooling2')(out)\n",
    "\n",
    "\n",
    "# out = Conv1D(128,3,padding='same')(out)\n",
    "# out = BatchNormalization()(out)\n",
    "# out = Activation('relu')(out)\n",
    "# out = Dropout(0.2)(out)\n",
    "# out = Conv1D(128,3,padding='same')(out)\n",
    "\n",
    "\n",
    "# out = Flatten()(out)\n",
    "# out = MaxPooling1D(2,padding='same', name ='pooling')(out)\n",
    "\n",
    "\n",
    "\n",
    "# fl_out_1 = Flatten()(dout_2)\n",
    "\n",
    "# fl_out_cnn = Flatten()(out)\n",
    "\n",
    "# out_new = concatenate( [fl_out_1, fl_out_cnn] , name='mergerguy')\n",
    "# out_new = concatenate( [dout_2, fl_out_cnn,dout_3] , name='mergerguy')\n",
    "# out_new = concatenate( [dout_2] , name='mergerguy')\n",
    "\n",
    "# dens_out_1 = Dense( 128, activation='relu' )(out_new)\n",
    "# dens_out_2 = Dense( 128, activation='relu' )(dens_out_1)\n",
    "# dens_out_3 = Dense( 128, activation='relu' )(dens_out_2)\n",
    "\n",
    "# fl2  = Flatten()(out_new)\n",
    "\n",
    "out_put_final = Dense(len(classes), activation='sigmoid', name='Event_output')(dout_2)\n",
    "\n",
    "# toService_1 = Dense( 130, name=\"to_service1\" )(dens_out_3)\n",
    "# toService_1 = Dense( 130, name=\"to_service2\" )(toService_1)\n",
    "\n",
    "# service_output = Dense(len(classes  ), activation=\"sigmoid\", name = 'service_output')(toService_1)\n",
    "\n",
    "\n",
    "\n",
    "model2 = Model(inputs=[inputs], outputs=[out_put_final])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model2.add(Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat'))\n",
    "# model2.add( LSTM(60 ,  recurrent_dropout=0.04, return_sequences=True))\n",
    "# model2.add( LSTM(30 ,  recurrent_dropout=0.14, return_sequences=True))\n",
    "# # model2.add(Bidirectional( LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True),merge_mode='concat'))\n",
    "# model2.add(TimeDistributed(Dense(256, activation='relu')))\n",
    "# model2.add(Dropout(0.1))\n",
    "# model2.add(Flatten())\n",
    "# model2.add(Dense(128, activation='relu'))\n",
    "# model2.add(Dropout(0.2))\n",
    "# model2.add(Dense(len(classes), activation='sigmoid'))\n",
    "\n",
    "# model2.compile(loss=weighted_categorical_crossentropy(weights=weights), optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "losses = {\n",
    "#     \"service_output\": f1_loss_perClass ,\n",
    "    \"Event_output\": f1_loss_perRow ,\n",
    "    \"Event_output\": \"categorical_crossentropy\",\n",
    "}\n",
    "lossWeights = {#\"service_output\": 20,\n",
    "               \"Event_output\": 30.0 ,\n",
    "    \"Event_output\": 20}\n",
    " \n",
    "\n",
    "\n",
    "model2.compile(loss=losses,loss_weights=lossWeights, optimizer='adam', metrics=[f1_perRow,f1_perClass,'acc'])\n",
    "# model2.compile(loss=losses, loss_weights=lossWeights, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('IoTDownNet', monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "print(model2.summary())\n",
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=80, batch_size=70, shuffle=True, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lstm_tests_known)) :\n",
    "    print( \"==================HOME Case : %s =============\" % test_names[i])\n",
    "    lstm_pred= model2.predict( rf_test_known[i][0])\n",
    "    \n",
    "#     print_info( lstm_tests[i][1], lstm_pred, classes , confidance=0.7)\n",
    "    print_info( rf_test_known[i][1], lstm_pred, classes , confidance=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "dim_size =15\n",
    "x_lstm_prossed_train,y_lstm_prossed_train, _ = pre_process_raw( x_train, y_train , dim_size, zero_pad=False, normalize=False,classes=classes)\n",
    "_, y_s_lstm_processed_train ,_ =  pre_process_raw( x_train, y_train_service , dim_size, zero_pad=False, normalize=False,classes=service_classes)\n",
    "# x_lstm_prossed_test,y_lstm_prossed_test, _ = pre_process_raw( x_test, y_test_2 , dim_size, zero_pad=True, normalize=False,classes=classes)\n",
    "lstm_tests  = [ pre_process_raw( x_test[i], y_test[i] , dim_size, zero_pad=False, normalize=False, classes=classes, class_cap=TEST_CLASS_CAP) for i in range(len(x_test)) ] \n",
    "lstm_tests_services  = [ pre_process_raw( x_test[i], y_test_service[i] , dim_size, zero_pad=False, normalize=True, classes=service_classes, class_cap=TEST_CLASS_CAP) for i in range(len(x_test)) ] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# known_indexes_train  = [ i for i in range(len(train_known_unknown_pred)) if train_known_unknown_pred[i][1] ==1 ]\n",
    "\n",
    "# x_lstm_prossed_train = x_lstm_prossed_train[known_indexes_train]\n",
    "# y_lstm_prossed_train = y_lstm_prossed_train[known_indexes_train]\n",
    "\n",
    "lstm_tests_known = [] \n",
    "\n",
    "for test_index in range(len(lstm_tests)):\n",
    "#     known_indexes= [ i for i in range(len(test_known_unknown_predicted[test_index])) if test_known_unknown_predicted[test_index][i][1] ==1 ]\n",
    "    \n",
    "    lstm_tests_known.append(  (lstm_tests[test_index][0], \n",
    "                            lstm_tests[test_index][1],\n",
    "                            lstm_tests[test_index][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# known_indexes_train  = [ i for i in range(len(train_known_unknown_pred)) if train_known_unknown_pred[i][1] ==1 ]\n",
    "\n",
    "# x_lstm_prossed_train = x_lstm_prossed_train[known_indexes_train]\n",
    "# y_lstm_prossed_train = y_lstm_prossed_train[known_indexes_train]\n",
    "\n",
    "# lstm_tests_known = [] \n",
    "\n",
    "# for test_index in range(len(rf_tests)):\n",
    "#     known_indexes= [ i for i in range(len(test_known_unknown_predicted[test_index])) if test_known_unknown_predicted[test_index][i][1] ==1 ]\n",
    "    \n",
    "#     lstm_tests_known.append(  (lstm_tests[test_index][0][known_indexes], \n",
    "#                             lstm_tests[test_index][1][known_indexes],\n",
    "#                             lstm_tests[test_index][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_lstm_prossed_test2 = np.expand_dims(x_lstm_prossed_test,axis=1)\n",
    "# x_lstm_prossed_train2 =np.expand_dims(x_lstm_prossed_train,axis=1)\n",
    "\n",
    "for tt  in range( len(lstm_tests_known) ):\n",
    "    lstm_tests_known[tt]= (lstm_tests_known[tt][0].reshape(len(lstm_tests_known[tt][0]),dim_size,1) ,\n",
    "                           lstm_tests_known[tt][1],\n",
    "                           lstm_tests_services[tt][1] )\n",
    "# x_lstm_prossed_test2 = x_lstm_prossed_test.reshape(len(x_lstm_prossed_test),dim_size,1)\n",
    "x_lstm_prossed_train2 =x_lstm_prossed_train.reshape(len(x_lstm_prossed_train),dim_size,1)\n",
    "\n",
    "# y_lstm_prossed_test2 = y_lstm_prossed_test.reshape(len(y_lstm_prossed_test),len(classes),1)\n",
    "# y_lstm_prossed_train2 =y_lstm_prossed_train.reshape(len(y_lstm_prossed_train),len(classes),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bidirectional LSTM :D \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Flatten\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import Input, merge, concatenate, Conv2D, MaxPooling2D, Activation, UpSampling2D, Dropout, Conv2DTranspose, UpSampling2D, Lambda\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization as bn\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.merge import add\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers.merge import add\n",
    "from keras.layers import Dense,Conv1D,Dropout,Activation,BatchNormalization,MaxPooling1D,Flatten,Masking,TimeDistributed\n",
    "from keras.layers.recurrent import LSTM,GRU,SimpleRNN\n",
    "from keras.models import Input,Sequential,Model\n",
    "from keras.layers.merge import add\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import MSE,MSLE\n",
    "model2 = Sequential()\n",
    "\n",
    "\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def f1_perClass(y_true, y_pred):\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "\n",
    "def f1_perRow(y_true, y_pred):\n",
    "\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss_perRow(y_true, y_pred):\n",
    "    \n",
    "#     y_true = y_true * \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1) \n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1) \n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1) \n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1) \n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "#     f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "def f1_loss_perClass(y_true, y_pred):\n",
    "#     key = K.variable([1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.])\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = (2*p*r / (p+r+K.epsilon())) #*key\n",
    "    \n",
    "#     f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs  = Input( ( dim_size,1 ) )\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(inputs)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# bi_d_1 = Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat') (inputs)\n",
    "lstm_1 =  LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True)(out)\n",
    "# lstm_2 = LSTM(30 ,  recurrent_dropout=0.14, return_sequences=True)(lstm_1)\n",
    "\n",
    "bi_d_1 =Dense(128, activation='relu')  (lstm_1)\n",
    "lstm_1 =  Dense(128, activation='relu')(bi_d_1)\n",
    "lstm_2 = Dense(128, activation='relu')(lstm_1)\n",
    "\n",
    "\n",
    "\n",
    "# td_1    = TimeDistributed(Dense(256, activation='relu'))(lstm_2)\n",
    "# dout_1  = Dropout(0.1)(td_1)\n",
    "dout_1  = Dropout(0.1)(lstm_2)\n",
    "flt_1   = Flatten()(dout_1)\n",
    "dense_1 = Dense(128, activation='relu')(flt_1)\n",
    "dout_2  = Dropout(0.2)(dense_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# bi_d_1 = Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat') (inputs)\n",
    "lstm_1 =  LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True)(inputs)\n",
    "# lstm_1 = LSTM(40 ,  recurrent_dropout=0.14, return_sequences=True)(lstm_1)\n",
    "\n",
    "bi_d_raw_1 =Dense(128, activation='relu')  (lstm_1)\n",
    "lstm_raw_1 =  Dense(128, activation='relu')(bi_d_raw_1)\n",
    "lstm_raw_2 = Dense(128, activation='relu')(lstm_raw_1)\n",
    "\n",
    "dout_1  = Dropout(0.1)(lstm_raw_2)\n",
    "flt_1   = Flatten()(dout_1)\n",
    "dense_1 = Dense(128, activation='relu')(flt_1)\n",
    "dout_3  = Dropout(0.2)(dense_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(inputs)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# out = Flatten()(out)\n",
    "# out = MaxPooling1D(2,padding='same', name ='pooling')(out)\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# out = Flatten()(out)\n",
    "# out = MaxPooling1D(2,padding='same', name ='pooling2')(out)\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# out = Flatten()(out)\n",
    "# out = MaxPooling1D(2,padding='same', name ='pooling')(out)\n",
    "\n",
    "\n",
    "\n",
    "# fl_out_1 = Flatten()(dout_2)\n",
    "\n",
    "fl_out_cnn = Flatten()(out)\n",
    "\n",
    "# out_new = concatenate( [fl_out_1, fl_out_cnn] , name='mergerguy')\n",
    "out_new = concatenate( [dout_2, fl_out_cnn,dout_3] , name='mergerguy')\n",
    "\n",
    "dens_out_1 = Dense( 128, activation='relu' )(out_new)\n",
    "dens_out_2 = Dense( 128, activation='relu' )(dens_out_1)\n",
    "dens_out_3 = Dense( 128, activation='relu' )(dens_out_2)\n",
    "\n",
    "# fl2  = Flatten()(out_new)\n",
    "\n",
    "out_put_final = Dense(len(classes), activation='sigmoid', name='Event_output')(dens_out_3)\n",
    "\n",
    "toService_1 = Dense( 130, name=\"to_service1\" )(dens_out_3)\n",
    "toService_1 = Dense( 130, name=\"to_service2\" )(toService_1)\n",
    "\n",
    "service_output = Dense(len(classes  ), activation=\"sigmoid\", name = 'service_output')(toService_1)\n",
    "\n",
    "\n",
    "\n",
    "model2 = Model(inputs=[inputs], outputs=[service_output])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model2.add(Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat'))\n",
    "# model2.add( LSTM(60 ,  recurrent_dropout=0.04, return_sequences=True))\n",
    "# model2.add( LSTM(30 ,  recurrent_dropout=0.14, return_sequences=True))\n",
    "# # model2.add(Bidirectional( LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True),merge_mode='concat'))\n",
    "# model2.add(TimeDistributed(Dense(256, activation='relu')))\n",
    "# model2.add(Dropout(0.1))\n",
    "# model2.add(Flatten())\n",
    "# model2.add(Dense(128, activation='relu'))\n",
    "# model2.add(Dropout(0.2))\n",
    "# model2.add(Dense(len(classes), activation='sigmoid'))\n",
    "\n",
    " \n",
    "weights = [\n",
    "1.0/(57.0 / len(y_train)),\n",
    "1.0/(19.0 / len(y_train)),\n",
    "1.0/(7.0 / len(y_train)),\n",
    "1.0/(14.0 / len(y_train)),\n",
    "1.0/(6.0 / len(y_train)),\n",
    "1.0/(176.0 / len(y_train)),\n",
    "1.0/(27.0 / len(y_train)),\n",
    "1.0/(35.0 / len(y_train)),\n",
    "1.0/(371.0 / len(y_train)),\n",
    "1.0/(11111.0 / len(y_train)),\n",
    "1.0/(4842.0 / len(y_train)),\n",
    "1.0/(119.0 / len(y_train)),\n",
    "1.0/(21.0 / len(y_train)),\n",
    "1.0/(1168.0 / len(y_train)),\n",
    "1.0/(63.0 / len(y_train)),\n",
    "1.0/(13305.0 / len(y_train)),\n",
    "1.0/(11111.0 / len(y_train)),\n",
    "]\n",
    "    \n",
    "\n",
    "\n",
    "# model2.compile(loss=weighted_categorical_crossentropy(weights=weights), optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "losses = {\n",
    "    \"service_output\": f1_loss_perClass ,\n",
    "    \"service_output\": f1_loss_perRow ,\n",
    "    \"service_output\": \"categorical_crossentropy\",\n",
    "}\n",
    "lossWeights = {\"service_output\": 200,\n",
    "               \"service_output\": 30.0 ,\n",
    "    \"service_output\": 20}\n",
    " \n",
    "\n",
    "\n",
    "model2.compile(loss=losses,loss_weights=lossWeights, optimizer='adam', metrics=[f1_perRow,f1_perClass,'acc'])\n",
    "# model2.compile(loss=losses, loss_weights=lossWeights, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('lstm_IoTDownNet_old_data', monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "print(model2.summary())\n",
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=100, batch_size=7000, shuffle=True, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model2.summary())\n",
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=20, batch_size=5000, shuffle=True, callbacks=callbacks_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.load_weights('lstm_IoTDownNet_old_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lstm_tests_known)) :\n",
    "    print( \"==================HOME Case : %s =============\" % test_names[i])\n",
    "    lstm_pred= model2.predict( lstm_tests_known[i][0])\n",
    "    \n",
    "#     print_info( lstm_tests[i][1], lstm_pred, classes , confidance=0.7)\n",
    "    print_info( lstm_tests_known[i][1], lstm_pred, classes , confidance=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(lstm_tests_known)) :\n",
    "    print( \"==================HOME Case : %s =============\" % test_names[i])\n",
    "    lstm_pred= model2.predict( lstm_tests_known[i][0])\n",
    "    \n",
    "#     print_info( lstm_tests[i][1], lstm_pred, classes , confidance=0.7)\n",
    "    print_info( lstm_tests_known[i][1], lstm_pred, classes , confidance=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist2.history['f1_perClass'], c='red')\n",
    "plt.plot(hist2.history['loss'], c='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lstm_tests)) :\n",
    "    print( \"==================HOME Case : %s =============\" % test_names[i])\n",
    "    lstm_pred= model2.predict( lstm_tests[i][0])\n",
    "    \n",
    "#     print_info( lstm_tests[i][1], lstm_pred, classes , confidance=0.7)\n",
    "    print_info( lstm_tests[i][1], lstm_pred, classes , confidance=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist2.history['f1_perClass'], c='red')\n",
    "plt.plot(hist2.history['loss'], c='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bidirectional LSTM :D \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Flatten\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import Input, merge, concatenate, Conv2D, MaxPooling2D, Activation, UpSampling2D, Dropout, Conv2DTranspose, UpSampling2D, Lambda\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization as bn\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.merge import add\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers.merge import add\n",
    "from keras.layers import Dense,Conv1D,Dropout,Activation,BatchNormalization,MaxPooling1D,Flatten,Masking,TimeDistributed\n",
    "from keras.layers.recurrent import LSTM,GRU,SimpleRNN\n",
    "from keras.models import Input,Sequential,Model\n",
    "from keras.layers.merge import add\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import MSE,MSLE\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def f1_perRow(y_true, y_pred):\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "\n",
    "def f1_perClass(y_true, y_pred):\n",
    "\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss_perClass(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "def f1_loss_perRow(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "\n",
    "\n",
    "inputs  = Input( ( dim_size,1 ) )\n",
    "\n",
    "\n",
    "\n",
    "# bi_d_1 = Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat') (inputs)\n",
    "lstm_1 =  LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True)(inputs)\n",
    "# lstm_2 = LSTM(30 ,  recurrent_dropout=0.14, return_sequences=True)(lstm_1)\n",
    "\n",
    "bi_d_1 =Dense(128, activation='relu')  (lstm_1)\n",
    "lstm_1 =  Dense(128, activation='relu')(bi_d_1)\n",
    "lstm_2 = Dense(128, activation='relu')(lstm_1)\n",
    "\n",
    "\n",
    "\n",
    "# td_1    = TimeDistributed(Dense(256, activation='relu'))(lstm_2)\n",
    "# dout_1  = Dropout(0.1)(td_1)\n",
    "dout_1  = Dropout(0.1)(lstm_2)\n",
    "flt_1   = Flatten()(dout_1)\n",
    "dense_1 = Dense(128, activation='relu')(flt_1)\n",
    "dout_2  = Dropout(0.2)(dense_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dens_out_3 = Dense( 128, activation='relu' )(dout_2)\n",
    "\n",
    "\n",
    "# fl2  = Flatten()(out_new)\n",
    "\n",
    "out_put_final = Dense(len(classes), activation='sigmoid', name='Event_output')(dens_out_3)\n",
    "\n",
    "toService_1 = Dense( 130, name=\"to_service1\" )(dens_out_3)\n",
    "toService_1 = Dense( 130, name=\"to_service2\" )(toService_1)\n",
    "\n",
    "service_output = Dense(len(classes  ), activation=\"sigmoid\", name = 'service_output')(toService_1)\n",
    "\n",
    "\n",
    "\n",
    "model2 = Model(inputs=[inputs], outputs=[service_output])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model2.add(Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat'))\n",
    "# model2.add( LSTM(60 ,  recurrent_dropout=0.04, return_sequences=True))\n",
    "# model2.add( LSTM(30 ,  recurrent_dropout=0.14, return_sequences=True))\n",
    "# # model2.add(Bidirectional( LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True),merge_mode='concat'))\n",
    "# model2.add(TimeDistributed(Dense(256, activation='relu')))\n",
    "# model2.add(Dropout(0.1))\n",
    "# model2.add(Flatten())\n",
    "# model2.add(Dense(128, activation='relu'))\n",
    "# model2.add(Dropout(0.2))\n",
    "# model2.add(Dense(len(classes), activation='sigmoid'))\n",
    "\n",
    "# model2.compile(loss=weighted_categorical_crossentropy(weights=weights), optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "losses = {\n",
    "#     \"service_output\": f1_loss_perClass ,\n",
    "    \"service_output\": f1_loss_perRow ,\n",
    "    \"service_output\": \"categorical_crossentropy\",\n",
    "}\n",
    "lossWeights = {#\"service_output\": 20,\n",
    "               \"service_output\": 30.0 ,\n",
    "    \"service_output\": 20}\n",
    " \n",
    "\n",
    "\n",
    "#model2.compile(loss=losses,loss_weights=lossWeights, optimizer='adam', metrics=[f1_perRow,f1_perClass,'acc'])\n",
    "model2.compile(loss=losses, loss_weights=lossWeights, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('IoTDownNet', monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "print(model2.summary())\n",
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=1000, batch_size=12500, shuffle=True, callbacks=callbacks_list)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bidirectional LSTM :D \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Flatten\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import Input, merge, concatenate, Conv2D, MaxPooling2D, Activation, UpSampling2D, Dropout, Conv2DTranspose, UpSampling2D, Lambda\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization as bn\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.merge import add\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers.merge import add\n",
    "from keras.layers import Dense,Conv1D,Dropout,Activation,BatchNormalization,MaxPooling1D,Flatten,Masking,TimeDistributed\n",
    "from keras.layers.recurrent import LSTM,GRU,SimpleRNN\n",
    "from keras.models import Input,Sequential,Model\n",
    "from keras.layers.merge import add\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import MSE,MSLE\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def f1_perRow(y_true, y_pred):\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "\n",
    "def f1_perClass(y_true, y_pred):\n",
    "\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss_perClass(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "def f1_loss_perRow(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "\n",
    "\n",
    "inputs  = Input( ( dim_size,1 ) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(inputs)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# out = Flatten()(out)\n",
    "# out = MaxPooling1D(2,padding='same', name ='pooling')(out)\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# out = Flatten()(out)\n",
    "# out = MaxPooling1D(2,padding='same', name ='pooling2')(out)\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# out = Flatten()(out)\n",
    "# out = MaxPooling1D(2,padding='same', name ='pooling')(out)\n",
    "\n",
    "\n",
    "\n",
    "# fl_out_1 = Flatten()(dout_2)\n",
    "\n",
    "fl_out_cnn = Flatten()(out)\n",
    "\n",
    "\n",
    "dens_out_3 = Dense( 128, activation='relu' )(fl_out_cnn)\n",
    "\n",
    "# fl2  = Flatten()(out_new)\n",
    "\n",
    "out_put_final = Dense(len(classes), activation='sigmoid', name='Event_output')(dens_out_3)\n",
    "\n",
    "toService_1 = Dense( 130, name=\"to_service1\" )(dens_out_3)\n",
    "toService_1 = Dense( 130, name=\"to_service2\" )(toService_1)\n",
    "\n",
    "service_output = Dense(len(classes  ), activation=\"sigmoid\", name = 'service_output')(toService_1)\n",
    "\n",
    "\n",
    "\n",
    "model2 = Model(inputs=[inputs], outputs=[service_output])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model2.add(Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat'))\n",
    "# model2.add( LSTM(60 ,  recurrent_dropout=0.04, return_sequences=True))\n",
    "# model2.add( LSTM(30 ,  recurrent_dropout=0.14, return_sequences=True))\n",
    "# # model2.add(Bidirectional( LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True),merge_mode='concat'))\n",
    "# model2.add(TimeDistributed(Dense(256, activation='relu')))\n",
    "# model2.add(Dropout(0.1))\n",
    "# model2.add(Flatten())\n",
    "# model2.add(Dense(128, activation='relu'))\n",
    "# model2.add(Dropout(0.2))\n",
    "# model2.add(Dense(len(classes), activation='sigmoid'))\n",
    "\n",
    "# model2.compile(loss=weighted_categorical_crossentropy(weights=weights), optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "losses = {\n",
    "#     \"service_output\": f1_loss_perClass ,\n",
    "    \"service_output\": f1_loss_perRow ,\n",
    "    \"service_output\": \"categorical_crossentropy\",\n",
    "}\n",
    "lossWeights = {#\"service_output\": 20,\n",
    "               \"service_output\": 30.0 ,\n",
    "    \"service_output\": 20}\n",
    " \n",
    "\n",
    "\n",
    "model2.compile(loss=losses,loss_weights=lossWeights, optimizer='adam', metrics=[f1_perRow,f1_perClass,'acc'])\n",
    "# model2.compile(loss=losses, loss_weights=lossWeights, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('IoTDownNet', monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "print(model2.summary())\n",
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=100, batch_size=12500, shuffle=True, callbacks=callbacks_list)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bidirectional LSTM :D \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Flatten\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import Input, merge, concatenate, Conv2D, MaxPooling2D, Activation, UpSampling2D, Dropout, Conv2DTranspose, UpSampling2D, Lambda\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization as bn\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.merge import add\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers.merge import add\n",
    "from keras.layers import Dense,Conv1D,Dropout,Activation,BatchNormalization,MaxPooling1D,Flatten,Masking,TimeDistributed\n",
    "from keras.layers.recurrent import LSTM,GRU,SimpleRNN\n",
    "from keras.models import Input,Sequential,Model\n",
    "from keras.layers.merge import add\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import MSE,MSLE\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def f1_perRow(y_true, y_pred):\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "\n",
    "def f1_perClass(y_true, y_pred):\n",
    "\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss_perClass(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1)) \n",
    "\n",
    "def f1_loss_perRow(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))\n",
    "\n",
    "\n",
    "\n",
    "inputs  = Input( ( dim_size,1 ) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# bi_d_1 = Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat') (inputs)\n",
    "lstm_1 =  LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True)(inputs)\n",
    "# lstm_1 = LSTM(40 ,  recurrent_dropout=0.14, return_sequences=True)(lstm_1)\n",
    "\n",
    "bi_d_raw_1 =Dense(128, activation='relu')  (lstm_1)\n",
    "lstm_raw_1 =  Dense(128, activation='relu')(bi_d_raw_1)\n",
    "lstm_raw_2 = Dense(128, activation='relu')(lstm_raw_1)\n",
    "\n",
    "dout_1  = Dropout(0.1)(lstm_raw_2)\n",
    "flt_1   = Flatten()(dout_1)\n",
    "dense_1 = Dense(128, activation='relu')(flt_1)\n",
    "dout_3  = Dropout(0.2)(dense_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(inputs)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# out = Flatten()(out)\n",
    "# out = MaxPooling1D(2,padding='same', name ='pooling')(out)\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# out = Flatten()(out)\n",
    "# out = MaxPooling1D(2,padding='same', name ='pooling2')(out)\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# out = Flatten()(out)\n",
    "# out = MaxPooling1D(2,padding='same', name ='pooling')(out)\n",
    "\n",
    "\n",
    "\n",
    "# fl_out_1 = Flatten()(dout_2)\n",
    "\n",
    "fl_out_cnn = Flatten()(out)\n",
    "\n",
    "# out_new = concatenate( [fl_out_1, fl_out_cnn] , name='mergerguy')\n",
    "out_new = concatenate( [ fl_out_cnn,dout_3] , name='mergerguy')\n",
    "\n",
    "dens_out_1 = Dense( 128, activation='relu' )(out_new)\n",
    "dens_out_2 = Dense( 128, activation='relu' )(dens_out_1)\n",
    "dens_out_3 = Dense( 128, activation='relu' )(dens_out_2)\n",
    "\n",
    "\n",
    "\n",
    "service_output = Dense(len(classes  ), activation=\"sigmoid\", name = 'service_output')(dens_out_3)\n",
    "\n",
    "\n",
    "\n",
    "model2 = Model(inputs=[inputs], outputs=[service_output])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model2.add(Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat'))\n",
    "# model2.add( LSTM(60 ,  recurrent_dropout=0.04, return_sequences=True))\n",
    "# model2.add( LSTM(30 ,  recurrent_dropout=0.14, return_sequences=True))\n",
    "# # model2.add(Bidirectional( LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True),merge_mode='concat'))\n",
    "# model2.add(TimeDistributed(Dense(256, activation='relu')))\n",
    "# model2.add(Dropout(0.1))\n",
    "# model2.add(Flatten())\n",
    "# model2.add(Dense(128, activation='relu'))\n",
    "# model2.add(Dropout(0.2))\n",
    "# model2.add(Dense(len(classes), activation='sigmoid'))\n",
    "\n",
    "# model2.compile(loss=weighted_categorical_crossentropy(weights=weights), optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "losses = {\n",
    "#     \"service_output\": f1_loss_perClass ,\n",
    "    \"service_output\": f1_loss_perRow ,\n",
    "    \"service_output\": \"categorical_crossentropy\",\n",
    "}\n",
    "lossWeights = {#\"service_output\": 20,\n",
    "               \"service_output\": 30.0 ,\n",
    "    \"service_output\": 20}\n",
    " \n",
    "\n",
    "\n",
    "model2.compile(loss=losses,loss_weights=lossWeights, optimizer='adam', metrics=[f1_perRow,f1_perClass,'acc'])\n",
    "# model2.compile(loss=losses, loss_weights=lossWeights, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('IoTDownNet', monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "print(model2.summary())\n",
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=100, batch_size=12500, shuffle=True, callbacks=callbacks_list)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {\n",
    "#     \"service_output\": f1_loss_perClass ,\n",
    "    \"service_output\": f1_loss_perClass ,\n",
    "#     \"service_output\": \"categorical_crossentropy\",\n",
    "}\n",
    "lossWeights = {#\"service_output\": 20,\n",
    "               \"service_output\": 30.0 ,\n",
    "#     \"service_output\": 20\n",
    "}\n",
    " \n",
    "\n",
    "\n",
    "model2.compile(loss=losses, loss_weights=lossWeights, optimizer=keras.optimizers.Adam(lr=1e-6  ), metrics=[f1_perRow,f1_perClass,'acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bidirectional LSTM :D \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Flatten\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import Input, merge, concatenate, Conv2D, MaxPooling2D, Activation, UpSampling2D, Dropout, Conv2DTranspose, UpSampling2D, Lambda\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization as bn\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.merge import add\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers.merge import add\n",
    "from keras.layers import Dense,Conv1D,Dropout,Activation,BatchNormalization,MaxPooling1D,Flatten,Masking,TimeDistributed\n",
    "from keras.layers.recurrent import LSTM,GRU,SimpleRNN\n",
    "from keras.models import Input,Sequential,Model\n",
    "from keras.layers.merge import add\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import MSE,MSLE\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def f1_perRow(y_true, y_pred):\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "\n",
    "def f1_perClass(y_true, y_pred):\n",
    "\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss_perClass(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "def f1_loss_perRow(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "\n",
    "\n",
    "inputs  = Input( ( dim_size,1 ) )\n",
    "\n",
    "\n",
    "\n",
    "# bi_d_1 = Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat') (inputs)\n",
    "lstm_1 =  LSTM(32 ,  recurrent_dropout=0.3, return_sequences=True)(inputs)\n",
    "lstm_2 = LSTM(32 ,  recurrent_dropout=0.3, return_sequences=True)(lstm_1)\n",
    "\n",
    "lstm_2=Flatten()(lstm_2)\n",
    "lstm_2 = Dense(256, activation='relu')(lstm_2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "service_output = Dense(len(classes  ), activation=\"sigmoid\", name = 'service_output')(lstm_2)\n",
    "\n",
    "\n",
    "\n",
    "model2 = Model(inputs=[inputs], outputs=[service_output])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model2.add(Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat'))\n",
    "# model2.add( LSTM(60 ,  recurrent_dropout=0.04, return_sequences=True))\n",
    "# model2.add( LSTM(30 ,  recurrent_dropout=0.14, return_sequences=True))\n",
    "# # model2.add(Bidirectional( LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True),merge_mode='concat'))\n",
    "# model2.add(TimeDistributed(Dense(256, activation='relu')))\n",
    "# model2.add(Dropout(0.1))\n",
    "# model2.add(Flatten())\n",
    "# model2.add(Dense(128, activation='relu'))\n",
    "# model2.add(Dropout(0.2))\n",
    "# model2.add(Dense(len(classes), activation='sigmoid'))\n",
    "\n",
    "# model2.compile(loss=weighted_categorical_crossentropy(weights=weights), optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "losses = {\n",
    "#     \"service_output\": f1_loss_perClass ,\n",
    "    \"service_output\": f1_loss_perRow ,\n",
    "    \"service_output\": \"categorical_crossentropy\",\n",
    "}\n",
    "lossWeights = {#\"service_output\": 20,\n",
    "               \"service_output\": 30.0 ,\n",
    "    \"service_output\": 20}\n",
    " \n",
    "\n",
    "\n",
    "model2.compile(loss=losses,loss_weights=lossWeights, optimizer='adam', metrics=[f1_perRow,f1_perClass,'acc'])\n",
    "# model2.compile(loss=losses, loss_weights=lossWeights, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('IoTDownNet', monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "print(model2.summary())\n",
    "\n",
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=100, batch_size=12500, shuffle=True, callbacks=callbacks_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split model\n",
    "## Bidirectional LSTM :D \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Flatten\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import Input, merge, concatenate, Conv2D, MaxPooling2D, Activation, UpSampling2D, Dropout, Conv2DTranspose, UpSampling2D, Lambda\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization as bn\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.merge import add\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers.merge import add\n",
    "from keras.layers import Dense,Conv1D,Dropout,Activation,BatchNormalization,MaxPooling1D,Flatten,Masking,TimeDistributed\n",
    "from keras.layers.recurrent import LSTM,GRU,SimpleRNN\n",
    "from keras.models import Input,Sequential,Model\n",
    "from keras.layers.merge import add\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import MSE,MSLE\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def f1_perRow(y_true, y_pred):\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "\n",
    "def f1_perClass(y_true, y_pred):\n",
    "\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss_perClass(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "def f1_loss_perRow(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "#splitting data\n",
    "X=x_lstm_prossed_train2\n",
    "y=y_lstm_prossed_train\n",
    "a,b,c,d,e,f,g,h,ii,jj,k,l,m,n,o,p=[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]\n",
    "print(len(y_lstm_prossed_train[0]))\n",
    "for i in range(0,len(y_lstm_prossed_train)):\n",
    "    a.append(y_lstm_prossed_train[i][0])\n",
    "    b.append(y_lstm_prossed_train[i][1])\n",
    "    c.append(y_lstm_prossed_train[i][2])\n",
    "    d.append(y_lstm_prossed_train[i][3])\n",
    "    e.append(y_lstm_prossed_train[i][4])\n",
    "    f.append(y_lstm_prossed_train[i][5])\n",
    "    g.append(y_lstm_prossed_train[i][6])\n",
    "    h.append(y_lstm_prossed_train[i][7])\n",
    "    ii.append(y_lstm_prossed_train[i][8])\n",
    "    jj.append(y_lstm_prossed_train[i][9])\n",
    "    k.append(y_lstm_prossed_train[i][10])\n",
    "    l.append(y_lstm_prossed_train[i][11])\n",
    "    m.append(y_lstm_prossed_train[i][12])\n",
    "    n.append(y_lstm_prossed_train[i][13])\n",
    "    o.append(y_lstm_prossed_train[i][14])\n",
    "    p.append(y_lstm_prossed_train[i][15])\n",
    "    \n",
    "zzzz=[]    \n",
    "zzzz.append(np.array(a))\n",
    "zzzz.append(np.array(b))\n",
    "zzzz.append(np.array(c))\n",
    "zzzz.append(np.array(d))\n",
    "zzzz.append(np.array(e))\n",
    "zzzz.append(np.array(f))\n",
    "zzzz.append(np.array(g))\n",
    "zzzz.append(np.array(h))\n",
    "zzzz.append(np.array(ii))\n",
    "zzzz.append(np.array(jj))\n",
    "zzzz.append(np.array(k))\n",
    "zzzz.append(np.array(l))\n",
    "zzzz.append(np.array(m))\n",
    "zzzz.append(np.array(n))\n",
    "zzzz.append(np.array(o))\n",
    "zzzz.append(np.array(p))\n",
    "\n",
    "\n",
    "for i in range(0,16):\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "    inputs  = Input( ( dim_size,1 ) )\n",
    "\n",
    "\n",
    "    out = Conv1D(128,3,padding='same')(inputs)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = Activation('relu')(out)\n",
    "    out = Dropout(0.2)(out)\n",
    "    out = Conv1D(128,3,padding='same')(out)\n",
    "    # bi_d_1 = Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat') (inputs)\n",
    "    lstm_1 =  LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True)(out)\n",
    "    # lstm_2 = LSTM(30 ,  recurrent_dropout=0.14, return_sequences=True)(lstm_1)\n",
    "\n",
    "    bi_d_1 =Dense(128, activation='relu')  (lstm_1)\n",
    "    lstm_1 =  Dense(128, activation='relu')(bi_d_1)\n",
    "    lstm_2 = Dense(128, activation='relu')(lstm_1)\n",
    "\n",
    "\n",
    "\n",
    "    # td_1    = TimeDistributed(Dense(256, activation='relu'))(lstm_2)\n",
    "    # dout_1  = Dropout(0.1)(td_1)\n",
    "    dout_1  = Dropout(0.1)(lstm_2)\n",
    "    flt_1   = Flatten()(dout_1)\n",
    "    dense_1 = Dense(128, activation='relu')(flt_1)\n",
    "    dout_2  = Dropout(0.2)(dense_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # bi_d_1 = Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat') (inputs)\n",
    "    lstm_1 =  LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True)(inputs)\n",
    "    # lstm_1 = LSTM(40 ,  recurrent_dropout=0.14, return_sequences=True)(lstm_1)\n",
    "\n",
    "    bi_d_raw_1 =Dense(128, activation='relu')  (lstm_1)\n",
    "    lstm_raw_1 =  Dense(128, activation='relu')(bi_d_raw_1)\n",
    "    lstm_raw_2 = Dense(128, activation='relu')(lstm_raw_1)\n",
    "\n",
    "    dout_1  = Dropout(0.1)(lstm_raw_2)\n",
    "    flt_1   = Flatten()(dout_1)\n",
    "    dense_1 = Dense(128, activation='relu')(flt_1)\n",
    "    dout_3  = Dropout(0.2)(dense_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    out = Conv1D(128,3,padding='same')(inputs)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = Activation('relu')(out)\n",
    "    out = Dropout(0.2)(out)\n",
    "    out = Conv1D(128,3,padding='same')(out)\n",
    "    # out = Flatten()(out)\n",
    "    # out = MaxPooling1D(2,padding='same', name ='pooling')(out)\n",
    "\n",
    "\n",
    "    out = Conv1D(128,3,padding='same')(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = Activation('relu')(out)\n",
    "    out = Dropout(0.2)(out)\n",
    "    out = Conv1D(128,3,padding='same')(out)\n",
    "    # out = Flatten()(out)\n",
    "    # out = MaxPooling1D(2,padding='same', name ='pooling2')(out)\n",
    "\n",
    "\n",
    "    out = Conv1D(128,3,padding='same')(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = Activation('relu')(out)\n",
    "    out = Dropout(0.2)(out)\n",
    "    out = Conv1D(128,3,padding='same')(out)\n",
    "    # out = Flatten()(out)\n",
    "    # out = MaxPooling1D(2,padding='same', name ='pooling')(out)\n",
    "\n",
    "\n",
    "\n",
    "    # fl_out_1 = Flatten()(dout_2)\n",
    "\n",
    "    fl_out_cnn = Flatten()(out)\n",
    "\n",
    "    # out_new = concatenate( [fl_out_1, fl_out_cnn] , name='mergerguy')\n",
    "    out_new = concatenate( [dout_2, fl_out_cnn,dout_3] , name='mergerguy')\n",
    "\n",
    "    dens_out_1 = Dense( 128, activation='relu' )(out_new)\n",
    "    dens_out_2 = Dense( 128, activation='relu' )(dens_out_1)\n",
    "    dens_out_3 = Dense( 128, activation='relu' )(dens_out_2)\n",
    "\n",
    "    # fl2  = Flatten()(out_new)\n",
    "\n",
    "    out_put_final = Dense(len(classes), activation='sigmoid', name='Event_output')(dens_out_3)\n",
    "\n",
    "    toService_1 = Dense( 130, name=\"to_service1\" )(dens_out_3)\n",
    "    toService_1 = Dense( 130, name=\"to_service2\" )(toService_1)\n",
    "\n",
    "    service_output = Dense(1, activation=\"sigmoid\", name = 'service_output')(toService_1)\n",
    "\n",
    "\n",
    "    losses = {\n",
    "    #     \"service_output\": f1_loss_perClass ,\n",
    "        \"service_output\": f1_loss_perRow ,\n",
    "        \"service_output\": \"binary_crossentropy\",\n",
    "    }\n",
    "    lossWeights = {#\"service_output\": 20,\n",
    "                   \"service_output\": 30.0 ,\n",
    "        \"service_output\": 20}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    model2 = Model(inputs=[inputs], outputs=[service_output])\n",
    "    model2.compile(loss=losses,loss_weights=lossWeights, optimizer='adam', metrics=[f1_perRow,f1_perClass,'acc'])\n",
    "    # model2.compile(loss=losses, loss_weights=lossWeights, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    checkpoint = ModelCheckpoint('IoTDownNet', monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "    hist2 = model2.fit(x_lstm_prossed_train2, zzzz[i], epochs=200, batch_size=12500, shuffle=True, callbacks=callbacks_list)\n",
    "    model2.save('number'+str(i)+'.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2.compile(loss=losses,loss_weights=lossWeights, optimizer=keras.optimizers.Adam(lr=5e-5  ), metrics=[f1_perRow,f1_perClass,'acc'])\n",
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=300, batch_size=16500, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss=f1_loss, optimizer=keras.optimizers.Adam(lr=8e-5  ), metrics=[f1,'acc'])\n",
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=300, batch_size=7500, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss=weighted_categorical_crossentropy(weights=weights), optimizer=keras.optimizers.Adam(lr=8e-5  ), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(lr=8e-5  ), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss=weighted_categorical_crossentropy(weights=weights), optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=300, batch_size=3500, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save( \"LSTM-sigmoid-withRemovedClasses\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist2.history['loss'], c='red')\n",
    "plt.plot(hist2.history['acc'], c='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist2.history['loss'], c='red')\n",
    "# plt.plot(hist2.history['acc'], c='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model2, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "# model2=load_model( \"LSTM_withSigmoid_LargeData_F1_E100_B500_MSE_False\"  \n",
    "#            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(lstm_tests)) :\n",
    "    print( \"==================HOME Case : %s =============\" % test_names[i])\n",
    "    makeReadable( classes=classes, confidance=0.5,data=lstm_tests[i][0],gt=lstm_tests[i][1],model=model2,path=test_names[i],x=lstm_tests[i][0])\n",
    "    \n",
    "#     lstm_pred= model2.predict( lstm_tests[i][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ------------- do not go any further :) ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred= model2.predict( lstm_tests[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred= model2.predict( lstm_tests[1][0])\n",
    "lstm_pred__ = np.array(list(lstm_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred__ = np.array(list(lstm_pred))\n",
    "print_info( lstm_tests[1][1], lstm_pred__, classes , confidance=0.43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in [1] :\n",
    "for i in range(len(lstm_tests)) :\n",
    "    print( \"==================HOME Case : %s =============\" % test_names[i])\n",
    "    lstm_pred= model2.predict( lstm_tests[i][0])\n",
    "    \n",
    "#     print_info( lstm_tests[i][1], lstm_pred, classes , confidance=0.7)\n",
    "    print_info( lstm_tests[i][1], lstm_pred, classes , confidance=0.5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in [1] :\n",
    "# for i in range(len(lstm_tests)) :\n",
    "    print( \"==================HOME Case : %s =============\" % test_names[i])\n",
    "    lstm_pred= model2.predict( lstm_tests[i][0])\n",
    "    \n",
    "#     print_info( lstm_tests[i][1], lstm_pred, classes , confidance=0.7)\n",
    "    print_info( lstm_tests[i][1], lstm_pred, classes , confidance=0.992)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred =model2.predict( x_lstm_prossed_test2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_info(y_lstm_prossed_test, lstm_pred, classes, confidance=0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred =model2.predict( x_lstm_prossed_test)\n",
    "print_info(y_lstm_prossed_test, lstm_pred, classes, confidance=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred = model2.predict( x_lstm_prossed_test)\n",
    "print_info(y_lstm_prossed_test, lstm_pred, classes, confidance=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_info(y_lstm_prossed_train, y_lstm_prossed_train, classes, confidance=0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x  in lstm_pred  if  np.sum(x) > 0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save( \"LSTM_withSigmoid_LargeData_F%s_E%d_B%d_M%s_%r\" %\n",
    "            (\n",
    "            FoldID,\n",
    "                Epoch_count,\n",
    "                Batch_size,\n",
    "                Mapper,\n",
    "                IgnoreEmpty\n",
    "            ) \n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred = model2.predict( x_lstm_prossed_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_info(y_lstm_prossed_test, lstm_pred, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_muhammed,y_train_muhammed, classes = pre_process_raw( x_train, y_train , dim_size, zero_pad=True, normalize=False)\n",
    "# x_test_muhammed,y_test_muhammed, classes = pre_process_raw( x_test, y_test , dim_size, zero_pad=True, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred = model2.predict( x_lstm_prossed_test )\n",
    "print_info(y_lstm_prossed_test, lstm_pred, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred = model2.predict( x_lstm_prossed_test )\n",
    "print_info(y_lstm_prossed_test, lstm_pred, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_info(y_lstm_prossed_test, pred, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len( y_lstm_prossed_train[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_size =160\n",
    "x_lstm_prossed_train,y_lstm_prossed_train, _ = pre_process_raw( x_train, y_train , dim_size, zero_pad=True, normalize=False,classes=classes)\n",
    "x_lstm_prossed_test,y_lstm_prossed_test, _ = pre_process_raw( x_test, y_test , dim_size, zero_pad=True, , class_cap=TEST_CLASS_CAPnormalize=False,classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([x  for x  in y_lstm_prossed_test if x[21]==1 or x[20]==1]), len(y_lstm_prossed_test  ) , len([x  for x  in y_lstm_prossed_test if x[21]==1 or x[20]==1])/len(y_lstm_prossed_test  ) *1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ x for x  in  pred if np.sum(x) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_lstm_prossed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_for_raun( pred   ):\n",
    "    pp = pred\n",
    "    pp[pp>=0.5] = 1\n",
    "    pp[pp<0.5] = 0\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([x for x in pred if np.sum( do_for_raun(x) )==0 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([x for x in pred if  do_for_raun(x)[20] ==1 or do_for_raun(x)[21] ==1 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# np.save(\"../files/muhammed/x_train.json\" , x_train_muhammed)\n",
    "# np.save(\"../files/muhammed/y_train.json\", )\n",
    "\n",
    "\n",
    "# np.save( \"../files/muhammed/x_train.json\", x_train_muhammed )\n",
    "# np.save(\"../files/muhammed/y_train.json\",  y_train_muhammed )\n",
    "# np.save( \"../files/muhammed/x_test.json\",x_test_muhammed )\n",
    "# np.save( \"../files/muhammed/y_test.json\",y_test_muhammed )\n",
    "# np.save( \"../files/muhammed/classes.json\",  classes )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_lstm_prossed_test) + len(x_lstm_prossed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6(omid)",
   "language": "python",
   "name": "omid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
